
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="超赞列表合集 awesome list chinese zh CN 中文 awesome-python awesome-nodejs">
      
      
        <meta name="author" content="chenjiajia">
      
      
        <link rel="canonical" href="https://asmcn.icopy.site/awesome/awesome-agi-cocosci/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Awesome agi cocosci - 超赞合集 awesome list chinese</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../_static/css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-0T5HGMNQ9E"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-0T5HGMNQ9E",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-0T5HGMNQ9E",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#awesome-artificial-general-intelligence-and-computational-cognitive-sciences" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="超赞合集 awesome list chinese" class="md-header__button md-logo" aria-label="超赞合集 awesome list chinese" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            超赞合集 awesome list chinese
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Awesome agi cocosci
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/icopy-site/awesome-cn" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    icopy-site/awesome-cn
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="超赞合集 awesome list chinese" class="md-nav__button md-logo" aria-label="超赞合集 awesome list chinese" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    超赞合集 awesome list chinese
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/icopy-site/awesome-cn" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    icopy-site/awesome-cn
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    平台
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    平台
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-nodejs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Node.js
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../frontend-dev-bookmarks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    前端开发
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ios/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    iOS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-android/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Android
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-IoT-hybrid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    IoT & Hybrid Apps
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-electron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Electron
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cordova/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Cordova
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-react-native/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    React Native
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-xamarin/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Xamarin
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linux
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_11" >
        
          
          <label class="md-nav__link" for="__nav_1_11" id="__nav_1_11_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Linux内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_11">
            <span class="md-nav__icon md-icon"></span>
            
  
    Linux内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-linux-containers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Containers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-macOS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    macOS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_13" >
        
          
          <label class="md-nav__link" for="__nav_1_13" id="__nav_1_13_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    macOS内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_13">
            <span class="md-nav__icon md-icon"></span>
            
  
    macOS内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-macos-command-line/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    命令行
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-macos-screensavers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    屏保
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mac/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    应用
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../open-source-mac-os-apps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开源应用
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-watchos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    watchOS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-jvm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    JVM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-salesforce/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Salesforce
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-aws/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Amazon Web Services
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Windows
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ipfs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    IPFS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-fuse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fuse
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-heroku/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Heroku
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-raspberry-pi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Raspberry Pi
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-qt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Qt
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-WebExtensions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    WebExtensions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rubymotion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RubyMotion
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-smart-tv/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Smart TV
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-gnome/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GNOME
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dotnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    .NET
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_29" >
        
          
          <label class="md-nav__link" for="__nav_1_29" id="__nav_1_29_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    .NET内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_29_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_29">
            <span class="md-nav__icon md-icon"></span>
            
  
    .NET内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dotnet-core/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Core
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-amazon-alexa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Amazon Alexa
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-digitalocean/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DigitalOcean
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-flutter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Flutter
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-home-assistant/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home Assistant
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    编程语言
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    编程语言
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-javascript/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    JavaScript
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    JavaScript内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    JavaScript内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-promises/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Promises
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-standard/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Standard Style
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../js-must-watch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    必看讲座
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jstips/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tips
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-network-js/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    网络层
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-micro-npm-packages/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Micro npm Packages
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mad-science/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Mad Science npm Packages
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maintenance-modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Maintenance Modules
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-npm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    npm
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ava/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AVA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-eslint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ESLint
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-fp-js/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Functional Programming
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-observables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Observables
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-npm-scripts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    npm scripts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-swift/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Swift
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Swift内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Swift内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-Swift-Education/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    教育
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-Swift-Playgrounds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    练习
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Python
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Python内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Python内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-asyncio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Asyncio
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-python-scientific-audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scientific Audio
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-circuitpython/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CircuitPython
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rust/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rust
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-haskell/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Haskell
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-purescript/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PureScript
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-go/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Go
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-scala/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scala
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ruby/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ruby
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-clojure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Clojure
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-clojurescript/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ClojureScript
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-elixir/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Elixir
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-elm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Elm
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-erlang/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Erlang
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Julia.jl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Julia
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-lua/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lua
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-c/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    C
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cpp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    C/C++
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-R/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    R
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    D
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Common Lisp
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-perl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Perl
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-groovy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Groovy
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Dart
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-java/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Java
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_29" >
        
          
          <label class="md-nav__link" for="__nav_2_29" id="__nav_2_29_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Java内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_29_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_29">
            <span class="md-nav__icon md-icon"></span>
            
  
    Java内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rxjava/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RxJava
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-kotlin/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kotlin
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ocaml/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    OCaml
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-coldfusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ColdFusion
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-fortran/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fortran
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-php/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PHP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_35" >
        
          
          <label class="md-nav__link" for="__nav_2_35" id="__nav_2_35_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    PHP内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_35_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_35">
            <span class="md-nav__icon md-icon"></span>
            
  
    PHP内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-composer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Composer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-delphi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Delphi
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-asm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Assembler
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-AutoHotkey/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AutoHotkey
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-AutoIt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AutoIt
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-crystal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Crystal
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-frege/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Frege
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cmake/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CMake
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-actionscript3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ActionScript 3
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-eta/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Eta
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-idris/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Idris
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    前端
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    前端
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../es6-tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ES6 Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-wpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web性能优化
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-css/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CSS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    CSS内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    CSS内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../critical-path-css-tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Critical-Path Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scalable-css-reading-list/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scalability
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../must-watch-css/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    必看讲座
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../css-protips/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Protips
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-react/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    React
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    React内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    React内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-relay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Relay
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../webcomponents-the-right-way/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web Components
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-polymer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Polymer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-angular/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Angular
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-backbone/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Backbone
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-html5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    HTML5
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-svg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SVG
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-canvas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Canvas
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-knockout/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    KnockoutJS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dojo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Dojo Toolkit
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Inspire/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Inspiration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ember/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ember
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-android-ui/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Android UI
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ios-ui/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    iOS UI
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-meteor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Meteor
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../BEM-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BEM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-flexbox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Flexbox
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../typography/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web Typography
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-a11y/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web Accessibility
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-material/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Material Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-d3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    D3
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-emails/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Emails
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-jquery/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    jQuery
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_30" >
        
          
          <label class="md-nav__link" for="__nav_3_30" id="__nav_3_30_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    jQuery内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_30_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_30">
            <span class="md-nav__icon md-icon"></span>
            
  
    jQuery内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jquery-tips-everyone-should-know/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tips
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-webaudio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web Audio
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../offline-first/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    离线优先
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-static-website-services/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    静态网站服务
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cyclejs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Cycle.js
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-text-editing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Text Editing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../motion-ui-design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Motion UI Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vue/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vue.js
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-marionette/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Marionette.js
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-aurelia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Aurelia
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-charting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Charting
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ionic/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ionic Framework 2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-chrome-devtools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chrome DevTools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-postcss/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PostCSS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-draft-js/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Draft.js
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-service-workers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Service Workers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-progressive-web-apps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Progressive Web Apps
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-choo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    choo
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-redux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Redux
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-webpack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    webpack
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-browserify/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Browserify
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-sass/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sass
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ant-design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ant Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-less/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Less
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-webgl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    WebGL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-preact/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Preact
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../progressive-enhancement-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Progressive Enhancement
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-nextjs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Next.js
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-hyperapp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hyperapp
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-lit-html/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    lit-html
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-jamstack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    JAMstack
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mobile-web-development/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    移动端web开发
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    后端
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    后端
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-flask/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Flask
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Docker
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vagrant/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vagrant
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-pyramid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pyramid
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-play1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Play1 Framework
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cakephp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CakePHP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-symfony/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Symfony
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
        
          
          <label class="md-nav__link" for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Symfony内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Symfony内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-symfony-education/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Education
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-laravel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Laravel
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10" >
        
          
          <label class="md-nav__link" for="__nav_4_10" id="__nav_4_10_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Laravel内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_10">
            <span class="md-nav__icon md-icon"></span>
            
  
    Laravel内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-Laravel-Education/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    教育
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rails/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rails
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_12" >
        
          
          <label class="md-nav__link" for="__nav_4_12" id="__nav_4_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Rails内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_12">
            <span class="md-nav__icon md-icon"></span>
            
  
    Rails内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rails-gem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-phalcon/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Phalcon
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../htaccess/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    有用的 `.htaccess`片段
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nginx-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    nginx
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dropwizard/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Dropwizard
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-kubernetes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kubernetes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-lumen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lumen
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-serverless/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Serverless框架
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-wicket/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Apache Wicket
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vertx-awesome/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vert.x
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-terraform/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Terraform
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vapor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vapor
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    计算机科学
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    计算机科学
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-courses/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    大学课程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-datascience/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    数据科学
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    数据科学内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    数据科学内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-learn-datascience/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    教程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-machine-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    机器学习
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_5" >
        
          
          <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    机器学习内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    机器学习内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Machine-Learning-Tutorials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    教程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../machine-learning-with-ruby/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ruby机器学习
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-CoreML-Models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Core ML Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-h2o/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    H2O
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6" >
        
          
          <label class="md-nav__link" for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    语音与子软语言处理内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    语音与子软语言处理内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-spanish-nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    西班牙语
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-with-ruby/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    NLP with Ruby
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-linguistics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    语言学
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cryptography/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    加密
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" >
        
          
          <label class="md-nav__link" for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    加密内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    加密内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-crypto-papers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    论文
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-computer-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    机器视觉
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-deep-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    深度学习
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_12" >
        
          
          <label class="md-nav__link" for="__nav_5_12" id="__nav_5_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    深度学习内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_12">
            <span class="md-nav__icon md-icon"></span>
            
  
    深度学习内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-tensorflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    TensorFlow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-deep-learning-papers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    论文
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-deep-learning-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    教育
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-deep-vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    深度视觉
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../computer-science/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开放的社会大学
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-functional-programming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    函数式变成
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-static-analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    静态分析和代码质量
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-information-retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    信息检索
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    大数据
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    大数据
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-bigdata/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    大数据
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-hadoop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hadoop
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-data-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    数据工程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-streaming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Streaming
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-spark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Apache Spark
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    理论
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    理论
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../papers-we-love/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    论文精选
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-talks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    演讲
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    算法
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algovis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    算法可视化
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-artificial-intelligence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    人工智能
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../search-engine-optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SEO
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-competitive-programming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    编程竞赛
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-math/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    数学
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-recursion-schemes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    递归
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    书籍
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    书籍
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../free-programming-books/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    免费编程书籍
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-software-quality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    免费软件测试书籍
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../GoBooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Go 书籍
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rbooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    R 书籍
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Mind-Expanding-Books/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    思维扩展类书籍
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-book-authoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    书籍作者
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ElixirBooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Elixir 书籍
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    编辑器
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    编辑器
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sublime-bookmarks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sublime Text
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vim-galore/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vim
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-atom/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Atom
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vscode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Visual Studio Code
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    游戏
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            
  
    游戏
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../magictools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    游戏开发
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-gametalks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    游戏演讲
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-godot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Godot
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../games/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开源游戏
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-unity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Unity
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-chess/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chess
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-love2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LÖVE
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-PICO-8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PICO-8
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-gbdev/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Game Boy Development
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-construct/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Construct 2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-gideros/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gideros
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    运维
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            
  
    运维
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick-look-plugins/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quick Look Plugins
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-devenv/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Dev Env
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dotfiles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Dotfiles
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-shell/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Shell
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-fish/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fish
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-cli-apps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    命令行应用
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-zsh-plugins/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ZSH 插件
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-github/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GitHub
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_9" >
        
          
          <label class="md-nav__link" for="__nav_11_9" id="__nav_11_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    GitHub内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    GitHub内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-browser-extensions-for-github/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    浏览器插件
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../github-cheat-sheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Cheat Sheet
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git-cheat-sheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Git Cheat Sheet & Git Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tips/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Git Tips
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-git-addons/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Git Add-ons
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ssh/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SSH
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FOSS-for-Dev/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FOSS for Developers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-hyper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hyper
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-powershell/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PowerShell
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-alfred-workflows/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Alfred Workflows
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../terminals-are-sexy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Terminals Are Sexy
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
        
          
          <label class="md-nav__link" for="__nav_12" id="__nav_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    娱乐
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            
  
    娱乐
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-scifi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Science Fiction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-fantasy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fantasy
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-geek-podcasts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Podcasts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-newsletters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Email Newsletters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-it-quotes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    IT Quotes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13" >
        
          
          <label class="md-nav__link" for="__nav_13" id="__nav_13_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    数据库
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13">
            <span class="md-nav__icon md-icon"></span>
            
  
    数据库
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-db/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Database
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mysql/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MySQL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-influxdb/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    InfluxDB
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-neo4j/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Neo4j
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mongodb/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MongoDB
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rethinkdb/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RethinkDB
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-tinkerpop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    TinkerPop
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-postgres/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PostgreSQL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-couchdb/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CouchDB
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-hbase/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    HBase
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
        
          
          <label class="md-nav__link" for="__nav_14" id="__nav_14_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    媒体
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            
  
    媒体
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../creative-commons-media/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Creative Commons Media
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-fonts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fonts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codeface/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Codeface
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-stock-resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stock Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-gif/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GIF
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-music/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    音乐
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-opensource-documents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开源文档
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-audio-visualization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Audio Visualization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-broadcasting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Broadcasting
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-pixel-art/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pixel Art
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ffmpeg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FFmpeg
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_15" >
        
          
          <label class="md-nav__link" for="__nav_15" id="__nav_15_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    学习
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_15">
            <span class="md-nav__icon md-icon"></span>
            
  
    学习
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-workshopper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CLI Workshoppers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../learn-to-program/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    学习编程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-speaking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    演讲
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-tech-videos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    科技视频
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dive-into-machine-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    深入机器学习
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-computer-history/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    计算机历史
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-programming-for-kids/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    少儿编程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-eg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    教育游戏
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-javascript-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    学习JavaScript
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_16" >
        
          
          <label class="md-nav__link" for="__nav_16" id="__nav_16_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    安全
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_16">
            <span class="md-nav__icon md-icon"></span>
            
  
    安全
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-appsec/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    应用安全
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-security/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    安全
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ctf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    夺旗赛
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-malware-analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    恶意软件分析
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../android-security-awesome/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Android安全
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-hacking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hacking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-honeypots/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Honeypots
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-incident-response/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Incident Response
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vehicle-security/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vehicle Security and Car Hacking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-web-security/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web安全
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-lockpicking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lockpicking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-umbraco/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Umbraco
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-refinerycms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Refinery CMS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-wagtail/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Wagtail
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-drupal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Drupal
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_17" >
        
          
          <label class="md-nav__link" for="__nav_17" id="__nav_17_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    硬件
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_17">
            <span class="md-nav__icon md-icon"></span>
            
  
    硬件
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Robotics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-iot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    IOT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-electronics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Electronics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-beacon/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Bluetooth Beacons
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guitarspecs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Electric Guitar Specifications
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_18" >
        
          
          <label class="md-nav__link" for="__nav_18" id="__nav_18_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    商业
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_18">
            <span class="md-nav__icon md-icon"></span>
            
  
    商业
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-open-company/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Open Companies
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../PlacesToPostYourStartup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Places to Post Your Startup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-okr/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    OKR Methodology
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-leading-and-managing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Leading and Managing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-indie/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Indie
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_19" >
        
          
          <label class="md-nav__link" for="__nav_19" id="__nav_19_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    工作
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_19">
            <span class="md-nav__icon md-icon"></span>
            
  
    工作
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-slack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Slack
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_19_2" >
        
          
          <label class="md-nav__link" for="__nav_19_2" id="__nav_19_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Slack内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_19_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_19_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Slack内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-slack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Slack
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-remote-job/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    远程工作
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-productivity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    生产力
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-job-boards/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Niche Job Boards
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-interview-questions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    面试
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-code-review/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Code Review
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_20" >
        
          
          <label class="md-nav__link" for="__nav_20" id="__nav_20_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    网络
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_20">
            <span class="md-nav__icon md-icon"></span>
            
  
    网络
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-sdn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    软件定义网络
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-network-analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    网络分析
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-pcaptools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PCAPTools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21" >
        
          
          <label class="md-nav__link" for="__nav_21" id="__nav_21_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    去中心化
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_21_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21">
            <span class="md-nav__icon md-icon"></span>
            
  
    去中心化
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-bitcoin/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    比特币
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ripple/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    波场
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-non-financial-blockchain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Non-Financial Blockchain
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mastodon/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Mastodon
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-Ethereum/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    以太坊
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_22" >
        
          
          <label class="md-nav__link" for="__nav_22" id="__nav_22_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    杂项
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_22_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_22">
            <span class="md-nav__icon md-icon"></span>
            
  
    杂项
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-json/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    JSON
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_22_2" >
        
          
          <label class="md-nav__link" for="__nav_22_2" id="__nav_22_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    JSON内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_22_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_22_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    JSON内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-geojson/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GeoJSON
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-json-datasets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Datasets
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../discount-for-student-dev/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    学生开发者优惠
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-sysadmin/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sysadmin
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-radio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Radio
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Awesome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-analytics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Analytics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-rest/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    REST
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-selenium/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Selenium
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-appium/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Appium
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ciandcd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    持续集成与交付
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../services-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Services Engineering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../free-for-dev/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开发者免费
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-answers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Answers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-sketch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sketch
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-projects-boilerplates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    脚手架
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-readme/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Readme
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ToolsOfTheTrade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-styleguides/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Styleguides
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guides/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    设计与开发指南
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../engineering-blogs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    工程师博客
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-selfhosted/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Self Hosted
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-foss-apps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FOSS Production Apps
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-gulp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gulp
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../amas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AMA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_22_26" >
        
          
          <label class="md-nav__link" for="__nav_22_26" id="__nav_22_26_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    AMA内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_22_26_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_22_26">
            <span class="md-nav__icon md-icon"></span>
            
  
    AMA内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ama-answers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Answers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-OpenSourcePhotography/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开源图片
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-opengl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    OpenGL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-graphql/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GraphQL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-transit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Transit
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-research/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    研究工具
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dataviz/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    数据可视化
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../shareable-links/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    社交媒体分享链接
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-microservices/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    微服务
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-Unicode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Unicode
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_22_36" >
        
          
          <label class="md-nav__link" for="__nav_22_36" id="__nav_22_36_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Unicode内容
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_22_36_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_22_36">
            <span class="md-nav__icon md-icon"></span>
            
  
    Unicode内容
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-codepoints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Code Points
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-for-beginners/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    新手友好项目
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-katas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Katas
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../toolsforactivism/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tools for Activism
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../citizen-science/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Citizen Science
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-tap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    TAP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-mqtt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MQTT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-hacking-locations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hacking Spots
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome4girls/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    女性开发者专属
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vorpal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vorpal
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-vulkan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vulkan
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-LaTeX/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LaTeX
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-funny-markov/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Funny Markov Chains
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Awesome-Bioinformatics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Bioinformatics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Colorful/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Colorful
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-steam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Steam
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bots/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Bots
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-sre/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Site Reliability Engineering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../empathy-in-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Empathy in Engineering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dtrace/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DTrace
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-userscripts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Userscripts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-pokemon/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pokémon
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-chatops/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ChatOps
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-falsehood/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Falsehood
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ddd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    领域驱动设计
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-quantified-self/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quantified Self
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-web-design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Web设计
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-jmeter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    JMeter
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-creative-coding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    创造性编程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-no-login-web-apps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    无需登录web应用
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-testing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    测试
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-free-software/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    免费软件
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-framer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Framer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-markdown/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Markdown
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-dev-fun/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Dev Fun
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-netherlands-events/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Events in the Netherlands
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-healthcare/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Healthcare
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-magento2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Magento 2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-tikz/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    TikZ
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-neuroscience/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Neuroscience
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ad-free/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ad-Free
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-esolangs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Esolangs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-prometheus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Prometheus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-homematic/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Homematic
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-ledger/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ledger
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-uncopyright/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    放弃版权
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-coins/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    加密货币工具与算法
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-diversity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Diversity
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-open-source-supporters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开源支持者
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-design-principles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    设计原则
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-regression-testing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Visual Regression Testing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-theravada/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Theravada
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-inspectit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    inspectIT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-maintainers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    开源项目维护者
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-calculators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Calculators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-captcha/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Captcha
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-jupyter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Jupyter
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-frc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FIRST Robotics Competition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-humane-tech/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Humane Technology
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-speakers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Speakers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-software-patreons/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Software Patreons
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../awesome-parasite/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Parasite
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#awesome-artificial-general-intelligence-and-computational-cognitive-sciences" class="md-nav__link">
    <span class="md-ellipsis">
      
        Awesome Artificial General Intelligence and Computational Cognitive Sciences
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contributing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contributing
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#papers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Papers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#abduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Abduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Abduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#explanation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Explanation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scientific-discovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scientific Discovery
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rationalization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Rationalization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-in-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Applications in AI
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Modeling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Modeling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-induction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Induction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonparametric-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Nonparametric Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Optimization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concepts" class="md-nav__link">
    <span class="md-ellipsis">
      
        Concepts
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Concepts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#theory-of-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      
        Theory of Concepts
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-concept-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Human Concept Representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-concept-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Concept Representation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complexity-information-theory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Complexity &amp; Information Theory
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Complexity &amp; Information Theory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#theory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Theory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dimensionality Reduction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visual Complexity
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communications
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Communications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#non-verbal-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Non-Verbal Communication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pragmatics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pragmatics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-compositionality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Language Compositionality
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordination" class="md-nav__link">
    <span class="md-ellipsis">
      
        Coordination
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#domain-specific-language" class="md-nav__link">
    <span class="md-ellipsis">
      
        Domain Specific Language
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Domain Specific Language">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#design-theory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Design Theory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#design-practises" class="md-nav__link">
    <span class="md-ellipsis">
      
        Design Practises
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#design-automation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Design Automation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#imperative-dsl-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Imperative DSL Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#declarative-dsl-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Declarative DSL Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logic-dsl-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Logic DSL Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dsl-program-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      
        DSL Program Synthesis
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cognitive-foundations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cognitive Foundations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-solving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Problem Solving
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Problem Solving">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-level-problem-solving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Human-Level Problem Solving
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Planning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intrinsic-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Intrinsic Motivation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inverse-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inverse Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#system-1-system-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        System 1 &amp; System 2
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="System 1 &amp; System 2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dual-coding-theory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dual-Coding Theory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-symbolic-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neural-Symbolic AI
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explainability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Explainability
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Explainability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#trustworthy-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Trustworthy AI
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strong-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Strong Machine Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explainable-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Explainable Deep Learning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embodied-intelligence" class="md-nav__link">
    <span class="md-ellipsis">
      
        Embodied Intelligence
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evolutionary-intelligence" class="md-nav__link">
    <span class="md-ellipsis">
      
        Evolutionary Intelligence
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methodologies-for-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      
        Methodologies for Experiments
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Methodologies for Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quantitative-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quantitative Analysis
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaling-up-behavioral-studies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaling Up Behavioral Studies
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decision-making" class="md-nav__link">
    <span class="md-ellipsis">
      
        Decision Making
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-answering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Question Answering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-machine-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Human-Machine Comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#association-test" class="md-nav__link">
    <span class="md-ellipsis">
      
        Association Test
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#virtual-reality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Virtual Reality
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta-level-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meta-Level Considerations
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Meta-Level Considerations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#meta-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meta Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#marrs-levels-of-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Marr's Levels of Analysis
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gestalt" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gestalt
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-aha-moment" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Aha! Moment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rationality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Rationality
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cognitive-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cognitive Architecture
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#science-logology" class="md-nav__link">
    <span class="md-ellipsis">
      
        Science Logology
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Science Logology">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#philosophy-of-science" class="md-nav__link">
    <span class="md-ellipsis">
      
        Philosophy of Science
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#science-of-science" class="md-nav__link">
    <span class="md-ellipsis">
      
        Science of Science
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#literature-mining" class="md-nav__link">
    <span class="md-ellipsis">
      
        Literature Mining
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scientific-writing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scientific Writing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#science-education" class="md-nav__link">
    <span class="md-ellipsis">
      
        Science Education
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#democratization-of-science" class="md-nav__link">
    <span class="md-ellipsis">
      
        Democratization of Science
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laboratory-automation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Laboratory Automation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-assisted-research" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Assisted Research
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#theory-of-mind" class="md-nav__link">
    <span class="md-ellipsis">
      
        Theory of Mind
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#analogy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Analogy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#causality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Causality
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#commonsense" class="md-nav__link">
    <span class="md-ellipsis">
      
        Commonsense
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Commonsense">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intuitive-physics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Intuitive Physics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-commonsense-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Commonsense Reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#commonsense-knowledgebase" class="md-nav__link">
    <span class="md-ellipsis">
      
        Commonsense Knowledgebase
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inductive-logic-program-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inductive Logic &amp; Program Synthesis
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#knowledge-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Knowledge Representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cognitive-development" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cognitive Development
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-in-the-open-world" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning in the Open World
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-with-cognitive-plausibility" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning with Cognitive Plausibility
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#academic-tools" class="md-nav__link">
    <span class="md-ellipsis">
      
        Academic Tools
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Academic Tools">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#courses" class="md-nav__link">
    <span class="md-ellipsis">
      
        Courses
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#programming" class="md-nav__link">
    <span class="md-ellipsis">
      
        Programming
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paper-writing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Paper Writing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paper-reading" class="md-nav__link">
    <span class="md-ellipsis">
      
        Paper Reading
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#literature-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Literature Management
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#knowledge-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Knowledge Management
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#institute-researcher" class="md-nav__link">
    <span class="md-ellipsis">
      
        Institute &amp; Researcher
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Institute &amp; Researcher">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mit" class="md-nav__link">
    <span class="md-ellipsis">
      
        MIT
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stanford" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stanford
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#princeton" class="md-nav__link">
    <span class="md-ellipsis">
      
        Princeton
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#harvard" class="md-nav__link">
    <span class="md-ellipsis">
      
        Harvard
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ucla" class="md-nav__link">
    <span class="md-ellipsis">
      
        UCLA
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#uc-berkeley" class="md-nav__link">
    <span class="md-ellipsis">
      
        UC Berkeley
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bnu" class="md-nav__link">
    <span class="md-ellipsis">
      
        BNU
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pku" class="md-nav__link">
    <span class="md-ellipsis">
      
        PKU
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ucsd" class="md-nav__link">
    <span class="md-ellipsis">
      
        UCSD
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nyu" class="md-nav__link">
    <span class="md-ellipsis">
      
        NYU
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jhu" class="md-nav__link">
    <span class="md-ellipsis">
      
        JHU
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sit" class="md-nav__link">
    <span class="md-ellipsis">
      
        SIT
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#people-book" class="md-nav__link">
    <span class="md-ellipsis">
      
        People &amp; Book
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="People &amp; Book">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#john-hopcroft" class="md-nav__link">
    <span class="md-ellipsis">
      
        John Hopcroft
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ulf-grenander" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ulf Grenander
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#david-marr" class="md-nav__link">
    <span class="md-ellipsis">
      
        David Marr
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#michael-tomasello" class="md-nav__link">
    <span class="md-ellipsis">
      
        Michael Tomasello
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#judea-pearl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Judea Pearl
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#susan-carey" class="md-nav__link">
    <span class="md-ellipsis">
      
        Susan Carey
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#daniel-kahneman" class="md-nav__link">
    <span class="md-ellipsis">
      
        Daniel Kahneman
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#karl-popper" class="md-nav__link">
    <span class="md-ellipsis">
      
        Karl Popper
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#about" class="md-nav__link">
    <span class="md-ellipsis">
      
        About
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Awesome agi cocosci</h1>

<div class="github-widget" data-repo="YuzheSHI/awesome-agi-cocosci"></div>
<div align="center">
    <img width="400" height="253" src="https://raw.githubusercontent.com/YuzheSHI/awesome-agi-cocosci/master/assets/abd_map.png" alt="Roadmap of studying Abduction">
</div>

<h2 id="awesome-artificial-general-intelligence-and-computational-cognitive-sciences">Awesome Artificial General Intelligence and Computational Cognitive Sciences <a href="https://awesome.re"><img alt="Awesome" src="https://awesome.re/badge.svg" /></a><a class="headerlink" href="#awesome-artificial-general-intelligence-and-computational-cognitive-sciences" title="Permanent link">&para;</a></h2>
<p>An <strong>awesome &amp; curated</strong> list for <strong>Artificial General Intelligence</strong>, an emerging inter-discipline field that combines artificial intelligence and computational cognitive sciences as majority, alone with probability and statistics, formal logic, cognitive and developmental psychology, computational philosophy, cognitive neuroscience, and computational sociology. We are promoting high-level machine intelligence by getting inspirations from the way that human learns and thinks, while obtaining a deeper understanding of human cognition simultaneously. We believe that this kind of reciprocative research is a potential way towards our big picture: building human-level intelligent systems with capabilities such as abstracting, explaining, learning, planning, and making decisions. And such intelligence may generally help people improve scientific research, engineering, and the arts, which are the hallmarks of human intelligence.</p>
<p><strong><em>Awesome AGI &amp; CoCoSci</em></strong> is an all-in-one collection, consisting of recources from basic courses and tutorials, to papers and books around diverse topics in mutiple perspectives. Both junior and senior researchers, whether learning, working on, or working around AGI and CoCoSci, meet their interest here.</p>
<h2 id="contributing">Contributing<a class="headerlink" href="#contributing" title="Permanent link">&para;</a></h2>
<p>Contributions are greatly welcomed! Please refer to <a href="https://github.com/YuzheSHI/awesome-agi-cocosci/blob/master/Contributing.md">Contribution Guidelines</a> before taking any actions.</p>
<p><span id = "c"></span></p>
<div class="codehilite"><pre><span></span><code>  * [Quantitative Analysis](#quantitative-analysis) 
&lt;!--* [Tasks &amp; Environments](#te)--&gt;
</code></pre></div>

<h2 id="papers">Papers<a class="headerlink" href="#papers" title="Permanent link">&para;</a></h2>
<h3 id="abduction">Abduction<a class="headerlink" href="#abduction" title="Permanent link">&para;</a></h3>
<h4 id="explanation">Explanation<a class="headerlink" href="#explanation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/abduction/index.html">Abduction</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Abduction, one of the three thinking patterns besides Induction and Deduction, being unique for its potential to introduce new ideas into current knowledge.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/scientific-explanation/">Scientific Explanation</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Scientific Explanation, a canonical application of Abduction.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/scientific-reduction/">Scientific Reduction</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Scientific Reduction, which comes with no explicit boundary with Explanation.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-nonmonotonic/">Non-monotonic Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Non-monotonic Logic, a family of formal frameworks devised to capture and represent defeasible inference.</p>
</li>
<li>
<p><a href="https://4lib.org/book/702071/e8ffe8">Philosophical Writings of Peirce</a> - <strong><em>Courier Corporation</em></strong>, 1955. [<a href="https://scholar.google.com/scholar?cluster=3917019015464129592">All Versions</a>]. Original writings by C. S. Peirce, the philosopher who first introduces the concept of Abduction.</p>
</li>
<li>
<p><a href="https://www.hps.cam.ac.uk/files/lipton-inference.pdf">Inference to the Best Explanation</a> - <strong><em>Routledge</em></strong>, 1991. [<a href="https://scholar.google.com/scholar?cluster=5097986614430666854">All Versions</a>]. Lipton's original paper on Inference to the Best Explanation as a specialized condition of Abduction.</p>
</li>
<li>
<p><a href="https://link.springer.com/book/10.1007/978-94-017-1733-5">Abductive Reasoning and Learning</a> - <strong><em>Springer</em></strong>, 2000. [<a href="https://scholar.google.com/scholar?cluster=12074269365138058159">All Versions</a>]. This book contains leading survey papers on the various aspects of Abduction, both logical and numerical approaches.</p>
</li>
<li>
<p><a href="https://link.springer.com/book/10.1007%2F978-3-642-03631-6">Abductive Cognition: The Epistemological and Eco-Cognitive Dimensions of Hypothetical Reasoning</a> - <strong><em>Springer</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=8707351442527595188">All Versions</a>]. Most philosophers of science in the twentieth century have concluded that no logic of creative processes exists and, moreover, that a rational model of discovery is impossible. In short, scientific creative inferences are irrational and there is no “reasoning” to hypotheses. On the other hand, some research in the area of artificial intelligence has shown that methods for discovery could be found that are computationally adequate for rediscovering --- or discovering for the first time --- empirical or theoretical laws and theorems.</p>
</li>
<li>
<p><a href="https://cognition.princeton.edu/sites/default/files/cognition/files/explanation_abductive_inference.pdf">Explanation and Abductive Inference</a> - <strong><em>The Oxford Handbook of Thinking and Reasoning</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=16126850654692681562">All Versions</a>]. This chapter reviews evidence from cognitive psychology and cognitive development concerning the structure and function of explanations, with a focus on the role of explanations in learning and inference. The findings highlight the value of understanding explanation and abductive inference both as phenomena in their own right and for the insights they provide concerning foundational aspects of human cognition, such as representation, learning, and inference.</p>
</li>
<li>
<p><a href="https://www.cell.com/AJHG/fulltext/S1364-6613(06)00132-X">Probabilistic models of cognition: Conceptual foundations</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12857321660837478492">All Versions</a>]. Remarkable progress in the mathematics and computer science of probability has led to a revolution in the scope of probabilistic models. In particular, ‘sophisticated’ probabilistic methods apply to structured relational systems such as graphs and grammars, of immediate relevance to the cognitive sciences. This review outlines progress in this rapidly developing field, which provides a potentially unifying perspective across a wide range of domains and levels of explanation.</p>
</li>
<li>
<p><a href="https://cognition.princeton.edu/sites/default/files/cognition/files/tics_explanation.pdf">The structure and function of explanations</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=2849189270394400667">All Versions</a>]. Generating and evaluating explanations is spontaneous, ubiquitous and fundamental to our sense of understanding. Recent evidence suggests that in the course of an individual's reasoning, engaging in explanation can have profound effects on the probability assigned to causal claims, on how properties are generalized and on learning. These effects follow from two properties of the structure of explanations: explanations accommodate novel information in the context of prior beliefs, and do so in a way that fosters generalization.</p>
</li>
<li>
<p><a href="https://scholar.princeton.edu/sites/default/files/cognition/files/explanatory_prefs_tics.pdf">Explanatory Preferences Shape Learning and Inference</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=2040551538203889465">All Versions</a>]. People often learn by seeking explanations, and they assess the viability of hypotheses by considering how well they explain the data. An emerging body of work reveals that both children and adults have strong and systematic intuitions about what constitutes a good explanation, and that these explanatory preferences have a systematic impact on explanation-based processes. In particular, people favor explanations that are simple and broad, with the consequence that engaging in explanation can shape learning and inference by leading people to seek patterns and favor hypotheses that support broad and simple explanations.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010027715000955">The Role of Explanatory Considerations in Updating</a> - <strong><em>Cognition</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=3089358487428261042">All Versions</a>]. This paper investigates experimentally controversy in philosophy about the connection between explanation and inference, of whether judgments of the explanatory goodness of hypotheses do play a role when people revise their degrees of belief in those hypotheses upon the receipt of new evidence.</p>
</li>
<li>
<p><a href="https://www.tandfonline.com/doi/full/10.1080/20445911.2016.1230122">Explanation, updating, and accuracy</a> - <strong><em>Journal of Cognitive Psychology</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=967127146748155733">All Versions</a>]. There is evidence that people update their credences partly on the basis of explanatory considerations. Philosophers have recently argued that to minimise the inaccuracy of their credences, people's updates also ought to be partly based on such considerations. However, there are many ways in which explanatory considerations can factor into updating, not all of which minimise inaccuracy. It is an open question whether in their updating, people take explanatory considerations into account in a way that philosophers would deem recommendable. To address this question, the authors re-analyse data from an experiment reported in Douven and Schupbach, “The role of explanatory considerations in updating”.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2018-03972-001">Best, second-best, and good-enough explanations: How they matter to reasoning</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3067550385175104201">All Versions</a>]. There is a wealth of evidence that people’s reasoning is influenced by explanatory considerations. Three experiments investigate the descriptive adequacy of a precise proposal to be found in the philosophical literature, to wit, that we should infer to the best explanation, provided certain additional conditions are met. The main conslusions are that (a) the quality of an explanation is a good predictor of people’s willingness to accept that explanation, and a better predictor than the prior probability of the explanation, and (b) if more than one possible explanation is given, people are the less willing to infer the best explanation the better they deem the second-best explanation.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001790">How explanation guides belief change</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15240531165875981526">All Versions</a>]. Philosophers have argued that people ought to change their graded beliefs via Bayes’ rule. Recent work in psychology indicates that people sometimes violate that rule by attending to explanatory factors. Results from computational modeling suggest that such violations may actually be rational.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2506_2">Use of current explanations in multicausal abductive reasoning</a> - <strong><em>Cognitive Science</em></strong>, 2001. [<a href="https://scholar.google.com/scholar?cluster=7816050625957759346&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11229-007-9223-4">Patterns of abduction</a> - <strong><em>Synthese</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=15230540023076470385&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A categorization for Abduction in the account of pure philosophy.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1570868314000895">Abduction: A categorical characterization</a> - <strong><em>Journal of Applied Logic</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=17834260152484836885&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.journals.uchicago.edu/doi/abs/10.1086/392744">Defending Abduction</a> - <strong><em>Philosophy of Science</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=13895790050138832555&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11229-009-9709-3">On the distinction between Peirce's abduction and Lipton's Inference to the best explanation</a> - <strong><em>Synthese</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=7865291004729010145&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11229-019-02337-z">Abduction − the context of discovery + underdetermination = inference to the best explanation</a> - <strong><em>Synthese</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4261649938116694095&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F3-540-45004-1_14">Towards an Architecture for Cognitive Vision Using Qualitative Spatio-temporal Representations and Abduction</a> - <strong><em>Spatial Cognition</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=8072265283930278310&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11229-018-1824-6">Abductive inference within a pragmatic framework</a> - <strong><em>Synthese</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10285954503043361393&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s00354-019-00059-x">Disjunctive Abduction</a> - <strong><em>New Generation Computing</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6664745483675209831&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00459/full">Probabilistic alternatives to Bayesianism: the case of explanationism</a> - <strong><em>Frontiers in Psychology</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=9016714668469830914&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A non-Bayesian account of Abduction.</p>
</li>
<li>
<p><a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010195405620571">A Probabilistic Theory of Abductive Reasoning</a> - <strong><em>ICAART</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=450937566244876051&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A probabilistic perspective for interpreting Abductive Reasoning.</p>
</li>
<li>
<p><a href="https://www.tandfonline.com/doi/full/10.1080/09528130600558141?scroll=top&amp;needAccess=true">The order effect in human abductive reasoning: an empirical and computational study</a> - <strong><em>Journal of Experimental &amp; Theoretical Artificial Intelligence</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3803536062463585043&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F978-3-642-15223-8_5">Abduction, Induction, and Analogy</a> - <strong><em>Model-Based Reasoning in Science and Technology</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=14979764682921693390&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The distinctions and relations between Abduction, Induction, and Analogy.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027718301094">Remembrance of inferences past: Amortization in human hypothesis generation</a> - <strong><em>Cognition</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=190340622765037472&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. A rational account of human hypothesis generation.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01142.x">The AHA! Experience: Creativity Through Emergent Binding in Neural Networks</a> - <strong><em>Cognitive Science</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=10006889101167052798&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2352154620300851">Explanation-seeking curiosity in childhood</a> - <strong><em>Current Opinion in Behavioral Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4167956555501133663&amp;hl=en&amp;as_sdt=2005">All Versions</a>]. A piece of developmental pshchological evidence for Abduction in young children.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2204.14267">A Grammar of Hypotheses for Visualization, Data, and Analysis</a> - 2022. [<a href="https://scholar.google.com/scholar?cluster=10321469321980973246">All Versions</a>]. This work presents a grammar for expressing hypotheses in visual data analysis to formalize the previously abstract notion of "analysis tasks." Through the lens of this grammar, the authors lay the groundwork for how a user's data analysis questions can be operationalized and automated as a set of hypotheses (a hypothesis space). The authors demonstrate that the grammar-based approach for analysis tasks can provide a systematic method towards unifying three disparate spaces in visualization research: the hypotheses a dataset can express (a data hypothesis space), the hypotheses a user would like to refine or verify through analysis (an analysis hypothesis space), and the hypotheses a visualization design is capable of supporting (a visualization hypothesis space). The authors illustrate how the formalization of these three spaces can inform future research in visualization evaluation, knowledge elicitation, analytic provenance, and visualization recommendation by using a shared language for hypotheses. Finally, the authors compare the proposed grammar-based approach with existing visual analysis models and discuss the potential of a new hypothesis-driven theory of visual analytics. </p>
</li>
</ul>
<h4 id="scientific-discovery">Scientific Discovery<a class="headerlink" href="#scientific-discovery" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/scientific-discovery/">Scientific Discovery</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Scientific Discovery, the process or product of successful scientific inquiry, sometimes an Abduction-like (Explanation) thinking pattern.</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/2241843/c5d7b3?id=2241843&amp;secret=c5d7b3">Models of Discovery: And Other Topics in the Methods of Science</a> - <strong><em>Springer</em></strong>, 1977. [<a href="https://scholar.google.com/scholar?cluster=9932701864897299105&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original book on search as scientific thinking.</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/970300/6b0ff7?id=970300&amp;secret=6b0ff7">Scientific discovery: Computational explorations of the creative processes</a> - <strong><em>MIT Press</em></strong>, 1987. [<a href="https://scholar.google.com/scholar?cluster=11327000316248254911">All Versions</a>]. The book is divided into four parts. Part I introduces the subject of discovery, defines the scope of our work, and discusses some of the issues that have surrounded and still surround our topic. Parts II and III contain the main body of our results, largely in the form of accounts of the performance of computer programs that simulate human thought processes to make scientific discoveries. Part II is devoted largely to the processes for inducing quantitative theories from data. Part III is devoted mainly to the processes for inducing qualitative descriptive and structural theories from data. In Part IV, on the basis of our experience, we discuss at a lower level of precision how the programs described in the preceding chapters could be combined into a single, more general discovery system, and we describe a wide range of the other component processes that enter into scientific discovery. </p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1201_1">Dual Space Search During Scientific Reasoning</a> - <strong><em>Cognitive Science</em></strong>, 1988. [<a href="https://scholar.google.com/scholar?cluster=17542852673494089523&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. The original paper on the dual space search as scientific thinking theory.</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/94n547fj">Complexity Management in a Discovery Task</a> - <strong><em>CogSci'92</em></strong>, 1992. [<a href="https://scholar.google.com/scholar?cluster=18138712608977258974">All Versions</a>]. Previous psychological research about scientific discovery has often focused on subjects' heuristics for discovering simple concepts with one relevant dimension or a few relevant dimensions with simple two-way interactions. This paper presents results from an experiment in which subjects had to discover a concept involving complex three-way interactions on a multi-valued output by running experiments in a computerized microworld. Twenty-two CMU undergraduates attempted the task, of which sixteen succeeded, in an average of 85 minutes. The analyses focus on three strategies used to regulate task complexity. First, subjects preferred depth-first to breadth-first search, with successful subjects regulating the number of features varied from experiment to experiment most effectively. Second, subjects systematically regulated the length of their experiments. Third, a new explicit search heuristic (Put Upon Stack Heuristic) used by successful subjects is described.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1071581996900324">A dual-space model of iteratively deepening exploratory learning</a> - <strong><em>International Journal of Human-Computer Studies</em></strong>, 1996. [<a href="https://scholar.google.com/scholar?cluster=17337189265334825678">All Versions</a>]. This paper describes a cognitive model of exploratory learning, which covers both trial-and-error and instruction-taking activities. The model, implemented in Soar, is grounded in empirical data of subjects in a task-oriented, trial-and-error exploratory learning situation. A key empirical finding reflected in the model is the repeated scanning of a subset of the available menu items, with increased attention to items on each successive scan. This is explained in terms of dual search spaces, the external interface and the user's internal knowledge, both of which must be tentatively explored with attention to changing costs and benefits.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0010028583710030">Heuristics for Scientific Experimentation: A Developmental Study</a> - <strong><em>Cognitive Psychology</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=2469515962071844494&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. A piece of evidence on children have basic scientific thinking skills.</p>
</li>
<li>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.645.248&amp;rep=rep1&amp;type=pdf">A 4-Space Model of Scientific Discovery</a> - <strong><em>CogSci'95</em></strong>, 1995. [<a href="https://scholar.google.com/scholar?cluster=1063157789682040473&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. Extending the dual space search.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.3758/BF03201090">When to trust the data: Further investigations of system error in a scientific reasoning task</a> - <strong><em>Memory &amp; Cognition</em></strong>, 1996. [<a href="https://scholar.google.com/scholar?cluster=3131191372086488656">All Versions</a>]. When evaluating experimental evidence, how do people deal with the possibility that some of the feedback is erroneous? The potential for error means that evidence evaluation must include decisions about when to “trust the data.” This paper presents two studies that focus on subjects’ responses to erroneous feedback in a hypothesis testing situation—a variant of Wason’s (1960) 2–4–6 rule discovery task in which some feedback was subject tosystem error: “hits” were reported as “misses” and vice versa. Results show that, in contrast to previous research, people are equally adept at identifying false negatives and false positives; further, successful subjects were less likely to use a positive test strategy (Klayman &amp; Ha, 1987) than were unsuccessful subjects. Finally, although others have found that generating possible hypotheses prior to experimentation increases success and task efficiency, such a manipulation did little to mitigate the effects of system error.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1987-20689-001">Confirmation, disconfirmation, and information in hypothesis testing</a> - <strong><em>Psychological Review</em></strong>, 1987. [<a href="https://scholar.google.com/scholar?cluster=1954141597807453515&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A psychological account on hypothesis testing.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2010-22980-001">Hypothesis generation, sparse categories, and the positive test strategy</a> - <strong><em>Psychological Review</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=4329636480235863472&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1990-03504-001">Children and adults as intuitive scientists</a> - <strong><em>Psychological Review</em></strong>, 1989. [<a href="https://scholar.google.com/scholar?cluster=9577945454476127070&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. A perspective against search as scientific thinking.</p>
</li>
</ul>
<h4 id="rationalization">Rationalization<a class="headerlink" href="#rationalization" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0885201414000744">Imagination and the generation of new ideas</a> - <strong><em>Cognitive Development</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=16920774374067505248">All Versions</a>]. A variety of theories have been put forth to explain the function of imagination, most notably that imagination engages and develops children's theory of mind and counterfactual reasoning. This work proposes that a primary role for imagination is as a cognitive mechanism for efficiently generating new ideas without observing new evidence. Learners must generate hypotheses before they can assess the truth of these hypotheses. Given infinite possibilities, how do learners constrain the process of hypothesis generation? The authors suggest that learners represent abstract criteria for the solution to a problem and generate solutions that, if true, would solve the problem. As a preliminary test of this idea, the authors show that, in the absence of any fact of the matter (i.e., when neither prior knowledge nor statistical data distinguishes competing hypotheses), 4–6-year-olds (mean: 63 months) systematically converge on solutions to problems, consistent with an ability to imagine the abstract properties of causal problems and their solutions.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661319302311">How We Know What Not To Think</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13106919756521743226">All Versions</a>]. Humans often represent and reason about unrealized possible actions---the vast infinity of things that were not (or have not yet been) chosen. This capacity is central to the most impressive of human abilities: causal reasoning, planning, linguistic communication, moral judgment, etc. Nevertheless, how do we select possible actions that are worth considering from the infinity of unrealized actions that are better left ignored? This work reviews research across the cognitive sciences, and find that the possible actions considered by default are those that are both likely to occur and generally valuable. This paper then offers a unified theory of why. The authors propose that (i) across diverse cognitive tasks, the possible actions we consider are biased towards those of general practical utility, and (ii) a plausible primary function for this mechanism resides in decision making.</p>
</li>
<li>
<p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/rationalization-is-rational/2A13B99ED09BD802C0924D3681FEC55B">Rationalization is rational</a> - <strong><em>Behavioral and Brain Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5165464589274056844">All Versions</a>]. [<a href="https://bpb-us-e1.wpmucdn.com/websites.harvard.edu/dist/0/59/files/2022/03/rationalization_is_rational.pdf">Preprint</a>]. Rationalization occurs when a person has performed an action and then concocts the beliefs and desires that would have made it rational. Then, people often adjust their own beliefs and desires to match the concocted ones. While many studies demonstrate rationalization, and a few theories describe its underlying cognitive mechanisms, we have little understanding of its function. Why is the mind designed to construct post hoc rationalizations of its behavior, and then to adopt them? This may accomplish an important task: transferring information between the different kinds of processes and representations that influence our behavior. Human decision making does not rely on a single process; it is influenced by reason, habit, instinct, norms, and so on. Several of these influences are not organized according to rational choice (i.e., computing and maximizing expected value). Rationalization extracts implicit information – true beliefs and useful desires – from the influence of these non-rational systems on behavior. </p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001480">Rationalizing constraints on the capacity for cognitive control</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=13060297961922073063">All Versions</a>]. Humans are remarkably limited in: (i) how many control-dependent tasks they can execute simultaneously, and (ii) how intensely they can focus on a single task. These limitations are universal assumptions of most theories of cognition. Yet, a rationale for why humans are subject to these constraints remains elusive. This feature review draws on recent insights from psychology, neuroscience, and machine learning, to suggest that constraints on cognitive control may result from a rational adaptation to fundamental, computational dilemmas in neural architectures. The reviewed literature implies that limitations in multitasking may result from a trade-off between learning efficacy and processing efficiency and that limitations in the intensity of commitment to a single task may reflect a trade-off between cognitive stability and flexibility.</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/5f64z7d7">Coalescing the Vapors of Human Experience into a Viable and Meaningful Comprehension</a> - <strong><em>CogSci'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=5460385008324352958">All Versions</a>]. Models of concept learning and theory acquisition often invoke a stochastic search process, in which learners generate hypotheses through some structured random process and thenevaluate them on some data measuring their quality or value. To be successful within a reasonable time-frame, these models need ways of generating good candidate hypotheses evenbefore the data are considered. Schulz (2012a) has proposed that studying the origins of new ideas in more everyday contexts, such as how we think up new names for things, can provide insight into the cognitive processes that generate good hypotheses for learning. We propose a simple generative model for how people might draw on their experience to propose new names in everyday domains such as pub names or action movies, and show that it captures surprisingly well the names that people actually imagine. We discuss the role for an analogous hypothesis-generation mechanism in enabling and constraining causal theory learning.</p>
</li>
</ul>
<h4 id="applications-in-ai">Applications in AI<a class="headerlink" href="#applications-in-ai" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.nature.com/articles/nature02236">Functional genomic hypothesis generation and experimentation by a robot scientist</a> - <strong><em>Nature</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17461972625475533182">All Versions</a>]. This paper describes a physically implemented robotic system that applies techniques from artificial intelligence to carry out cycles of scientific experimentation. The system automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle. The system is applied to the determination of gene function using deletion mutants of yeast (Saccharomyces cerevisiae) and auxotrophic growth experiments. The authors built and tested a detailed logical model (involving genes, proteins and metabolites) of the aromatic amino acid synthesis pathway.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370293900154?via%3Dihub">Interpretation as abduction</a> - <strong><em>Artificial Intelligence</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=12658433318211361322">All Versions</a>]. Abduction is inference to the best explanation. The authors have developed an approach to abductive inference, called “weighted abduction”, that has resulted in a significant simplification of how the problem of interpreting texts is conceptualized. The interpretation of a text is the minimal explanation of why the text would be true. More precisely, to interpret a text, one must prove the logical form of the text from what is already mutually known, allowing for coercions, merging redundancies where possible, and making assumptions where necessary. It is shown how such “local pragmatics” problems as reference resolution, the interpretation of compound nominals, the resolution of syntactic ambiguity and metonymy, and schema recognition can be solved in this manner. Moreover, this approach of “interpretation as abduction” can be combined with the older view of “parsing as deduction” to produce an elegant and thorough integration of syntax, semantics, and pragmatics, one that spans the range of linguistic phenomena from phonology to discourse structure.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/000437029390061F?via%3Dihub">Probabilistic Horn abduction and Bayesian networks</a> - <strong><em>Artificial Intelligence</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=7728248035489349629">All Versions</a>]. This paper presents a simple framework for Horn-clause abduction, with probabilities associated with hypotheses. The framework incorporates assumptions about the rule base and independence assumptions amongst hypotheses. It is shown how any probabilistic knowledge representable in a discrete Bayesian belief network can be represented in this framework. The main contribution is in finding a relationship between logical and probabilistic notions of evidential reasoning. This provides a useful representation language in its own right, providing a compromise between heuristic and epistemic adequacy.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-540-39879-0_6">Abductive Inference in Bayesian Networks: A Review</a> - <strong><em>Advances in Bayesian Networks</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=8502276402734843212">All Versions</a>]. The goal of this paper is to serve as a survey for the problem of abductive inference (or belief revision) in Bayesian networks. Thus, the problem is introduced in its two variants: total abduction (or MPE) and partial abduction (or MAP) . Also, the problem is formulated in its general case, that is, looking for the K best explanations. Then, a (non exhaustive) review of exact and approximate algorithms for dealing with both abductive inference problems is carried out. Finally, the authors collect the main complexity results appeared in the literature for both problems (MPE and MAP).</p>
</li>
<li>
<p><a href="https://academic.oup.com/logcom/article-abstract/2/6/719/942121">Abductive Logic Programming</a> - <strong><em>Journal of Logic Computation</em></strong>, 1992. [<a href="https://scholar.google.com/scholar?cluster=18119357517656745518">All Versions</a>]. This paper is a survey and critical overview of recent work on the extension of logic programming to perform abductive reasoning (abductive logic programming). The authors outline the general framework of abduction and its applications to knowledge assimilation and default reasoning; and they introduce an argumentation-theoretic approach to the use of abduction as an interpretation for negation as failure. </p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0743106699000758">ACLP: Abductive Constraint Logic Programming</a> - <strong><em>The Journal of Logic Programming</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=14319574550421192429">All Versions</a>]. This paper presents the framework of Abductive Constraint Logic Programming (ACLP), which integrates Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving. This integration of constraint solving into abductive reasoning facilitates a general form of constructive abduction and enables the application of abduction to computationally demanding problems. The paper studies the formal declarative and operational semantics of the ACLP framework together with its application to various problems.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/3-540-45628-7_16">Abduction in Logic Programming</a> - <strong><em>Computational Logic</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=902643678163312237">All Versions</a>]. [<a href="https://web.stanford.edu/class/cs227/Readings/Abudction%20in%20LP.pdf">Preprint</a>]. Abduction in Logic Programming started in the late 80s, early 90s, in an attempt to extend logic programming into a framework suitable for a variety of problems in Artificial Intelligence and other areas of Computer Science. This paper aims to chart out the main developments of the field over the last ten years and to take a critical view of these developments from several perspectives: logical, epistemological, computational and suitability to application. The paper attempts to expose some of the challenges and prospects for the further development of the field.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.5555/2283696.2283887">Bayesian Abductive Logic Programs: A Probabilistic Logic for Abductive Reasoning</a> - <strong><em>IJCAI'11</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=4453424083730209198">All Versions</a>]. [<a href="https://www.cs.utexas.edu/~ml/papers/raghavan.starai10.pdf">Preprint</a>]. This work introduces Bayesian Abductive Logic Programs (BALP), a probabilistic logic that adapts Bayesian Logic Programs (BLPs) for abductive reasoning. Like BLPs, BALPs also combine first-order logic and Bayes nets. However, unlike BLPs, which use deduction to construct Bayes nets, BALPs employ logical abduction. As a result, BALPs are more suited for problems like plan/activity recognition that require abductive reasoning.</p>
</li>
<li>
<p><a href="https://www.cs.utexas.edu/~ml/papers/raghavan.ecml11.pdf">Abductive Plan Recognition by Extending Bayesian Logic Programs</a> - <strong><em>ECML'11</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=7276511797197017483">All Versions</a>]. Plan recognition is the task of predicting an agent’s top-level plans based on its observed actions. It is an abductive reasoning task that involves inferring cause from effect. Most existing approaches to plan recognition use either first-order logic or probabilistic graphical models. While the former cannot handle uncertainty, the latter cannot handle structured representations. In order to overcome these limitations, this work develops an approach to plan recognition using Bayesian Logic Programs (BLPs), which combine first-order logic and Bayesian networks. Since BLPs employ logical deduction to construct the networks, they cannot be used effectively for plan recognition. Therefore, the authors extend BLPs to use logical abduction to construct Bayesian networks and call the resulting model Bayesian Abductive Logic Programs (BALPs). The authors learn the parameters in BALPs using the Expectation Maximization algorithm adapted for BLPs. Finally, the authors present an experimental evaluation of BALPs on three benchmark data sets and compare its performance with the state-of-the-art for plan recognition.</p>
</li>
<li>
<p><a href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6624/6619">An Approach to Abductive Reasoning in Equational Logic</a> - <strong><em>IJCAI'13</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=686895264429811190">All Versions</a>]. Abduction has been extensively studied in propositional logic because of its many applications in artificial intelligence. However, its intrinsic complexity has been a limitation to the implementation of abductive reasoning tools in more expressive logics. The authors have devised such a tool in ground flat equational logic, in which literals are equations or disequations between constants. The tool is based on the computation of prime implicates. It uses a relaxed paramodulation calculus, designed to generate all prime implicates of a formula, together with a carefully defined data structure storing the implicates and able to efficiently detect, and remove, redundancies.</p>
</li>
<li>
<p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/3964">Abduction-Based Explanations for Machine Learning Models</a> - <strong><em>AAAI'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7355960657107994022">All Versions</a>]. The growing range of applications of Machine Learning (ML) in a multitude of settings motivates the ability of computing small explanations for predictions made. Small explanations are generally accepted as easier for human decision makers to understand. Most earlier work on computing explanations is based on heuristic approaches, providing no guarantees of quality, in terms of how close such solutions are from cardinality- or subset-minimal explanations. This paper develops a constraint-agnostic solution for computing explanations for any ML model. The proposed solution exploits abductive reasoning, and imposes the requirement that the ML model can be represented as sets of constraints using some target constraint reasoning system for which the decision problem can be answered with some oracle. The experimental results, obtained on well-known datasets, validate the scalability of the proposed approach as well as the quality of the computed solutions.</p>
</li>
<li>
<p><a href="https://www.ijcai.org/proceedings/2021/0424.pdf">Probabilistic Sufficient Explanations</a> - <strong><em>IJCAI'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1874102360688341104">All Versions</a>]. Understanding the behavior of learned classifiers is an important task, and various black-box explanations, logical reasoning approaches, and model-specific methods have been proposed. This paper introduces probabilistic sufficient explanations, which formulate explaining an instance of classification as choosing the "simplest" subset of features such that only observing those features is "sufficient" to explain the classification. That is, sufficient to give us strong probabilistic guarantees that the model will behave similarly when all features are observed under the data distribution. In addition, the authors leverage tractable probabilistic reasoning tools such as probabilistic circuits and expected predictions to design a scalable algorithm for finding the desired explanations while keeping the guarantees intact. The experiments demonstrate the effectiveness of the algorithm in finding sufficient explanations, and showcase its advantages compared to Anchors and logical explanations. </p>
</li>
<li>
<p><a href="https://www.aclweb.org/anthology/H91-1024.pdf">Machine Translation Using Abductive Inference</a> - <strong><em>COLING</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?cluster=15275163177548183539">All Versions</a>]. Many existing approaches to machine translation take for granted that the information presented in the output is found somewhere in the input, and, moreover, that such information should be expressed at a single representational level, say, in terms of the parse trees or of "semantic" assertions. Languages, however, not only express the equivalent information by drastically different linguistic means, but also often disagree in what distinctions should be expressed linguistically at all. For example, in translating from Japanese to English, it is often necessary to supply determiners for noun phrases, and this in general cannot be done without deep understanding of the source text. Similarly, in translating from English to Japanese, politeness considerations, which in English are implicit in the social situation and explicit in very diffuse ways in, for example, the heavy use of hypotheticals, must be realized grammatically in Japanese. Machine translation therefore requires that the appropriate inferences be drawn and that the text be interpreted to some depth. Recently, an elegant approach to inference in discourse interpretation has been developed at a number of sites, all based on the notion of abduction, and the authors have begun to explore its potential application to machine translation. The authors argue that this approach provides the possibility of deep reasoning and of mapping between the languages at a variety of levels.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2105.07758">Automated Biodesign Engineering by Abductive Meta-Interpretive Learning</a> - <strong><em>AAAI Spring Symposium Series 2021 on Artificial Intelligence for Synthetic Biology</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=543730388062329581">All Versions</a>]. This work proposes an automated biodesign engineering framework empowered by Abductive Meta-Interpretive Learning (MetaAbd), a novel machine learning approach that combines symbolic and sub-symbolic machine learning, to further enhance the design-build-test-learn cycle by enabling the learning machine to 1) exploit domain knowledge and learn human-interpretable models that are expressed by formal languages such as first-order logic; 2) simultaneously optimise the structure and parameters of the models to make accurate numerical predictions; 3) reduce the cost of experiments and effort on data annotation by actively generating hypotheses and examples.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2308.12740">Human Comprehensible Active Learning of Genome-Scale Metabolic Networks</a> - <strong><em>AAAI Spring Symposium Series 2023 on Computational Scientific Discovery</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10875437066608527790">All Versions</a>]. [<a href="http://cogsys.org/symposium/discovery-2023/abstracts/Abstract_3169.pdf">Extended Abstract</a>]. [<a href="http://cogsys.org/symposium/discovery-2023/talks/Ai.pdf">Slides</a>]. This work introduces a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that reduce the experimental cost of learning gene functions in comparison to randomly selected experiments. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-022-00470-y">Automated causal inference in application to randomized controlled clinical trials</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=7468801599737513065">All Versions</a>]. Randomized controlled trials (RCTs) are considered the gold standard for testing causal hypotheses in the clinical domain; however, the investigation of prognostic variables of patient outcome in a hypothesized cause–effect route is not feasible using standard statistical methods. This work proposes a new automated causal inference method (AutoCI) built on the invariant causal prediction (ICP) framework for the causal reinterpretation of clinical trial data. Compared with existing methods, the authors show that the proposed AutoCI allows one to clearly determine the causal variables of two real-world RCTs of patients with endometrial cancer with mature outcome and extensive clinicopathological and molecular data. This is achieved via suppressing the causal probability of non-causal variables by a wide margin. In ablation studies, the authors further demonstrate that the assignment of causal probabilities by AutoCI remains consistent in the presence of confounders. In conclusion, these results confirm the robustness and feasibility of AutoCI for future applications in real-world clinical analysis.</p>
</li>
</ul>
<h3 id="bayesian-modeling">Bayesian Modeling<a class="headerlink" href="#bayesian-modeling" title="Permanent link">&para;</a></h3>
<h4 id="bayesian-induction">Bayesian Induction<a class="headerlink" href="#bayesian-induction" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/epistemology-bayesian/">Bayesian Epistemology</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on the nature of uncertainty modeling in Bayesian Epistemology.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/nature14541">Probabilistic machine learning and artificial intelligence</a> - <strong><em>Nature</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=1783282361269717744">All Versions</a>]. Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.</p>
</li>
<li>
<p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/generalization-similarity-and-bayesian-inference/595CAA321C9C56270C624057021DE77A">Generalization, similarity, and Bayesian inference</a> - <strong><em>Behavioral and Brain Sciences</em></strong>, 2001. [<a href="https://scholar.google.com/scholar?cluster=14074987155133342565">All Versions</a>]. [<a href="http://web.mit.edu/cocosci/archive/Papers/tenenbaum_griffiths01.pdf">Preprint</a>]. Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. The authors recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. This framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/1998/hash/d010396ca8abf6ead8cacc2c2f2f26c7-Abstract.html">Bayesian modeling of human concept learning</a> - <strong><em>NeurIPS'98</em></strong>, 1998. [<a href="https://scholar.google.com/scholar?cluster=3772493362518191863">All Versions</a>]. [<a href="http://web.mit.edu/cocosci/archive/Papers/bayes.pdf">Preprint</a>]. This work considers the problem of learning concepts from small numbers of positive examples, a feat which humans perform routinely but which computers are rarely capable of. Bridging machine learning and cognitive science perspectives, this work presents both theoretical analysis and an empirical study with human subjects for the simple task oflearning concepts corresponding to axis-aligned rectangles in a multidimensional feature space. Existing learning models, when applied to this task, cannot explain how subjects generalize from only a few examples of the concept. The author proposes a principled Bayesian model based on the assumption that the examples are a random sample from the concept to be learned. The model gives precise fits to human behavior on this simple task and provides qualitati ve insights into more complex, realistic cases of concept learning.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/1999/hash/86d7c8a08b4aaa1bc7c599473f5dddda-Abstract.html">Rules and Similarity in Concept Learning</a> - <strong><em>NeurIPS'99</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=10968021160883668417">All Versions</a>]. [<a href="http://web.mit.edu/cocosci/archive/Papers/nips99preprint.pdf">Preprint</a>]. This paper argues that two apparently distinct modes of generalizing concepts - abstracting rules and computing similarity to exemplars - should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes - which rules are abstracted, how similarity is measured - as well as why generalization should appear rule- or similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.</p>
</li>
<li>
<p><a href="https://www.cell.com/AJHG/fulltext/S1364-6613(06)00134-3">Theory-based Bayesian models of inductive learning and reasoning</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=6741344960992898446">All Versions</a>]. [<a href="http://www.charleskemp.com/papers/TenenbaumGK06.pdf">Preprint</a>]. Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. This paper argues that both components are necessary to explain the nature, use and acquisition of human knowledge, and the authors introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations. </p>
</li>
<li>
<p><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.114.2.245">Word learning as Bayesian inference</a> - <strong><em>Psychological Review</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=5476233692839102256">All Versions</a>]. [<a href="https://tallinzen.net/media/readings/xu_tenenbaum_2007.pdf">Preprint</a>]. The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.1192788">How to Grow a Mind: Statistics, Structure, and Abstraction</a> - <strong><em>Science</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=2667398573353002097">All Versions</a>]. [<a href="https://cocosci.princeton.edu/tom/papers/growamind.pdf">Preprint</a>]. This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.aab3050">Human-level concept learning through probabilistic program induction</a> - <strong><em>Science</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=11844685101409624506">All Versions</a>]. [<a href="https://ai6034.mit.edu/wiki/images/LakeDec2015.pdf">Preprint</a>]. [<a href="https://cims.nyu.edu/~brenden/LakeEtAl2015Science_supp.pdf">Supplementary Material</a>]. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. This work presents a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.</p>
</li>
<li>
<p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">Building Machines That Learn and Think Like People</a> - <strong><em>Behavioral and Brain Sciences</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=8504723689348856287">All Versions</a>]. [<a href="https://leylaroksancaglar.github.io/Caglar_Hanson_2017.pdf">Preprint</a>]. Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. The authors review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, the authors argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. The authors suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-024-01991-9">Building machines that learn and think with people</a> - <strong><em>Nature Human Behavior</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=4420595706578245444">All Versions</a>]. [<a href="https://arxiv.org/abs/2408.03943">Preprint</a>]. This perspective shows how the science of collaborative cognition can be put to work to engineer systems that really can be called ‘thought partners’, systems built to meet humans' expectations and complement humans' limitations. The authors lay out several modes of collaborative thought in which humans and artificial intelligence thought partners can engage, and they propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, this work motivates an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the constructed partners actively build and reason over models of the human and world.</p>
</li>
<li>
<p><a href="http://web.mit.edu/cocosci/archive/Papers/cogsci01_final.pdf">The rational basis of representativeness</a> - <strong><em>CogSci'01</em></strong>, 2001. [<a href="https://scholar.google.com/scholar?cluster=11464039134248091466&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2011/hash/2c89109d42178de8a367c0228f169bf8-Abstract.html">Testing a Bayesian Measure of Representativeness Using a Large Image Database</a> - <strong><em>NeurIPS'11</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=8576570792794301292&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/tom/papers/abbott_cogsci2012_wordnet.pdf">Constructing a hypothesis space from the Web for large-scale Bayesian word learning</a> - <strong><em>CogSci'12</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9266416266046851766&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/sciadv.adg2488">Human-level few-shot concept induction through minimax entropy learning</a> - <strong><em>Science Advances</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?&amp;cluster=9084477652494351940">All Versions</a>]. This paper introduces a computational model designed to emulate human inductive reasoning on abstract reasoning tasks, such as those in IQ tests, using a minimax entropy approach. This method combines identifying the most effective constraints on data via minimum entropy with determining the best combination of them via maximum entropy.</p>
</li>
</ul>
<h4 id="generative-model">Generative Model<a class="headerlink" href="#generative-model" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://github.com/YuzheSHI/generative-modeling-explained">Generative Modeling Explained</a> - <strong><em>Statistical Machine Learning Tutorials</em></strong>, 2022. This tutorial on generative modeling is in part of Statistical Machine Learning Tutorial by Ying Nian Wu at UCLA Statistics. The tutorial goes over the key equations and algorithms for learning recent generative models, including energy-based models, diffusion/score-based models, autoregressive/flow-based models, VAEs, and GANs, and explains the connections between these models.</p>
</li>
<li>
<p><a href="https://www.taylorfrancis.com/books/mono/10.1201/9780429258411/bayesian-data-analysis-andrew-gelman-donald-rubin-john-carlin-hal-stern">Bayesian Data Analysis</a> - <strong><em>Chapman and Hall/CRC</em></strong>, 1995. [<a href="https://scholar.google.com/scholar?cluster=5067275302121330689&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Don Rubin's introductory book on Bayesian models.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1023/A:1007925832420">Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling</a> - <strong><em>International Journal of Computer Vision</em></strong>, 1998. [<a href="https://scholar.google.com/scholar?cluster=11604954524863138240">All Versions</a>]. [<a href="https://dash.harvard.edu/bitstream/handle/1/3637117/Mumford_FRAME.pdf?sequence=1">Preprint</a>]. This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. The theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.</p>
</li>
<li>
<p><a href="https://www.ams.org/journals/qam/2019-77-02/S0033-569X-2018-01528-5/home.html">A tale of three probabilistic families: Discriminative, descriptive, and generative models</a> - <strong><em>Quarterly of Applied Mathematics</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6129609629126793774">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~ywu/QAM2018.pdf">Preprint</a>]. The pattern theory of Grenander is a mathematical framework where patterns are represented by probability models on random variables of algebraic structures. In this paper, the authors review three families of probability models, namely, the discriminative models, the descriptive models, and the generative models. A discriminative model is in the form of a classifier. It specifies the conditional probability of the class label given the input signal. A descriptive model specifies the probability distribution of the signal, based on an energy function defined on the signal. A generative model assumes that the signal is generated by some latent variables via a transformation. The authors shall review these models within a common framework and explore their connections, and shall also review the recent developments that take advantage of the high approximation capacities of deep neural networks. </p>
</li>
<li>
<p><a href="https://www.jstor.org/stable/43638808?seq=1">From information scaling of natural images to regimes of statistical models</a> - <strong><em>Quarterly of Applied Mathematics</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=17387130978932998303">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~sczhu/papers/Quarterly_final.pdf">Preprint</a>].  One fundamental property of natural image data that distinguishes vision from other sensory tasks such as speech recognition is that scale plays a profound role in image formation and interpretation. Specifically, visual objects can appear at a wide range of scales in the images due to the change of viewing distance as well as camera resolution. The same objects appearing at different scales produce different image data with different statistical properties. In particular, this work shows that the entropy rate of the image data changes over scale. Moreover, the inferential uncertainty changes over scale too. The authors call these changes information scaling. They then examine both empirically and theoretically two prominent and yet largely isolated classes of image models, namely, wavelet sparse coding models and Markov random field models. The results indicate that the two classes of models are appropriate for two different entropy regimes: sparse coding targets low entropy regimes, whereas Markov random fields are appropriate for high entropy regimes. Because information scaling connects different entropy regimes, both sparse coding and Markov random fields are necessary for representing natural image data, and information scaling triggers transitions between these two regimes. </p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v48/xiec16.html">A Theory of Generative ConvNet</a> - <strong><em>ICML'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=11062907630625111054">All Versions</a>]. The authors show that a generative random field model, which they call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. For a further assumption that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then a generative ConvNet model that is unique among energy-based models is obtained: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. </p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/8519332">Cooperative Training of Descriptor and Generator Networks</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=18202808849093155435">All Versions</a>]. This paper studies the cooperative training of two generative models for image modeling and synthesis. Both models are parametrized by convolutional neural networks (ConvNets). The first model is a deep energy-based model, whose energy function is defined by a bottom-up ConvNet, which maps the observed image to the energy. We call it the descriptor network. The second model is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed image. The maximum likelihood learning algorithms of both models involve MCMC sampling such as Langevin dynamics. This work observes that the two learning algorithms can be seamlessly interwoven into a cooperative learning algorithm that can train both models simultaneously. Specifically, within each iteration of the cooperative learning algorithm, the generator model generates initial synthesized examples to initialize a finite-step MCMC that samples and trains the energy-based descriptor model. After that, the generator model learns from how the MCMC changes its synthesized examples. That is, the descriptor model teaches the generator model by MCMC, so that the generator model accumulates the MCMC transitions and reproduces them by direct ancestral sampling.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/hash/fa3060edb66e6ff4507886f9912e1ab9-Abstract.html">Learning Latent Space Energy-Based Prior Model</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=9945264852135249894">All Versions</a>]. [<a href="https://bpucla.github.io/latent-space-ebm-prior-project/">Project</a>]. [<a href="https://github.com/bpucla/latent-space-EBM-prior">Code</a>]. A milestone paper on Latent Energy-Based Model.</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=v_1Soh8QUNc">Learning Energy-Based Models by Diffusion Recovery Likelihood</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=4399294843209736764">All Versions</a>]. [<a href="https://github.com/ruiqigao/recovery_likelihood">Code</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=PxTIG12RRHS&amp;utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Score-Based Generative Modeling through Stochastic Differential Equations</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14592788616550656262">All Versions</a>].</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v119/li20i.html">Latent Space Factorisation and Manipulation via Matrix Subspace Projection</a> - <strong><em>ICML'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9592355331559392684">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/6796444">Minimax entropy principle and its application to texture modeling</a> - <strong><em>Neural Computing</em></strong>, 1997. [<a href="https://scholar.google.com/scholar?cluster=407872717119429940">All Versions</a>]. [<a href="https://www.dam.brown.edu/people/mumford/vision/papers/1997e--MinimaxEntropy-NC.pdf">Preprint</a>]. This article proposes a general theory and methodology, called the minimax entropy principle, for building statistical models for images (or signals) in a variety of applications. This principle consists of two parts. The first is the maximum entropy principle for feature binding (or fusion): for a given set of observed feature statistics, a distribution can be built to bind these feature statistics together by maximizing the entropy over all distributions that reproduce them. The second part is the minimum entropy principle for feature selection: among all plausible sets of feature statistics, we choose the set whose maximum entropy distribution has the minimum entropy. Computational and inferential issues in both parts are addressed; in particular, a feature pursuit procedure is proposed for approximately selecting the optimal set of features. The minimax entropy principle is then corrected by considering the sample variation in the observed feature statistics, and an information criterion for feature pursuit is derived. The minimax entropy principle is applied to texture modeling, where a novel Markov random field (MRF) model, called FRAME (filter, random field, and minimax entropy), is derived, and encouraging results are obtained in experiments on a variety of texture images.</p>
</li>
<li>
<p><a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473879">Parameter Expansion for Data Augmentation</a> - <strong><em>Journal of the American Statistical Association</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=15342818142955984734">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~ywu/research/papers/PXDA.pdf">Preprint</a>]. Viewing the observed data of a statistical model as incomplete and augmenting its missing parts are useful for clarifying concepts and central to the invention of two well-known statistical algorithms: expectation-maximization (EM) and data augmentation. Recently, the authors demonstrated that expanding the parameter space along with augmenting the missing data is useful for accelerating iterative computation in an EM algorithm. The main purpose of this article is to rigorously define a parameter expanded data augmentation (PX-DA) algorithm and to study its theoretical properties. The PX-DA is a special way of using auxiliary variables to accelerate Gibbs sampling algorithms and is closely related to reparameterization techniques.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/1000239">Image segmentation by data-driven markov chain monte carlo</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=3461400072144667491">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~sczhu/papers/DDMCMC_reprint.pdf">Preprint</a>]. This paper presents a computational paradigm called Data-Driven Markov Chain Monte Carlo (DDMCMC) for image segmentation in the Bayesian statistical framework. The paper contributes to image segmentation in four aspects. First, it designs efficient and well-balanced Markov Chain dynamics to explore the complex solution space and, thus, achieves a nearly global optimal solution independent of initial segmentations. Second, it presents a mathematical principle and a K-adventurers algorithm for computing multiple distinct solutions from the Markov chain sequence and, thus, it incorporates intrinsic ambiguities in image segmentation. Third, it utilizes data-driven (bottom-up) techniques, such as clustering and edge detection, to compute importance proposal probabilities, which drive the Markov chain dynamics and achieve tremendous speedup in comparison to the traditional jump-diffusion methods. Fourth, the DDMCMC paradigm provides a unifying framework in which the role of many existing segmentation algorithms, such as, edge detection, clustering, region growing, split-merge, snake/balloon, and region competition, are revealed as either realizing Markov chain dynamics or computing importance proposal probabilities. Thus, the DDMCMC paradigm combines and generalizes these segmentation methods in a principled way.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2006/file/87f4d79e36d68c3031ccf6c55e9bbd39-Paper.pdf">Efficient Learning of Sparse Representations with an Energy-Based Model</a> - <strong><em>NeurIPS'06</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=2247668190782691760">All Versions</a>].</p>
</li>
<li>
<p><a href="http://yann.lecun.com/exdb/publis/orig/lecun-06.pdf">A Tutorial on Energy-Based Learning</a> - <strong><em>Predicting Structured Data, MIT Press</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=8819502341081664768&amp;hl=en&amp;as_sdt=0,5">All Versiosn</a>]. Yann LeCun's tutorial on energy-based learning.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1511.06434">Unsupervised Representaton Learning with Deep Convolutional Generative Adversarial Networks</a> - <strong><em>ICLR'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3321343160055675528&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.jmlr.org/papers/v20/18-173.html">Analysis of Langevin Monte Carlo via Convex Optimization</a> - <strong><em>Journal of Machine Learning Research</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=5305860199396047317">All Versions</a>]. This paper provides new insights on the Unadjusted Langevin Algorithm. The authors show that this method can be formulated as the first order optimization algorithm for an objective functional defined on the Wasserstein space of order <span class="arithmatex"><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span>. Using this interpretation and techniques borrowed from convex optimization, the authors give a non-asymptotic analysis of this method to sample from log-concave smooth target distribution on <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^d</span><script type="math/tex">\mathbb{R}^d</script></span>. Based on this interpretation, the authors propose two new methods for sampling from a non-smooth target distribution. These new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm for largescale Bayesian inference. Using the optimization perspective, the authors provide non-asymptotic convergence analysis for the newly proposed methods. </p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.aag2612">A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs</a> - <strong><em>Science</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=1478382321633671444">All Versions</a>]. [<a href="https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2020/Lec22/GeorgeCAPCHAS.pdf">Preprint</a>]. Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, this work introduces a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010028516302766">Where do hypotheses come from?</a> - <strong><em>Cognitive Psychology</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=17480320046655923235">All Versions</a>]. [<a href="https://gershmanlab.com/pubs/Dasgupta17.pdf">Preprint</a>]. Why are human inferences sometimes remarkably close to the Bayesian ideal and other times systematically biased? In particular, why do humans make near-rational inferences in some natural domains where the candidate hypotheses are explicitly available, whereas tasks in similar domains requiring the self-generation of hypotheses produce systematic deviations from rational inference. This work proposes that these deviations arise from algorithmic processes approximating Bayes’ rule. Specifically in our account, hypotheses are generated stochastically from a sampling process, such that the sampled hypotheses form a Monte Carlo approximation of the posterior.</p>
</li>
</ul>
<h4 id="nonparametric-model">Nonparametric Model<a class="headerlink" href="#nonparametric-model" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.jstor.org/stable/2958008?seq=1">A Bayesian Analysis of Some Non-parametric Problems</a> - <strong><em>The Annals of Statistics</em></strong>, 1973. [<a href="https://scholar.google.com/scholar?cluster=3969163427460060902">All Versions</a>]. [<a href="https://people.stat.sc.edu/hansont/stat740/Ferguson1973.pdf">Preprint</a>]. A classic review on non-parametric problems.</p>
</li>
<li>
<p><a href="https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/antoniak.pdf">Mixtures of Dirichlet Process with Applications to Bayesian Nonparametric Problems</a> - <strong><em>The Annals of Statistics</em></strong>, 1974. [<a href="https://scholar.google.com/scholar?cluster=17937202534282344046&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on Dirichlet Process modeling for non-parametric problems.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0022000000917112">Latent Semantic Indexing: A Probabilistic Analysis</a> - <strong><em>Journal of Computer and System Sciences</em></strong>, 2000. [<a href="https://scholar.google.com/scholar?cluster=7296120469860429813&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on hierarchical topic model.</p>
</li>
<li>
<p><a href="https://projecteuclid.org/journals/statistical-science/volume-19/issue-1/Nonparametric-Bayesian-Data-Analysis/10.1214/088342304000000017.full">Nonparametric Bayesian Data Analysis</a> - <strong><em>Statistical Science</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=13476170780072319995">All Versions</a>]. This paper reviews the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each inference problem the authors review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Pólya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models and model validation with DP and Pólya tree extensions of parametric models. </p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2003/file/7b41bfa5085806dfa24b8c9de0ce567f-Paper.pdf">Hierarchical topic models and the nested Chinese restaurant process</a> - <strong><em>NeurIPS'03</em></strong>, 2003. [<a href="https://scholar.google.com/scholar?cluster=15040818675282958700&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper for nested Chinese restaurant process.</p>
</li>
<li>
<p><a href="https://www.aaai.org/Papers/AAAI/2006/AAAI06-061.pdf">Learning Systems of Concepts with an Infinite Relational Model</a> - <strong><em>AAAI'06</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3207350432755252565&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/1667053.1667056">The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies</a> - <strong><em>Journal of the ACM</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=8216933258869737505&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://mlg.eng.cam.ac.uk/zoubin/papers/ibptr.pdf">Infinite Latent Feature Models and the Indian Buffet Process</a> - <strong><em>Gatsby Computational Neuroscience Unit Technical Report 2005-001</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=13180738480564152907&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://jmlr.org/papers/v12/griffiths11a.html">The Indian Buffet Process: An Introduction and Review</a> - <strong><em>Journal of Machine Learning Research</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=6301314251995890943">All Versions</a>]. The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. This work gives a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. The authors then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes. </p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.5555/3020336.3020347">Nonparametric Bayesian Logic</a> - <strong><em>UAI'05</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=18267211625980322095">All Versions</a>]. [<a href="https://www.cs.ubc.ca/~nando/papers/npblog.pdf">Preprint</a>]. The Bayesian Logic (BLOG) language was recently developed for defining first-order probability models over worlds with unknown numbers of objects. It handles important problems in AI, including data association and population estimation. This paper extends BLOG by adopting generative processes over function spaces — known as nonparametrics in the Bayesian literature. This work introduces syntax for reasoning about arbitrary collections of objects, and their properties, in an intuitive manner. By exploiting exchangeability, distributions over unknown objects and their attributes are cast as Dirichlet processes, which resolve difficulties in model selection and inference caused by varying numbers of objects. </p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.5555/3020419.3020485">Infinite Hidden Relational Models</a> - <strong><em>UAI'06</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=2143172296528388141">All Versions</a>]. [<a href="https://www.dbs.ifi.lmu.de/~yu_k/uai06_relation.pdf">Preprint</a>]. Relational learning analyzes the probabilistic constraints between the attributes of entities and relationships. This work extends the expressiveness of relational models by introducing for each entity (or object) an infinite-dimensional latent variable as part of a Dirichlet process (DP) mixture model. This work discusses inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. The authors extended the Chinese restaurant process to be applicable to relational modeling. </p>
</li>
<li>
<p><a href="https://alchemy.cs.washington.edu/papers/kok07/kok07.pdf">Statistical Predicate Invention</a> - <strong><em>ICML'07</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=17009312281859401704">All Versions</a>]. This work proposes statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. This work proposes an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. The proposed approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate).</p>
</li>
</ul>
<h4 id="bayesian-optimization">Bayesian Optimization<a class="headerlink" href="#bayesian-optimization" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/7352306">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a> - <strong><em>Proceedings of the IEEE</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=2039456143890648437">All Versions</a>]. [<a href="https://www.cs.princeton.edu/~rpa/pubs/shahriari2016loop.pdf">Preprint</a>]. Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html">Practical Bayesian Optimization of Machine Learning Algorithms</a> - <strong><em>NeurIPS'12</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=14442949298925775705">All Versions</a>]. The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. This work considers this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). The authors show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. The authors describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. These proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1807.02811">A Tutorial on Bayesian Optimization</a> - 2018. [<a href="https://scholar.google.com/scholar?cluster=7971934771645047583">All Versions</a>]. Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. This tutorial describes how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. The authors then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. The authors conclude with a discussion of Bayesian optimization software and future research directions in the field. This tutorial provides a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications. </p>
</li>
<li>
<p><a href="https://www.cell.com/matter/fulltext/S2590-2385(24)00006-7">Human-in-the-loop for Bayesian autonomous materials phase mapping</a> - <strong><em>Matter</em></strong>. [<a href="https://scholar.google.com/scholar?cluster=1442913820064050211">All Versions</a>]. Autonomous experimentation achieves user objectives more efficiently than Edisonian studies by combining machine learning and laboratory automation to iteratively select and perform experiments. Integrating knowledge from theory, simulations, literature, and human intuition into the machine learning model can further increase this advantage. This work presents a set of methods for probabilistically integrating human input into an autonomous materials exploration campaign for composition-structure phase mapping. During the campaign, the user can provide input by indicating potential phase boundaries or phase regions with their uncertainty or indicating regions of interest. The input is then integrated through probabilistic priors, resulting in a probabilistic distribution over potential phase maps given the data, model, and human input. This work demonstrates an improvement in phase-mapping performance given appropriate human input.</p>
</li>
</ul>
<h3 id="concepts">Concepts<a class="headerlink" href="#concepts" title="Permanent link">&para;</a></h3>
<h4 id="theory-of-concepts">Theory of Concepts<a class="headerlink" href="#theory-of-concepts" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/concepts/">Concepts</a> - <strong><em>Plato Stanford</em></strong>. A collection of the computational philosophical debates about the concepts.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Theory-theory">Theory-theory</a> - <strong><em>Wikipedia</em></strong>. Wikipedia for the Theory theory, a perspective that contextualizes concepts in theoretical (or empirical) systems.</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/3659332/11fa44">Conceptual Change in Childhood</a> - <strong><em>MIT Press</em></strong>, 1985. [<a href="https://scholar.google.com/scholar?cluster=11720022076481483465">All Versions</a>]. Susan Carey's book on the theory theory of concepts in child development.</p>
</li>
<li>
<p><a href="http://library.lol/main/6A8215E9BAEB77F198C98CD75C517E02">Words, thoughts, and theories</a> - <strong><em>MIT Press</em></strong>, 1997. [<a href="https://scholar.google.com/scholar?cluster=16726462136203686735&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Alison Gopnik's book that articulates and defends the "theory theory" of cognitive and semantic development, the idea that infants and young children, like scientists, learn about the world by forming and revising theories-a view of the origins of knowledge and meaning that has broad implications for cognitive science.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1994-97940-009">The Theory Theory</a> - <strong><em>Mapping the mind: Domain specificity in cognition and culture, Cambridge University Press</em></strong>, 1994. [<a href="https://scholar.google.com/scholar?cluster=9397889700764191662&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Alison Gopnik's original paper on the theory theory.</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/844457/42178f?id=844457&amp;secret=42178f">The Origin of Concepts</a> - <strong><em>Oxford University Press</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11493102398422813821&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Susan Carey's extended book on the theory theory of concepts in child development.</p>
</li>
<li>
<p><a href="https://osf.io/preprints/psyarxiv/xrnb2">What we mean when we say semantic: A Consensus statement on the nomenclature of semantic memory</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=7464626532716945232">All Versions</a>]. The aim of this multidisciplinary workgroup was to establish consensus definitions for some of the major recurring constructs in semantic research (e.g., concept, amodal, abstract). These efforts yielded a glossary consisting of succinct definitions, agreement, subjective confidence ratings, relevant theoretical background, and principled dissenting views. These core definitions will potentially yield benchmarks for aligning perspectives and improving cross-disciplinary communication in semantic research.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2012-12791-001">Reconstructing constructivism: Causal models, Bayesian learning mechanisms, and the theory theory</a> - <strong><em>Psychological Bulletin</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=11218217347365817167&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Alison Gopnik's review on the constructivism idea of developmental research, including the theory theory of concepts.</p>
</li>
<li>
<p><a href="https://groups.psych.northwestern.edu/gentner/newpdfpapers/MedinGoldstoneGentner90.pdf">Similarity involving attributes and relations: Judgments of similarity and difference are not inverses</a> - <strong><em>Psychological Science</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?cluster=13205938250772079784">All Versions</a>]. Theory on similarity judgement by attributes and relations.</p>
</li>
</ul>
<h4 id="human-concept-representation">Human Concept Representation<a class="headerlink" href="#human-concept-representation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30250-3">Structuring Knowledge with Cognitive Maps and Cognitive Graphs</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7196012353183004425">All Versions</a>]. [<a href="https://www.sas.upenn.edu/psych/epsteinlab/pdfs/Peer%20Brunec%20Newcombe%20Epstein%20TiCS%202020%20Cog%20maps%20and%20cog%20graphs.pdf">Preprint</a>]. Humans and animals use mental representations of the spatial structure of the world to navigate. The classical view is that these representations take the form of Euclidean cognitive maps, but alternative theories suggest that they are cognitive graphs consisting of locations connected by paths. The authors review evidence suggesting that both map-like and graph-like representations exist in the mind/brain that rely on partially overlapping neural systems. Maps and graphs can operate simultaneously or separately, and they may be applied to both spatial and nonspatial knowledge. By providing structural frameworks for complex information, cognitive maps and cognitive graphs may provide fundamental organizing schemata that allow us to navigate in physical, social, and conceptual spaces.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/nature17637">Natural speech reveals the semantic maps that tile human cerebral cortex</a> - <strong><em>Nature</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=14997953800741854188">All Versions</a>]. [<a href="https://www.polyu.edu.hk/cbs/rclcn/images/cdl_articles/H/Huth_et_al._2016.pdf">Preprint</a>]. [<a href="https://github.com/HuthLab/speechmodeltutorial">Code &amp; Tutorial</a>]. The meaning of language is represented in regions of the cerebral cortex collectively known as the ‘semantic system’. However, little of the semantic system has been mapped comprehensively, and the semantic selectivity of most regions is unknown. This work systematically maps semantic selectivity across the cortex using voxel-wise modelling of functional MRI (fMRI) data collected while subjects listened to hours of narrative stories. This work shows that the semantic system is organized into intricate patterns that seem to be consistent across individuals. The authors then use a novel generative model to create a detailed semantic atlas. The results suggest that most areas within the semantic system represent information about specific semantic domains, or groups of related concepts, and the atlas shows which domains are represented in each area. This study demonstrates that data-driven methods---commonplace in studies of human neuroanatomy and functional connectivity---provide a powerful and efficient means for mapping functional representations in the brain.</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/full/10.1177/09567976211003877">Idiosyncratic Tower of Babel: Individual differences in word-meaning representation increase as word abstractness increases</a> - <strong><em>Psychological Science</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=18214600097352809308">All Versions</a>]. [<a href="http://bilab.bnu.edu.cn/paper/2021/Wang_2021_Psychology%20Science.pdf">All Versions</a>]. Humans primarily rely on language to communicate, on the basis of a shared understanding of the basic building blocks of communication: words. Do we mean the same things when we use the same words? Although cognitive neural research on semantics has revealed the common principles of word-meaning representation, the factors underlying the potential individual variations in word meanings are unknown. This work empirically characterized the intersubject consistency of 90 words across 20 adult subjects (10 female) using both behavioral measures (rating-based semantic-relationship patterns) and neuroimaging measures (word-evoked brain activity patterns). Across both the behavioral and neuroimaging experiments, this work showed that the magnitude of individual disagreements on word meanings could be modeled on the basis of how much language or sensory experience is associated with a word and that this variation increases with word abstractness. Uncovering the cognitive and neural origins of word-meaning disagreements across individuals has implications for potential mechanisms to modulate such disagreements.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-022-01316-8">Semantic projection recovers rich human knowledge of multiple object features from word embeddings</a> - <strong><em>Nature Human Behavior</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=2499199921371106654">All Versions</a>]. [<a href="https://cap.csail.mit.edu/sites/default/files/research-pdfs/Semantic%20projection%20recovers%20rich%20human%20knowledge%20of%20multiple%20object%20features%20from%20word%20embeddings.pdf">Preprint</a>]. How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts—that is, are more semantically related—are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. This work proposes a domain-general method to extract context-dependent relationships from word embeddings: ‘semantic projection’ of word-vectors onto lines that represent features such as size (the line connecting the words ‘small’ and ‘big’) or danger (‘safe’ to ‘dangerous’), analogous to ‘mental scales’. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.</p>
</li>
<li>
<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00385/full">Using a high-dimensional graph of semantic space to model relationships among words</a> - <strong><em>Frontiers in Psychology</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=472523411548302295">All Versions</a>]. The GOLD model (Graph Of Language Distribution) is a network model constructed based on co-occurrence in a large corpus of natural language that may be used to explore what information may be present in a graph-structured model of language, and what information may be extracted through theoretically-driven algorithms as well as standard graph analysis methods. The present study will employ GOLD to examine two types of relationship between words: semantic similarity and associative relatedness. Semantic similarity refers to the degree of overlap in meaning between words, while associative relatedness refers to the degree to which two words occur in the same schematic context. It is expected that a graph structured model of language constructed based on co-occurrence should easily capture associative relatedness, because this type of relationship is thought to be present directly in lexical co-occurrence. However, it is hypothesized that semantic similarity may be extracted from the intersection of the set of first-order connections, because two words that are semantically similar may occupy similar thematic or syntactic roles across contexts and thus would co-occur lexically with the same set of nodes. Two versions the GOLD model that differed in terms of the co-occurence window, bigGOLD at the paragraph level and smallGOLD at the adjacent word level, were directly compared to the performance of a well-established distributional model, Latent Semantic Analysis (LSA). The superior performance of the GOLD models (big and small) suggest that a single acquisition and storage mechanism, namely co-occurrence, can account for associative and conceptual relationships between words and is more psychologically plausible than models using singular value decomposition (SVD).</p>
</li>
<li>
<p><a href="https://academic.oup.com/cercor/article/33/15/9280/7190929">Simple shape feature computation across modalities: convergence and divergence between the ventral and dorsal visual streams</a> - <strong><em>Cerebral Cortex</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=5977822802446917081">All Versions</a>]. [<a href="http://bilab.bnu.edu.cn/paper/2023/Tian_2023_CC.pdf">Preprints</a>]. Shape processing, whether by seeing or touching, is pivotal to object recognition and manipulation. Although the low-level signals are initially processed by different modality-specific neural circuits, multimodal responses to object shapes have been reported along both ventral and dorsal visual pathways. To understand this transitional process, the authors conducted visual and haptic shape perception fMRI experiments to test basic shape features (i.e. curvature and rectilinear) across the visual pathways. Using a combination of region-of-interest-based support vector machine decoding analysis and voxel selection method, the authors found that the top visual-discriminative voxels in the left occipital cortex (OC) could also classify haptic shape features, and the top haptic-discriminative voxels in the left posterior parietal cortex (PPC) could also classify visual shape features. Furthermore, these voxels could decode shape features in a cross-modal manner, suggesting shared neural computation across visual and haptic modalities. In the univariate analysis, the top haptic-discriminative voxels in the left PPC showed haptic rectilinear feature preference, whereas the top visual-discriminative voxels in the left OC showed no significant shape feature preference in either of the two modalities. Together, these results suggest that mid-level shape features are represented in a modality-independent manner in both the ventral and dorsal streams.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41597-019-0341-x">The Database of Cross-Linguistic Colexifications, reproducible analysis of cross-linguistic polysemies</a> - <strong><em>Scientific Data</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4039754406289857135">All Versions</a>]. [<a href="https://clics.clld.org/">Project</a>]. Advances in computer-assisted linguistic research have been greatly influential in reshaping linguistic research. With the increasing availability of interconnected datasets created and curated by researchers, more and more interwoven questions can now be investigated. Such advances, however, are bringing high requirements in terms of rigorousness for preparing and curating datasets. This work presents CLICS, a Database of Cross-Linguistic Colexifications (CLICS). CLICS tackles interconnected interdisciplinary research questions about the colexifcation of words across semantic categories in the world’s languages, and show-cases best practices for preparing data for cross-linguistic research.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S001002772300183X">Locating what comes to mind in empirically derived representational spaces</a> - <strong><em>Cognition</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=57834483230365927">All Versions</a>]. An evidence-based study concluding that people call category members to mind according to their location in representational space, specifically based on the predicted usefulness of considering category members with particular features.</p>
</li>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00171-2">Why concepts are (probably) vectors</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=4315363807034184312">All Versions</a>]. For decades, cognitive scientists have debated what kind of representation might characterize human concepts. Whatever the format of the representation, it must allow for the computation of varied properties, including similarities, features, categories, definitions, and relations. It must also support the development of theories, ad hoc categories, and knowledge of procedures. Here, the authors discuss why vector-based representations provide a compelling account that can meet all these needs while being plausibly encoded into neural architectures. This view has become especially promising with recent advances in both large language models and vector symbolic architectures. These innovations show how vectors can handle many properties traditionally thought to be out of reach for neural models, including compositionality, definitions, structures, and symbolic computational processes.</p>
</li>
</ul>
<h4 id="ai-concept-representation">AI Concept Representation<a class="headerlink" href="#ai-concept-representation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.ade4401">A principal odor map unifies diverse tasks in olfactory perception</a> - <strong><em>Science</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17847258457660438418">All Versions</a>]. [<a href="https://github.com/osmoai/publications/tree/main/lee_et_al_2023">Code</a>]. [<a href="https://www.kaggle.com/datasets/aryanamitbarsainyan/multi-labelled-smiles-odors-dataset">Data (Reproduced)</a>]. [<a href="https://centaur.reading.ac.uk/113304/1/Mayhew%20et%20al%20for%20Centaur.pdf">Preprint</a>]. [<a href="https://www.thegoodscentscompany.com/">GoodScents Database</a>]. [<a href="http://www.leffingwell.com/bacispmp.htm">Leffingwell Database</a>]. Mapping molecular structure to odor perception is a key challenge in olfaction. This work used graph neural networks to generate a principal odor map (POM) that preserves perceptual relationships and enables odor quality prediction for previously uncharacterized odorants. The model was as reliable as a human in describing odor quality: On a prospective validation set of 400 out-of-sample odorants, the model-generated odor profile more closely matched the trained panel mean than did the median panelist. By applying simple, interpretable, theoretically rooted transformations, the POM outperformed chemoinformatic models on several other odor prediction tasks, indicating that the POM successfully encoded a generalized map of structure-odor relationships. This approach broadly enables odor prediction and paves the way toward digitizing odors.</p>
</li>
<li>
<p><a href="https://elifesciences.org/articles/82502">Metabolic activity organizes olfactory representations</a> - <strong><em>eLife</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8857896396450033667">All Versions</a>]. [<a href="https://github.com/osmoai/publications/tree/main/qian_et_al_2023">Code &amp; Data</a>]. Odorous compounds with similar POM representations are more likely to co-occur within a substance and be metabolically closely related; metabolic reaction sequences also follow smooth paths in POM despite large jumps in molecular structure.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9136877">A Review of Tactile Information: Perception and Action Through Touch</a> - <strong><em>IEEE Transactions on Robotics</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15493221881484741343">All Versions</a>]. [<a href="https://www.researchgate.net/profile/Qiang-Li-110/publication/342797645_A_Review_of_Tactile_Information_Perception_and_Action_Through_Touch/links/602f95bc92851c4ed5806e9f/A-Review-of-Tactile-Information-Perception-and-Action-Through-Touch.pdf">Preprint</a>]. Tactile sensing is a key sensor modality for robots interacting with their surroundings. These sensors provide a rich and diverse set of data signals that contain detailed information collected from contacts between the robot and its environment. The data are however not limited to individual contacts and can be used to extract a wide range of information about the objects in the environment as well as the actions of the robot during the interactions. This article provides an overview of tactile information and its applications in robotics. The authors present a hierarchy consisting of raw, contact, object, and action levels to structure the tactile information, with higher-level information often building upon lower-level information. The authors discuss different types of information that can be extracted at each level of the hierarchy. The article also includes an overview of different types of robot applications and the types of tactile information that they employ. Finally the article ends with a discussion for future tactile applications which are still beyond the current capabilities of robots.</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/44s454ng">Semantic features of object concepts generated with GPT-3</a> - <strong><em>CogSci'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=16958563995984242923">All Versions</a>]. Semantic features have been playing a central role in investigating the nature of our conceptual representations. Yet the enormous time and effort required to empirically sample and norm features from human raters has restricted their use to a limited set of manually curated concepts. Given recent promising developments with transformer-based language models, here the authors asked whether it was possible to use such models to automatically generate meaningful lists of properties for arbitrary object concepts and whether these models would produce features similar to those found in humans. To this end, the authors probed a GPT-3 model to generate semantic features for 1,854 objects and compared automatically-generated features to existing human feature norms. GPT-3 generated many more features than humans, yet showed a similar distribution in the types of generated features. Generated feature norms rivaled human norms in predicting similarity, relatedness, and category membership, while variance partitioning demonstrated that these predictions were driven by similar variance in humans and GPT-3. Together, these results highlight the potential of large language models to capture important facets of human knowledge and yield a new approach for automatically generating interpretable feature sets, thus drastically expanding the potential use of semantic features in psychological and linguistic studies. </p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/8953737">Connecting Touch and Vision via Cross-Modal Prediction</a> - <strong><em>CVPR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=17326564895972374001">All Versions</a>]. [<a href="https://github.com/YunzhuLi/VisGel">Project</a>]. Humans perceive the world using multi-modal sensory inputs such as vision, audition, and touch. This work investigates the cross-modal connection between vision and touch. The main challenge in this cross-domain modeling task lies in the significant scale discrepancy between the two: while our eyes perceive an entire visual scene at once, humans can only feel a small region of an object at any given moment. To connect vision and touch, this work introduces new tasks of synthesizing plausible tactile signals from visual inputs as well as imagining how we interact with objects given tactile data as input. To accomplish the goals, the authors first equip robots with both visual and tactile sensors and collect a large-scale dataset of corresponding vision and tactile image sequences. To close the scale gap, the authors present a new conditional adversarial model that incorporates the scale and location information of the touch. Human perceptual studies demonstrate that the model can produce realistic visual images from tactile data and vice versa.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2022.tacl-1.69/">Unit Testing for Concepts in Neural Networks</a> - <strong><em>Transactions of the Association for Computational Linguistics</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=3036662275506971282&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Testing the concept representation by neural networks through Fodor's theory of concepts.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2024.acl-long.820/">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</a> - <strong><em>ACL'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=5847238732288003106">All Versions</a>]. A preliminary work empirically showing that the intermediate embeddings of multilingual Transformers (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. Also, the embedding of abstract concept space lies closer to English than to other languages.</p>
</li>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00035-4">From task structures to world models: what do LLMs know?</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=14836877410607949822">All Versions</a>]. [<a href="http://cncl.yale.edu/sites/default/files/pub-downloads/yildirim-paul-llms-knowledge.pdf">Preprint</a>]. In what sense does a large language model (LLM) have knowledge? The authors answer by granting LLMs ‘instrumental knowledge’: knowledge gained by using next-word generation as an instrument. The authors then ask how instrumental knowledge is related to the ordinary, ‘worldly knowledge’ exhibited by humans, and explore this question in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. The authors discuss ways LLMs could recover degrees of worldly knowledge and suggest that such recovery will be governed by an implicit, resource-rational tradeoff between world models and tasks. The authors' answer to this question extends beyond the capabilities of a particular AI system and challenges assumptions about the nature of knowledge and intelligence.</p>
</li>
</ul>
<h3 id="complexity-information-theory">Complexity &amp; Information Theory<a class="headerlink" href="#complexity-information-theory" title="Permanent link">&para;</a></h3>
<h4 id="theory">Theory<a class="headerlink" href="#theory" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="http://www.cs.yale.edu/homes/yry/readings/general/shannon1948.pdf">A Mathematical Theory of Communication</a> - <strong><em>The Bell System Technical Journal</em></strong>, 1948. [<a href="https://scholar.google.com/scholar?cluster=8313213127749369813">All Versions</a>]. Shannon's original paper on Information Theory.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1973-01647-001">Complexity and the representation of patterned sequences of symbols</a> - <strong><em>Psychological Review</em></strong>, 1972. [<a href="https://scholar.google.com/scholar?cluster=3426861135318645138">All Versions</a>]. Herbert Simon's review on subjective complexity.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/1057698">Visual Pattern Discrimination</a> - <strong><em>IRE Transactions on Information Theory</em></strong>, 1962. [<a href="https://scholar.google.com/scholar?cluster=10729525966103382864">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5390997">Algorithmic Information Theory</a> - <strong><em>IBM Journal of Research and Development</em></strong>, 1977. [<a href="https://scholar.google.com/scholar?cluster=14735710867906424793">All Versions</a>]. Chaitin's original paper on Algorithmic Information Theory.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2003/hash/b06b5541a62ed438f956b662b4e1ec28-Abstract.html">From Algorithmic to Subjective Randomness</a> - <strong><em>NeurIPS'03</em></strong>, 2003. [<a href="https://scholar.google.com/scholar?cluster=14721764738308036578">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v202/shi23i.html">On the Complexity of Bayesian Generalization</a> - <strong><em>ICML'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=5817813824878811147">All Versions</a>]. [<a href="https://github.com/SHI-Yu-Zhe/bayesian-generalization-complexity">Project</a>]. [<a href="https://drive.google.com/file/d/1eCuFqBYN8kuiAmoVtXWedXW0r0TdY55W/view">Models</a>]. This work examines concept generalization at a large scale in the natural visual spectrum. Established computational modes (i.e., rule-based or similarity-based) are primarily studied isolated, focusing on confined and abstract problem spaces. This work studies these two modes when the problem space scales up and when the complexity of concepts becomes diverse. At the representational level, the authors investigate how the complexity varies when a visual concept is mapped to the representation space. Prior literature has shown that two types of complexities build an inverted-U relation. Leveraging Representativeness of Attribute (RoA), the authors computationally confirm: Models use attributes with high RoA to describe visual concepts, and the description length falls in an inverted-U relation with the increment in visual complexity. At the computational level, the authors examine how the complexity of representation affects the shift between the rule- and similarity-based generalization. The authors hypothesize that category-conditioned visual modeling estimates the co-occurrence frequency between visual and categorical attributes, thus potentially serving as the prior for the natural visual world. Experimental results show that representations with relatively high subjective complexity outperform those with relatively low subjective complexity in rule-based generalization, while the trend is the opposite in similarity-based generalization. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-025-01092-w">Quantifying artificial intelligence through algorithmic generalization</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=16525845943864999422">All Versions</a>]. The rapid development of artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, AI systems fall short on tests requiring algorithmic reasoning---a glaring limitation, given the necessity for interpretable and reliable technology. Despite a surge in reasoning benchmarks emerging from the academic community, no theoretical framework exists to quantify algorithmic reasoning in AI systems. Here the authors adopt a framework from computational complexity theory to quantify algorithmic generalization using algebraic expressions: algebraic circuit complexity. Algebraic circuit complexity theory---the study of algebraic expressions as circuit models---is a natural framework for studying the complexity of algorithmic computation. Algebraic circuit complexity enables the study of generalization by defining benchmarks in terms of the computational requirements for solving a problem. Moreover, algebraic circuits are generic mathematical objects; an arbitrarily large number of samples can be generated for a specified circuit, making it an ideal experimental sandbox for the data-hungry models that are used today. In this Perspective, the authors adopt tools from algebraic circuit complexity, apply them to formalize a science of algorithmic generalization, and address key challenges for its successful application to AI science.</p>
</li>
</ul>
<h4 id="dimensionality-reduction">Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1084.4695&amp;rep=rep1&amp;type=pdf">A global geometric framework for nonlinear dimensionality reduction</a> - <strong><em>Science</em></strong>, 2000. [<a href="https://scholar.google.com/scholar?cluster=14602426245887619907">All Versions</a>]. The original paper on spectrum clustering.</p>
</li>
<li>
<p><a href="https://asset-pdf.scinapse.io/prod/2100495367/2100495367.pdf">Reducing the dimensionality of data with neural networks</a> - <strong><em>Science</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=15344645275208957628">All Versions</a>]. The original paper on Variational Autoencoder.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1206.5538.pdf">Representation Learning: A Review and New Perspectives</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=559463397382443088">All Versions</a>]. Yoshua Bengio's review on representation learning.</p>
</li>
<li>
<p><a href="http://www.stat.ucla.edu/~jxie/personalpage_file/publications/representation_learning_Review.pdf">Representation Learning: A Statistical Perspective</a> - <strong><em>Annual Review of Statistics and Its Application</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14358027809538175293">All Versions</a>]. Song-Chun Zhu and Ying Nian Wu's review on representation learning, in an account of statistics.</p>
</li>
<li>
<p><a href="http://robotics.caltech.edu/wiki/images/8/8f/DeepLearningBottleneck.pdf">Deep Learning and the Information Bottleneck Principle</a> - <strong><em>IEEE Information Theory Workshop'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=13152354842433826281">All Versions</a>]. The first paper identifying the problem of information bottleneck in representation learning.</p>
</li>
<li>
<p><a href="https://artemyk.github.io/assets/pdf/papers/Saxe%20et%20al_2019_On%20the%20information%20bottleneck%20theory%20of%20deep%20learning.pdf">On the information bottleneck theory of deep learning</a> - <strong><em>Journal of Statistical Mechanics: Theory and Experiment</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12271240925674881982">All Versions</a>].</p>
</li>
</ul>
<h4 id="visual-complexity">Visual Complexity<a class="headerlink" href="#visual-complexity" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.researchgate.net/profile/Don-Donderi-2/publication/7337589_Visual_Complexity_A_Review/links/5f0875ed45851550509a3a7a/Visual-Complexity-A-Review.pdf">Visual complexity: a review</a> - <strong><em>Psychological Bulletin</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=10747901143387624939">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2006-00818-005">APA</a>]. A psychological account on visual complexity.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0141938205000120">Compressed File Length Predicts Search Time and Errors on Visual Displays</a> - <strong><em>Displays</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=15600966633648834042">All Versions</a>]. Compressed file size, an objective, easily obtained measure of display complexity, predicts both subjective complexity judgments and objective search performance. It is analogous to algorithmic complexity, a theoretical but impractical measure of bit string complexity. The data suggest that it may be possible to use the compressed file size measure to predict display performance in applied tasks.</p>
</li>
<li>
<p><a href="https://stefan.winklerbros.net/Publications/qomex2013si.pdf">Image complexity and spatial information</a> - <strong><em>International Workshop on Quality of Multimedia Experience</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=16011036229039693102">All Versions</a>].</p>
</li>
<li>
<p><a href="https://perception.jhu.edu/files/PDFs/21_Complexity_Speaking/SunFirestone_SpeakingSeeing_2021_JEPG.pdf">Seeing and speaking: How verbal “description length” encodes visual complexity</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=246820603191585233">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2021-83037-001">APA</a>]. Empirical evidencs showing the relation between visual complexity and description length.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2205.05666.pdf">Identifying concept libraries from language about object structure</a> - <strong><em>CogSci'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=4019205027627496528">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010027722003158">Show or tell? Exploring when (and why) teaching with language outperforms demonstration</a> - <strong><em>Cognition</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11837154580063293174">All Versions</a>]. The findings of this paper suggest that language communicates complex concepts by directly transmitting abstract rules. In contrast, demonstrations transmit examples, requiring the learner to infer the rules.</p>
</li>
</ul>
<h3 id="communications">Communications<a class="headerlink" href="#communications" title="Permanent link">&para;</a></h3>
<h4 id="non-verbal-communication">Non-Verbal Communication<a class="headerlink" href="#non-verbal-communication" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1551-6709.2009.01090.x">The Interactive Evolution of Human Communication Systems</a> - <strong><em>Cognitive Science</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=6689941517686043970">All Versions</a>]. Nicolas Fay's original paper on iconicity.</p>
</li>
<li>
<p><a href="https://benjamins.com/catalog/pc.22.2.05fay">Iconicity: From sign to system in human communication and language</a> - <strong><em>Pragmatics &amp; Cognition</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=8525760321117094567">All Versions</a>]. This paper explores the role of iconicity in spoken language and other human communication systems.</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/abs/10.1177/108835769400900301">The Picture Exchange Communication System</a> - <strong><em>Behavior Modification</em></strong>, 1994. [<a href="https://scholar.google.com/scholar?cluster=18113491434570143349&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1080/15326900701221363">Graphical Language Games: Interactional Constraints on Representational Form</a> - <strong><em>Cognitive Science</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=280214578402050136&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The first paper introducing the graphical language game.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0378216614001830">A multimodal discourse theory of visual narrative</a> - <strong><em>Journal of Pragmatics</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=912273653379961242&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ayankumarbhunia.github.io/pixelor/image/pixelor.pdf">Pixelor: A Competitive Sketching AI Agent. So you think you can beat me?</a> - <strong><em>ACM SIGGRAPH'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6676723059377806081&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="http://sketchx.ai/pixelor">Project</a>]. Rationality in feature sketching.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s42113-019-00058-7">Pragmatic Inference and Visual Abstraction Enable Contextual Flexibility During Visual Communication</a> - <strong><em>Computational Brain &amp; Behavior</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17971107104483505071&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A computational account on the rational behavior in graphical language games.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/550ff553efc2c58410f277c667d12786-Abstract-Conference.html">Emergent Graphical Conventions in a Visual Communication Game</a> - <strong><em>NeurIPS</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17122070906194572150">All Versions</a>]. A computational account on the emergence of iconic language.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3610591.3616427">AI Nüshu: An Exploration of Language Emergence in Sisterhood Through the Lens of Computational Linguistics</a> - <strong><em>ACM SIGGRAPH Asia'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6849286654402017109&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. By continually observing their environment and communicating, AI agents trained in the Chinese dictionary and the Nüshu corpus collaborate towards creating a standard writing system to encode Chinese.</p>
</li>
<li>
<p><a href="https://www.eva.mpg.de/documents/Elsevier/Liszkowski_Twelve_Cognition_2008_1554509.pdf">Twelve-month-olds communicate helpfully and appropriately for knowledgeable and ignorant partners</a> - <strong><em>Cognition</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=8202048572661677635&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on child pointing.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s10115-006-0062-2">Toward understanding the importance of gesture in distributed scientific collaboration</a> - <strong><em>Knowledge and Information Systems</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3145646721897130511">All Versions</a>]. </p>
</li>
</ul>
<h4 id="pragmatics">Pragmatics<a class="headerlink" href="#pragmatics" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/pragmatics/">Pragmatics</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account of Pragmatics, whilch studies utterances in specific contexts.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/abs/10.1126/science.1218633">Predicting Pragmatic Reasoning in Language Games</a> - <strong><em>Science</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=15533081031935746054">All Versions</a>]. [<a href="https://langcog.stanford.edu/papers_new/frank-2012-science.pdf">Preprint</a>]. One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. This work examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers’ intended referents. The model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S136466131630122X">Pragmatic Language Interpretation as Probabilistic Inference</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=11393505968563356130">All Versions</a>]. Understanding language requires more than the use of fixed conventions and more than decoding combinatorial structure. Instead, comprehenders make exquisitely sensitive inferences about what utterances mean given their knowledge of the speaker, language, and context. Building on developments in game theory and probabilistic modeling, the authors describe the rational speech act (RSA) framework for pragmatic reasoning. RSA models provide a principled way to formalize inferences about meaning in context; they have been used to make successful quantitative predictions about human behavior in a variety of different tasks and situations, and they explain why complex phenomena, such as hyperbole and vagueness, occur. More generally, they provide a computational framework for integrating linguistic structure, world knowledge, and context in pragmatic language understanding.</p>
</li>
<li>
<p><a href="http://cocolab.stanford.edu/papers/BergenLevyGoodman-LexUnc.pdf">Pragmatic Reasoning through Semantic Inference</a> - <strong><em>Semantics &amp; Pragmatics</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=1433855075217315997">All Versions</a>].</p>
</li>
<li>
<p><a href="https://semantics.uchicago.edu/kennedy/docs/processing.pdf">Processing gradable adjectives in context: A visual world study</a> - <strong><em>Semantics and Linguistic Theory</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=13426776838629402579">All Versions</a>]. Adjective understanding as a rational inference in the context.</p>
</li>
<li>
<p><a href="https://transacl.org/index.php/tacl/article/view/1142">Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding</a> - <strong><em>Transactions of the Association for Computational Linguistics</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=11119271811833503059">All Versions</a>].</p>
</li>
<li>
<p><a href="https://compdevlab.yale.edu/docs/2019/2019_ChildDev_Pragmatics.pdf">Social Pragmatics: Preschoolers Rely on Commonsense Psychology to Resolve Referential Underspecification</a> - <strong><em>Child Development</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=16352913537004112920">All Versions</a>]. A piece of evidence for children's capability on social pragmatics.</p>
</li>
<li>
<p><a href="http://cocolab.stanford.edu/papers/CohnGordonEtAl2018_NAACL.pdf">Pragmatically Informative Image Captioning with Character-Level Inference</a> - <strong><em>NAACL'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=1670953084401884599">All Versions</a>].</p>
</li>
<li>
<p><a href="https://aclanthology.org/2020.findings-emnlp.173/">Pragmatic Issue-Sensitive Image Captioning</a> - <strong><em>EMNLP Findings'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10608257248144445301">All Versions</a>]. Application of Rational Speech Act to Image Captioning. </p>
</li>
<li>
<p><a href="https://cogsci.mindmodeling.org/2019/papers/0091/0091.pdf">Disentangling contributions of visual information and interaction history in the formation of graphical conventions</a> - <strong><em>CogSci'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15046353579508199394&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-021-01145-1">How young children integrate information sources to infer the meaning of words</a> - <strong><em>Nature Human Behavior</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10144794357802769844">All Versions</a>]. Before formal education begins, children typically acquire a vocabulary of thousands of words. This learning process requires the use of many different information sources in their social environment, including their current state of knowledge and the context in which they hear words used. This paper specifies a developmental model according to which children consider information sources in an age-specific way and integrate them via Bayesian inference. This work presents a developmental theory of information integration during language learning and illustrates how formal models can be used to make a quantitative test of the predictive and explanatory power of competing theories.</p>
</li>
<li>
<p><a href="https://semprag.org/index.php/sp/article/view/sp.5.6/pdf">Information Structure in Discourse: Towards an Integrated Formal Theory of Pragmatics</a> - <strong><em>Semantics and Pragmatics</em></strong>, 1998. [<a href="https://scholar.google.com/scholar?cluster=9127222314768938599&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11098-020-01490-3">When Lingens meets Frege: communication without common ground</a> - <strong><em>Philosophical Studies</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10912415595149303257&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2307.07871">The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents</a> - <strong><em>ICML'23 Workshop on Theory-of-Mind</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11933410239580707313&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://sites.google.com/view/socialai-school">Project</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41599-020-0404-9">Language as shaped by the environment: linguistic construal in a collaborative spatial task</a> - <strong><em>Humanities and Social Sciences Communications</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7842508027049437987">All Versions</a>]. [<a href="https://osf.io/sxtaq">Code &amp; Data</a>]. [<a href="https://dialoguetoolkit.github.io/chattool/">Dialogue Experimental Toolkit(DiET)</a>]. The present study sets out to experimentally investigate how environmental factors come to shape the emergence of linguistic conventions. To this end, the authors adapt the classical Maze Game task to test the hypothesis that participants routinise different linguistic strategies to communicate positions in the maze contingent on particular environmental affordances (i.e. structure of the mazes). The results confirm that subtle environmental motivations drive the emergence of different communicative conventions in an otherwise identical task, suggesting that linguistic adaptations are highly sensitive to factors of the shared task environment.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2008.12142">Exploring Urban Form Through Openstreetmap Data: A Visual Introduction</a> - <strong><em>Urban Experience and Design: Contemporary Perspectives on Improving the Public Realm</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7094530618542001733&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/gboeing/osmnx">OSMnx Tool</a>]. [<a href="https://github.com/YuzheSHI/awesome-agi-cocosci/blob/master/ https://www.openstreetmap.org/">OpenStreetMap Website</a>].</p>
</li>
<li>
<p><a href="https://www.speech.kth.se/~edlund/bielefeld/references/garrod-and-anderson-1987.pdf">Saying what you mean in dialogue: A study in conceptual and semantic co-ordination</a> - <strong><em>Cognition</em></strong>, 1987. [<a href="https://scholar.google.com/scholar?cluster=15377075954534820544&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.sfs.uni-tuebingen.de/~gjaeger/lehre/ws0708/spieltheorie/garrod.pdf">Conversation, co-ordination and convention: an empirical investigation of how groups establish linguistic conventions</a> - <strong><em>Cognition</em></strong>, 1994. [<a href="https://scholar.google.com/scholar?cluster=3784850469297049700&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="language-compositionality">Language Compositionality<a class="headerlink" href="#language-compositionality" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/compositionality/">Compositionality</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on compositionality, one of the distinctive feature of language.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-024-07522-w">Language is primarily a tool for communication rather than thought</a> - <strong><em>Nature</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13724799649075764503">All Versions</a>]. This perspective brings recent evidence from neuroscience and allied disciplines to argue that in modern humans, language is a tool for communication, contrary to a prominent view that we use language for thinking. The authors begins by introducing the brain network that supports linguistic ability in humans. They then review evidence for a double dissociation between language and thought, and discuss several properties of language that suggest that it is optimized for communication. This perspective concludes that although the emergence of language has unquestionably transformed human culture, language does not appear to be a prerequisite for complex thought, including symbolic thought. Instead, language is a powerful tool for the transmission of cultural knowledge; it plausibly co-evolved with humans' thinking and reasoning capacities, and only reflects, rather than gives rise to, the signature sophistication of human cognition. </p>
</li>
<li>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.3235">On The Emergence Of Compositionality</a> - <strong><em>Proceedings of the Evolution of Language Conference'06</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=16315741180717951222&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on the emergence of compositionality.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1612.07182.pdf">Multi-Agent Cooperation and the Emergence of (Natural) Language</a> - <strong><em>ICLR'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=1931070702879918446&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on the emergence of language in multi-agent reinforcement learning.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2017/hash/70222949cc0db89ab32c9969754d4758-Abstract.html">Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols</a> - <strong><em>NeurIPS'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17308624474306270808&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1804.03980">Emergent communication through negotiation</a> - <strong><em>ICLR'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8825869866742501521&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2019-07481-001">The language of generalization</a> - <strong><em>Psychological Review</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7723877614160376324&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2004.09124">Compositionality and Generalization in Emergent Languages</a> - <strong><em>ACL'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5792073344743965767&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/5kv636c5">Word formation supports efficient communication: The case of compounds</a> - <strong><em>CogSci'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17465553221758916299&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2311.17227">War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=3598519753107761968&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="coordination">Coordination<a class="headerlink" href="#coordination" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/scirobotics.abm4183">In situ bidirectional human-robot value alignment</a> - <strong><em>Science Robotics</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=18342339995965564405">All Versions</a>]. [<a href="https://par.nsf.gov/servlets/purl/10351399">Preprint</a>]. This paper proposes an explainable artificial intelligence (XAI) system in which a group of robots predicts users’ values by taking in situ feedback into consideration while communicating their decision processes to users through explanations. To learn from human feedback, the XAI system integrates a cooperative communication model for inferring human values associated with multiple desirable goals. To be interpretable to humans, it simulates human mental dynamics and predicts optimal explanations using graphical models.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2304.14656.pdf">From Explicit Communication to Tacit Cooperation: A Novel Paradigm for Cooperative MARL</a> - <strong><em>AAMAS'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=12114270828108588849">All Versions</a>]. Drawing inspiration from human team cooperative learning, this paper proposes a novel paradigm that facilitates a gradual shift from explicit communication to tacit cooperation.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-025-01038-2">The future of open human feedback</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=8282207012874793204">All Versions</a>]. Human feedback on conversations with language models is central to how these systems learn about the world, improve their capabilities and are steered towards desirable and safe behaviours. However, this feedback is mostly collected by frontier artificial intelligence labs and kept behind closed doors. This work brings together interdisciplinary experts to assess the opportunities and challenges to realizing an open ecosystem of human feedback for artificial intelligence. The authors first look for successful practices in the peer-production, open-source and citizen-science communities. The authors then characterize the main challenges for open human feedback. For each, the authors survey current approaches and offer recommendations. The authors end by envisioning the components needed to underpin a sustainable and open human feedback ecosystem. In the centre of this ecosystem are mutually beneficial feedback loops, between users and specialized models, incentivizing a diverse stakeholder community of model trainers and feedback providers to support a general open feedback pool.</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=b7VmJ4107q">HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making</a> - <strong><em>AAMAS'26</em></strong>, 2026. [<a href="https://scholar.google.com/scholar?cluster=11127617290205220978">All Versions</a>]. Benchmarks are crucial for assessing multi-agent reinforcement learning (MARL) algorithms. While StarCraft II-related environments have driven significant advances in MARL, existing benchmarks like SMAC focus primarily on micromanagement, limiting comprehensive evaluation of high-level strategic intelligence. To address this, this work introduces HLSMAC, a new cooperative MARL benchmark with 12 carefully designed StarCraft II scenarios based on classical stratagems from the <em>Thirty-Six Stratagems</em>. Each scenario corresponds to a specific stratagem and is designed to challenge agents with diverse strategic elements, including tactical maneuvering, timing coordination, and deception, thereby opening up avenues for evaluating high-level strategic decision-making capabilities. The authors also propose novel metrics across multiple dimensions beyond conventional win rate, such as ability utilization and advancement efficiency, to assess agents' overall performance within the HLSMAC environment. The authors conduct a large-scale evaluation of 21 state-of-the-art MARL algorithms and LLM-based agents, with additional multi-seed analysis for relatively better-performing methods. The results demonstrate that HLSMAC serves as a robust testbed for advancing multi-agent strategic decision-making.</p>
</li>
</ul>
<h3 id="domain-specific-language">Domain Specific Language<a class="headerlink" href="#domain-specific-language" title="Permanent link">&para;</a></h3>
<h4 id="design-theory">Design Theory<a class="headerlink" href="#design-theory" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Domain-specific_language">Domain-Specific Language</a> - <strong><em>Wikipedia</em></strong>. Wikipedia encyclopedia entry on Domain Specific Languages.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Domain_engineering">Domain Engineering</a> - <strong><em>Wikipedia</em></strong>. Wikipedia encyclopedia entry on Domain Engineering.</p>
</li>
<li>
<p><a href="https://martinfowler.com/books/dsl.html">Domain-Specific Languages</a> - <strong><em>Pearson Education</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=3653365103385845410">All Versions</a>]. [<a href="https://martinfowler.com/dsl.html">Domain-Specific Languages Guide</a>]. When carefully selected and used, Domain-Specific Languages (DSLs) may simplify complex code, promote effective communication with customers, improve productivity, and unclog development bottlenecks. In Domain-Specific Languages, noted software development expert Martin Fowler first provides the information software professionals need to decide if and when to utilize DSLs. Then, where DSLs prove suitable, Fowler presents effective techniques for building them, and guides software engineers in choosing the right approaches for their applications.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Comparison_of_multi-paradigm_programming_languages">Comparison of multi-paradigm programming languages</a> - <strong><em>Wikipedia</em></strong>. Programming languages may support multiple programming paradigms. This Wikipedia encyclopedia entry lists a concise reference for the programming paradigms.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/947955.1083808">Epigrams on programming</a> - <strong><em>ACM SIGPLAN Notices</em></strong>, 1982. [<a href="https://scholar.google.com/scholar?cluster=6439127299132936476">All Versions</a>].</p>
</li>
<li>
<p><a href="https://tomassetti.me/domain-specific-languages/">The complete guide to (external) Domain Specific Languages</a>. An introduction to Domain Specific Languages (DSL) based on 19 DSL cases.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/1118890.1118892">When and How to Develop Domain-Specific Languages</a> - <strong><em>ACM Computing Surveys</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=8598236436890577027">All Versions</a>]. [<a href="https://people.cs.ksu.edu/~schmidt/505f14/Lectures/WhenDSL.pdf">Preprint</a>]. Domain-specific languages (DSLs) are languages tailored to a specific application domain. They offer substantial gains in expressiveness and ease of use compared with general-purpose programming languages in their domain of application. DSL development is hard, requiring both domain knowledge and language development expertise. Few people have both. Not surprisingly, the decision to develop a DSL is often postponed indefinitely, if considered at all, and most DSLs never get beyond the application library stage. Although many articles have been written on the development of particular DSLs, there is very limited literature on DSL development methodologies and many questions remain regarding when and how to develop a DSL. To aid the DSL developer, this survey paper identifies patterns in the decision, analysis, design, and implementation phases of DSL development. These patterns improve and extend earlier work on DSL design patterns.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1409.2378">Design Guidelines for Domain Specific Languages</a> - <strong><em>OOPSLA Workshop on Domain-Specific Modeling (DSM' 09)</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=1962567819031018744">All Versions</a>]. Designing a new domain specific language is as any other complex task sometimes error-prone and usually time consuming, especially if the language shall be of high-quality and comfortably usable. Existing tool support focuses on the simplification of technical aspects but lacks support for an enforcement of principles for a good language design. In this paper we investigate guidelines that are useful for designing domain specific languages, largely based on our experience in developing languages as well as relying on existing guidelines on general purpose (GPLs) and modeling languages. This work defined Guidelines to support a DSL developer to achieve better quality of the language design and a better acceptance among its users.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/352029.352035">Domain-specific languages: an annotated bibliography</a> - <strong><em>ACM SIGPLAN Notices</em></strong>, 2000. [<a href="https://scholar.google.com/scholar?cluster=8845429548327315750">All Versions</a>]. A survey on the topic of domain-specific languages as used for the construction and maintenance of software systems. The survey lists a selection of 75 key publications in the area, and provides a summary for each of the papers. Moreover, the survey discusses terminology, risks and benefits, example domain-specific languages, design methodologies, and implementation techniques.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/6511840">Usability Evaluation of Domain-Specific Languages</a> - <strong><em>ICQICT'12</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=3047215455890195199">All Versions</a>]. [<a href="http://www-ctp.di.fct.unl.pt/QUASAR/Resources/Papers/2012/Barisic2012SEDES.pdf">Preprint</a>]. The purpose of this proposal is to contribute to the systematic activity of Software Language Engineering by focusing on the issue of the Usability evaluation of DSLs. Usability evaluation is often skipped, relaxed, or at least omitted from papers reporting development of DSLs. The authors argue that a systematic approach based on User Interface experimental validation techniques should be used to assess the impact of new DSLs. For that purpose, the authors propose to merge common Usability evaluation processes with the DSL development process.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-642-36654-3_6">Domain-Specific Modeling Languages: Requirements Analysis and Design Guidelines</a> - <strong><em>Domain Engineering: Product Lines, Languages, and Conceptual Models</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=15620404537599157753">All Versions</a>]. In recent years, the development of domain-specific modeling languages has gained remarkable attention. This is for good reasons. A domain-specific modeling language incorporates concepts that represent domain-level knowledge. Hence, systems analysts are not forced to reconstruct these concepts from scratch. At the same time, domain-specific modeling languages contribute to model integrity, because they include already constraints that would otherwise have to be added manually. Even though there has been a considerable amount of research on developing and using domain-specific modeling languages, there is still lack of comprehensive methods to guide the design of these languages. With respect to the complexity and risk related to developing a domain-specific modeling language, this is a serious shortfall. This chapter is aimed at a contribution to filling the gap. At first, it presents guidelines for selecting a metamodeling language. Its main focus is on supporting the process from analyzing requirements to specifying and evaluating a domain-specific modeling language.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0950584915001858">Domain-Specific Languages: A Systematic Mapping Study</a> - <strong><em>Information and Software Technology</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=10633158457161608253">All Versions</a>]. This study reports on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs). The main objective of the described work was to perform an SMS on DSLs to better understand the DSL research field, identify research trends, and any possible open issues. This SMS discusses two main research questions: research space and trends/demographics of the literature within the field of DSLs. Both research questions are further subdivided into several research sub-questions. The results from the first research question clearly show that the DSL community focuses more on the development of new techniques/methods rather than investigating the integrations of DSLs with other software engineering processes or measuring the effectiveness of DSL approaches. Furthermore, there is a clear lack of evaluation research. Amongst different DSL development phases more attention is needed in regard to domain analysis, validation, and maintenance. The second research question revealed that the number of publications remains stable, and has not increased over the years.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/full/10.1145/3638243">Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State of the Practice</a> - <strong><em>ACM Transactions on Software Engineering and Methodology</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=16602627179061103486">All Versions</a>]. Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of the proposed conceptual framework which presents the “route” of transformations that a domain expert may choose to take while developing their solution.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3452379">PLIERS: A Process that Integrates User-Centered Methods into Programming Language Design</a> - <strong><em>ACM Transactions on Computer-Human Interaction</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14808918880141361248">All Versions</a>]. Programming language design requires making many usability-related design decisions. However, existing HCI methods can be impractical to apply to programming languages: languages have high iteration costs, programmers require significant learning time, and user performance has high variance. To address these problems, the authors adapted both formative and summative HCI methods to make them more suitable for programming language design. The authors integrated these methods into a new process, PLIERS, for designing programming languages in a user-centered way. The authors assessed PLIERS by using it to design two new programming languages. Glacier extends Java to enable programmers to express immutability properties effectively and easily. Obsidian is a language for blockchains that includes verification of critical safety properties. Empirical studies showed that the PLIERS process resulted in languages that could be used effectively by many programmers and revealed additional opportunities for language improvement.</p>
</li>
</ul>
<h4 id="design-practises">Design Practises<a class="headerlink" href="#design-practises" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0164121214002799">Quantifying usability of domain-specific languages: An empirical study on software maintenance</a> - <strong><em>Journal of Systems and Software</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=3450893039446010260">All Versions</a>]. A DSL aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/2685028">A Taxonomy of Domain-Specific Aspect Languages</a> - <strong><em>ACM Computing Surveys</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=17254174131160041640">All Versions</a>]. Domain-Specific Aspect Languages (DSALs) are Domain-Specific Languages (DSLs) designed to express crosscutting concerns. Compared to DSLs, their aspectual nature greatly amplifies the language design space. This survey structures this space in order to shed light on and compare the different domain-specific approaches to deal with crosscutting concerns. This survey reports on a corpus of 36 DSALs covering the space, discuss a set of design considerations, and provide a taxonomy of DSAL implementation approaches. This work serves as a frame of reference to DSAL and DSL researchers, enabling further advances in the field, and to developers as a guide for DSAL implementations.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9904438">No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization</a> - <strong><em>IEEE Transactions on Visualization and Computer Graphics</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17206818917381447796">All Versions</a>]. There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. This study surveys and analyzes the design and implementation of 57 JSON-style DSLs for visualization. The authors analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. The authors identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, the authors seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3622851">How Domain Experts Use an Embedded DSL</a> - <strong><em>OOPSLA'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8416124186663074528">All Versions</a>]. Programming tools are increasingly integral to research and analysis in myriad domains, including specialized areas with no formal relation to computer science. Embedded domain-specific languages (eDSLs) have the potential to serve these programmers while placing relatively light implementation burdens on language designers. However, barriers to eDSL use reduce their practical value and adoption. This work aims to deepen the understanding of how programmers use eDSLs and identify user needs to inform future eDSL designs. The authors performed a contextual inquiry (9 participants) with domain experts using Mimi, an eDSL for climate change economics modeling. A thematic analysis identified five key themes, including: the interaction between the eDSL and the host language has significant and sometimes unexpected impacts on eDSL user experience, and users preferentially engage with domain-specific communities and code templates rather than host language resources.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-981-96-0780-8_9">Abstract Hardware Grounding Towards the Automated Design of Automation Systems</a> - <strong><em>ICIRA'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=3331524500088540378">All Versions</a>]. [<a href="https://arxiv.org/abs/2410.05663">Preprint</a>]. Crafting automation systems tailored for specific domains requires aligning the space of human experts’ semantics with the space of robot executable actions, and scheduling the required resources and system layout accordingly. Regrettably, there are three major gaps, fine-grained domain-specific knowledge injection, heterogeneity between human knowledge and robot instructions, and diversity of users’ preferences, resulting automation system design a case-by-case and labour-intensive effort, thus hindering the democratization of automation. This work refers to this challenging alignment as the abstract hardware grounding problem, where the authors firstly regard the procedural operations in humans’ semantics space as the abstraction of hardware requirements, then the authors ground such abstractions to instantiated hardware devices, subject to constraints and preferences in the real world—optimizing this problem is essentially standardizing and automating the design of automation systems. On this basis, this work develops an automated design framework in a hybrid data-driven and principle-derived fashion. Results on designing self-driving laboratories for enhancing experiment-driven scientific discovery suggest the proposed framework’s potential to produce compact systems that fully satisfy domain-specific and user-customized requirements with no redundancy.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10766486">Constraint Representation Towards Precise Data-Driven Storytelling</a> - <strong><em>VIS-Gen4DS'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=12234019078719898658">All Versions</a>]. [<a href="https://arxiv.org/abs/2410.07535">Preprint</a>]. A position paper on DSL for data-driven storytelling. Data-driven storytelling serves as a crucial bridge for communicating ideas in a persuasive way. However, the manual creation of data stories is a multifaceted, labor-intensive, and case-specific effort, limiting their broader application. As a result, automating the creation of data stories has emerged as a significant research thrust. Despite advances in Artificial Intelligence, the systematic generation of data stories remains challenging due to their hybrid nature: they must frame a perspective based on a seed idea in a top-down manner, similar to traditional storytelling, while coherently grounding insights of given evidence in a bottom-up fashion, akin to data analysis. These dual requirements necessitate precise constraints on the permissible space of a data story. This viewpoint proposes integrating constraints into the data story generation process. Defined upon the hierarchies of interpretation and articulation, constraints shape both narrations and illustrations to align with seed ideas and contextualized evidence. The authors identify the taxonomy and required functionalities of these constraints. Although constraints can be heterogeneous and latent, this position paper explores the potential to represent them in a computation-friendly fashion via Domain-Specific Languages. The authors believe that leveraging constraints will facilitate both artistic and scientific aspects of data story generation.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s44160-024-00649-8">Reproducibility in automated chemistry laboratories using computer science abstractions</a> - <strong><em>Nature Synthesis</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=2583939834455194329">All Versions</a>]. While abstraction is critical for the transferability of automated laboratory science in (bio)chemical and materials sciences, its improper implementation is a technical debt taken against the reproducibility of experimental results. Over the decades, computer science has developed guidelines and strategies for how abstractions are captured in programming languages---particularly concerning the substitutability of implementations of abstracted ideas and the clear definition of the contexts in which abstractions are used. However, few programming languages developed for automated experiments fully leverage the wisdom learned in computer science. To achieve collaborative sharing of scientific knowledge via automated laboratories, the way that experimental protocols are codified and interpreted by machine agents must use abstractions responsibly and with reproducibility, rather than solely transferability, at its core. This Review discusses how computer science principles of abstraction can be translated to create more reproducible automation as an enabler for the acceleration of collaborative research with self-driving laboratories.</p>
</li>
</ul>
<h4 id="design-automation">Design Automation<a class="headerlink" href="#design-automation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://aclanthology.org/2024.acl-long.659/">AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints</a> - <strong><em>ACL'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=588082932830954126">All Versions</a>]. [<a href="https://arxiv.org/abs/2406.12324">Preprint</a>]. [<a href="https://autodsl.org/procedure/papers/acl24shi.html">Project</a>]. The original paper on the automated design of DSLs, referred to as AutoDSL. Accurate representation of procedures in restricted scenarios, such as non-standardized scientific experiments, requires precise depiction of constraints. Unfortunately, Domain-Specific Language (DSL), as an effective tool to express constraints structurally, often requires case-by-case hand-crafting, necessitating customized, labor-intensive efforts. To overcome this challenge, this paper introduces the AutoDSL framework to automate DSL-based constraint design across various domains. Utilizing domain specified experimental protocol corpora, AutoDSL optimizes syntactic constraints and abstracts semantic constraints. Quantitative and qualitative analyses of the DSLs designed by AutoDSL across five distinct domains highlight its potential as an auxiliary module for language models, aiming to improve procedural planning and execution.</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=9nUBh4V6SA">Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs</a> - <strong><em>ICLR'25</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=6090102857833092474">All Versions</a>]. [<a href="https://autodsl.org/procedure/papers/iclr25shi.html">Project</a>]. Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, this work proposes a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. The authors further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.</p>
</li>
</ul>
<h4 id="imperative-dsl-applications">Imperative DSL Applications<a class="headerlink" href="#imperative-dsl-applications" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.aav2211">Organic synthesis in a modular robotic system driven by a chemical programming language</a> - <strong><em>Science</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13920677955690815682">All Versions</a>]. [<a href="https://www.chem.gla.ac.uk/cronin/images/pubs/387-Steiner-ScienceJan19.full.pdf">Preprint</a>]. [<a href="https://www.science.org/doi/10.1126/science.aav8816">Perspective: Democratizing synthesis by automation</a>]. This paper develops an autonomous compiler and robotic laboratory platform to synthesize organic compounds on the basis of standardized methods descriptions. The platform comprises conventional equipment such as round-bottom flasks, separatory funnels, and a rotary evaporator to maximize its compatibility with extant literature. The authors showcase the system with short syntheses of three common pharmaceuticals that proceeded comparably to manual synthesis.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41557-020-00596-9">Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine</a> - <strong><em>Nature Chemistry</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=18024303106901939347">All Versions</a>]. [<a href="https://eprints.gla.ac.uk/231947/">Preprint</a>]. Although the automatic synthesis of molecules has been established, each reaction class uses bespoke hardware. This means that the connection of multi-step syntheses in a single machine to run many different protocols and reactions is not possible, as manual intervention is required. This paper shows how the Chemputer synthesis robot can be programmed to perform many different reactions, including solid-phase peptide synthesis, iterative cross-coupling and accessing reactive, unstable diazirines in a single, unified system with high yields and purity. Developing universal and modular hardware that can be automated using one software system makes a wide variety of batch chemistry accessible. This is shown by the proposed system, which performed around 8,500 operations while reusing only 22 distinct steps in 10 unique modules, with the code able to access 17 different reactions. The authors also demonstrate a complex convergent robotic synthesis of a peptide reacted with a diazirine---a process requiring 12 synthetic steps.</p>
</li>
<li>
<p><a href="https://jbioleng.biomedcentral.com/track/pdf/10.1186/1754-1611-4-13.pdf">Biocoder: A programming language for standardizing and automating biology protocols</a> - <strong><em>Journal of Biological Engineering</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=0,5&amp;cluster=15572197190838916795">All Versions</a>]. [<a href="https://github.com/nmz787/BioCoder">Project</a>]. [<a href="https://www.microsoft.com/en-us/download/details.aspx?id=52556">Microsoft Page</a>] This paper introduces BioCoder, a C++ library that enables biologists to express the exact steps needed to execute a protocol. In addition to being suitable for automation, BioCoder converts the code into a readable, English-language description for use by biologists.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s44160-023-00473-6">Universal chemical programming language for robotic synthesis repeatability</a> - <strong><em>Nature Synthesis</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=3455106495990439366">All Versions</a>]. [<a href="https://www.chem.gla.ac.uk/cronin/images/pubs/rauschen-natsynthesisjan24.pdf">Preprint</a>]. The amount of chemical synthesis literature is growing quickly; however, it takes a long time to share and evaluate new processes among laboratories. This paper presents an approach that uses a universal chemical programming language (χDL) to encode and execute synthesis procedures for a variety of chemical reactions, including reductive amination, ring formation, esterification, carbon–carbon bond formation and amide coupling on four different hardware systems in two laboratories. With around 50 lines of code per reaction, the approach uses abstraction to efficiently compress chemical protocols.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-024-45444-3">An integrated self-optimizing programmable chemical synthesis and reaction engine</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=9157508627971047184">All Versions</a>]. Robotic platforms for chemistry are developing rapidly but most systems are not currently able to adapt to changing circumstances in real-time. This paper presents a dynamically programmable system capable of making, optimizing, and discovering new molecules which utilizes seven sensors that continuously monitor the reaction. By developing a dynamic programming language, the work demonstrates the 10-fold scale-up of a highly exothermic oxidation reaction, end point detection, as well as detecting critical hardware failures.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/full/10.1145/3604568">Building an Open Representation for Biological Protocols</a> - <strong><em>ACM Journal on Emerging Technologies in Computing Systems</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17225405546647782000">All Versions</a>]. Laboratory protocols are critical to biological research and development, yet difficult to communicate and reproduce across projects, investigators, and organizations. While many attempts have been made to address this challenge, there is currently no available protocol representation that is unambiguous enough for precise interpretation and automation, yet simultaneously “human friendly” and abstract enough to enable reuse and adaptation. The Laboratory Open Protocol language (LabOP) is a free and open protocol representation aiming to address this gap, building on a foundation of UML, Autoprotocol, Aquarium, SBOL RDF, and the Provenance Ontology. LabOP provides a linked-data representation both for protocols and for records of their execution and the resulting data, as well as a framework for exporting from LabOP for execution by either humans or laboratory automation. LabOP is currently implemented in the form of an RDF knowledge representation, specification document, and Python library, and supports execution as manual “paper protocols,” by Autoprotocol or by Opentrons. From this initial implementation, LabOP is being further developed as an open community effort.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606789">KnitScript: A Domain-Specific Scripting Language for Advanced Machine Knitting</a> - <strong><em>UIST'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=KnitScript%3A+A+Domain-Specific+Scripting+Language+for+Advanced+Machine+Knitting&amp;btnG=">All Versions</a>]. [<a href="https://pypi.org/project/knit-script/">Project</a>]. This paper presents KnitScript, a domain-specific machine knitting scripting language that supports computationally driven knitting designs. KnitScript provides a comprehensive virtual model of knitting machines, giving access to machine-level capabilities as they are needed while automating a variety of tedious and error-prone details.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11119-020-09770-y">A domain‑specifc language framework for farm management information systems in precision agriculture</a> - <strong><em>Precision Agriculture</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1495954486695213496">All Versions</a>]. This paper proposes a domain-specific language framework for the design and development of precision-agriculture FMISs, which copes with challenges on supporting the understandability, enhancing communication and analysis of the design decisions, and the communication among stakeholders.</p>
</li>
<li>
<p><a href="https://fse.studenttheses.ub.rug.nl/25731/">Corel: A DSL for Cooking Recipes</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=9477049800574267813">All Versions</a>]. [<a href="https://roorda.dev/recipes/0">Corel recipe page</a>]. [<a href="https://www.fao.org/infoods/infoods/tables-and-databases/faoinfoods-databases/en/">International Network of Food Data Systems (INFOODS)</a>]. The Corel DSL for cooking recipes enables understanding of and computation with ingredients, and can construct a nutrition label for the recipe.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/full/10.1145/3613905.3650756">"We Need Structured Output": Towards User-centered Constraints on Large Language Model Output</a> - <strong><em>CHI EA'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=12105435542197416648">All Versions</a>]. [<a href="https://research.google/pubs/we-need-structured-output-towards-user-centered-constraints-on-large-language-model-output/">Preprint</a>]. Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. This work surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. The authors identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. The authors conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.adg4320">Evolution-inspired engineering of nonribosomal peptide synthetases</a> - <strong><em>Science</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=1405723492554017729">All Versions</a>]. Many clinically used drugs are derived from natural microbial products that are assembled in a stepwise fashion by the condensation of amino acids or acyl groups. Using insights from evolutionary analysis, two independent groups now show that the cumbersome enzyme complexes that produce these molecules can be pieced together to create new products on demand---if one knows the right spot for joining the pieces. Working with nonribosomal peptide synthetases, Bozhüyük et al. developed an approach called XUT (“exchange unit between T domains”) and demonstrated the production of a proteasome inhibitor by an enzyme complex containing fragments of five separate systems. Mabesoone et al. worked with polyketide synthases, demonstrating facile deletion and insertion of conceptually similar exchange units, producing a large number of related polyketide products with diverse modifications. These approaches are an important step forward for rational engineering of large enzyme complexes for small-molecule drug discovery and production.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-024-54067-7">OCTOPUS: operation control system for task optimization and job parallelization via a user-optimal scheduler</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=4172377940787950525">All Versions</a>]. The material acceleration platform, empowered by robotics and artificial intelligence, is a transformative approach for expediting material discovery processes across diverse domains. However, the development of an operating system for material acceleration platform faces challenges in simultaneously managing diverse experiments from multiple users. Specifically, when it is utilized by multiple users, the overlapping challenges of experimental modules or devices can lead to inefficiencies in both resource utilization and safety hazards. To overcome these challenges, this work presents an operation control system for material acceleration platform, namely, OCTOPUS, which is an acronym for operation control system for task optimization and job parallelization via a user-optimal scheduler. OCTOPUS streamlines experiment scheduling and optimizes resource utilization through integrating its interface node, master node and module nodes. Leveraging process modularization and a network protocol, OCTOPUS ensures the homogeneity, scalability, safety and versatility of the platform. In addition, OCTOPUS embodies a user-optimal scheduler. Job parallelization and task optimization techniques mitigate delays and safety hazards within realistic operational environments, while the closed-packing schedule algorithm efficiently executes multiple jobs with minimal resource waste. Copilot of OCTOPUS is developed to promote the reusability of OCTOPUS for potential users with their own sets of lab resources, which substantially simplifies the process of code generation and customization through GPT recommendations and client feedback. This work offers a solution to the challenges encountered within the platform accessed by multiple users, and thereby will facilitate its widespread adoption in material development processes.</p>
</li>
</ul>
<h4 id="declarative-dsl-applications">Declarative DSL Applications<a class="headerlink" href="#declarative-dsl-applications" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.nature.com/articles/nbt.1666">The BioPAX community standard for pathway data sharing</a> - <strong><em>Nature Biotechnology</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=11368332679628594895">All Versions</a>]. [<a href="https://core.ac.uk/download/pdf/216139091.pdf">Preprint</a>]. Biological Pathway Exchange (BioPAX) is a standard language to represent biological pathways at the molecular and cellular level and to facilitate the exchange of pathway data. BioPAX can represent metabolic and signaling pathways, molecular and genetic interactions and gene regulation networks.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.abd7331">Learning the language of viral evolution and escape</a> - <strong><em>Science</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=13862653184613223515">All Versions</a>]. The ability for viruses to mutate and evade the human immune system and cause infection, called viral escape, remains an obstacle to antiviral and vaccine development. Understanding the complex rules that govern escape could inform therapeutic design. This work modeled viral escape with machine learning algorithms originally developed for human natural language. The authors identified escape mutations as those that preserve viral infectivity but cause a virus to look different to the immune system, akin to word changes that preserve a sentence’s grammaticality but change its meaning. With this approach, language models of influenza hemagglutinin, HIV-1 envelope glycoprotein (HIV Env), and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) Spike viral proteins can accurately predict structural escape patterns using sequence data alone. This study represents a promising conceptual bridge between natural language and viral evolution.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-023-39396-3">Artificial intelligence driven design of catalysts and materials for ring opening polymerization using a domain-specific language</a> - <strong><em>Nature Communications</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6595955912508683146">All Versions</a>]. [<a href="https://github.com/IBM/ibm-materials-notebook">Project</a>]. Advances in machine learning (ML) and automated experimentation are poised to vastly accelerate research in polymer science. Data representation is a critical aspect for enabling ML integration in research workflows, yet many data models impose significant rigidity making it difficult to accommodate a broad array of experiment and data types found in polymer science. This inflexibility presents a significant barrier for researchers to leverage their historical data in ML development. This work shows that a domain specific language, termed Chemical Markdown Language (CMDL), provides flexible, extensible, and consistent representation of disparate experiment types and polymer structures. CMDL enables seamless use of historical experimental data to fine-tune regression transformer (RT) models for generative molecular design tasks. The authors demonstrate the utility of this approach through the generation and the experimental validation of catalysts and polymers in the context of ring-opening polymerization---although the authors provide examples of how CMDL can be more broadly applied to other polymer classes. Critically, this work shows how the CMDL tuned model preserves key functional groups within the polymer structure, allowing for experimental validation. These results reveal the versatility of CMDL and how it facilitates translation of historical data into meaningful predictive and generative models to produce experimentally actionable output.</p>
</li>
<li>
<p><a href="https://docs.openlaw.io/">OpenLaw</a> - <strong><em>OpenLaw.io</em></strong>. It is now possible to model all or parts of legal agreements using code (smart contracts), decreasing the cost and friction of creating, securing, and generating binding legal agreements. Lawyers lack basic tools to build these dynamic, “smart” contracts in a way that is enforceable and understandable to a legal professional. OpenLaw is a technology stack to help power next generation "smart" legal agreements, with a domain-specific markup language, a integration framework, and a series of general applications.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s10994-021-06120-5">Scenic: a language for scenario specification and data generation</a> - <strong><em>Machine Learning</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=13790565080942515865">All Versions</a>]. This paper proposes a domain-specific language, Scenic, for describing scenarios that are distributions over scenes and the behaviors of their agents over time. Scenic combines concise, readable syntax for spatiotemporal relationships with the ability to declaratively impose hard and soft constraints over the scenario.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9169399">Domain Specific Language for Smart Contract Development</a> - <strong><em>ICBC'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16998538751745390273">All Versions</a>]. [<a href="http://eprints-dev5.cs.univie.ac.at/6341/1/PID6382125.pdf">Preprint</a>]. This research addresses the understanding hardness raised from the conceptual discrepancy between contractual clauses and corresponding code of the Solidity programming language, by the design and study of a domain-specific smart contract language based on higher level of abstraction that can be automatically transformed to an implementation.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0950584921002081">iContractML 2.0: A domain-specific language for modeling and deploying smart contracts onto multiple blockchain platforms</a> - <strong><em>Information and Software Technology</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=1548144959305241494">All Versions</a>]. Smart contracts play a vital role in many fields. Despite being called smart, the development of smart contracts is a tedious task beyond defining a set of contractual rules. In addition to business knowledge, coding a smart contract requires strong technical knowledge in a multiplex of new and rapidly changing domain-specific languages and blockchain platforms. The goal of this paper is to assist developers in building smart contracts independently from the language or the target blockchain platform. In which, this paper presents the second-generation smart contract language iContractML 2.0. iContractML 2.0 is an extensible framework that empowers developers to model and generate functional smart contract code that can be deployed onto multiple blockchain platforms.</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v130/lew21a.html">PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=2892523061439714130">All Versions</a>]. This work presents PClean, a probabilistic programming language (PPL) for leveraging dataset-specific knowledge to automate Bayesian cleaning, automating Bayesian approaches given the diversity of real-world error patterns and the hardness of inference.</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v139/tavares21a.html">A Language for Counterfactual Generative Models</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=2067748786482591497">All Versions</a>]. [<a href="https://github.com/zenna/Omega.jl">Project</a>]. This paper presents Omega, a probabilistic programming language with support for counterfactual inference. This feature is accomplished by introducing a new operator to probabilistic programming akin to Pearl’s do. </p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/6030048">Product Line Engineering Using Domain-Specific Languages</a> - <strong><em>ISPLC'11</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=17589685299346185442">All Versions</a>]. [<a href="https://voelter.de/data/pub/VoelterVisser-PLEusingDSLs.pdf">Preprint</a>]. This paper investigates the application of domain-specific languages in product line engineering (PLE). It starts by analyzing the limits of expressivity of feature models. Feature models correspond to context-free grammars without recursion, which prevents the expression of multiple instances and references. The authors then show how domain-specific languages (DSLs) can serve as a middle ground between feature modeling and programming. They can be used in cases where feature models are too limited, while keeping the separation between problem space and solution space provided by feature models. This work then categorizes useful combinations between configuration with feature model and construction with DSLs and provide an integration of DSLs into the conceptual framework of PLE. Finally the authors show how use of a consistent, unified formalism for models, code, and configuration can yield important benefits for managing variability and trace ability.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/9613674">A Domain-Specific Language for Product-Process-Resource Modeling</a> - <strong><em>ETFA'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=6006131184799036515">All Versions</a>]. This paper presents the design of the PPR-DSL to effectively and efficiently represent Product-Process-Resource (PPR) aspects and evaluate constraints defined for modeling PPR views in the Formalized Process Description standard (VDI 3682).</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11263-018-1103-5">Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars</a> - <strong><em>International Journal of Computer Vision</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8301697457354598778">All Versions</a>]. [<a href="https://yzhu.io/publication/scenesynthesis2018ijcv/paper.pdf">Preprint</a>]. This work proposes a systematic learning-based approach to the generation of massive quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D images thereof, with associated ground truth information, for the purposes of training, benchmarking, and diagnosing learning-based computer vision and robotics algorithms. In particular, the authors devise a learning-based pipeline of algorithms capable of automatically generating and rendering a potentially infinite variety of indoor scenes by using a stochastic grammar, represented as an attributed Spatial And-Or Graph, in conjunction with state-of-the-art physics-based rendering. The pipeline is capable of synthesizing scene layouts with high diversity, and it is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. It renders photorealistic RGB images of the generated scenes while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity, and material information (detailed to object parts), as well as environments (e.g., illuminations and camera viewpoints). The authors demonstrate the value of the synthesized dataset, by improving performance in certain machine-learning-based scene understanding tasks—depth and surface normal prediction, semantic segmentation, reconstruction, etc.---and by providing benchmarks for and diagnostics of trained models by modifying object attributes and scene properties in a controllable manner.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2410.16770">The Scene Language: Representing Scenes with Programs, Words, and Embeddings</a> - <strong><em>CVPR'25</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=8704845413716059914">All Versions</a>]. [<a href="https://ai.stanford.edu/~yzzhang/projects/scene-language/">Project</a>]. This paper introduces the Scene Language, a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. It represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. This representation can be inferred from pre-trained language models via a training-free inference technique, given text or image inputs. </p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.abc7531">A prometastatic splicing program regulated by SNRPA1 interactions with structured RNA elements</a> - <strong><em>Science</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9442501233912150085">All Versions</a>]. Pathological changes in alternative splicing patterns are considered a hallmark of cancer, yet the underlying regulatory programs that control this process remain largely unknown. A major obstacle to better understanding these programs is that the bioinformatic strategies commonly used for the discovery of cis-regulatory elements fail to capture the contribution of RNA secondary structure to regulatory information. To address this, this work had previously developed the computational framework TEISER (Tool for Eliciting Informative Structural Elements in RNA), which uses both RNA structural and sequence information to identify cis-regulatory elements that are informative of transcriptomic changes. Here, the authors introduce pyTEISER (pythonic TEISER), which incorporates experimentally derived and additional computationally predicted RNA structural information to investigate the RNA sequence and structural code that governs a broader range of RNA-related processes, including splicing and RNA processing, in addition to steady-state gene expression.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-025-00981-4">Goals as reward-producing programs</a> - <strong><em>Nature Human Behavior</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=10210797462358956689">All Versions</a>]. [<a href="https://github.com/guydav/goals-as-reward-producing-programs/">Project</a>]. People are remarkably capable of generating their own goals, beginning with child’s play and continuing into adulthood. Despite considerable empirical and computational work on goals and goal-oriented behaviour, models are still far from capturing the richness of everyday human goals. This work bridges this gap by collecting a dataset of human-generated playful goals (in the form of scorable, single-player games), modelling them as reward-producing programs and generating novel human-like goals through program synthesis. Reward-producing programs capture the rich semantics of goals through symbolic operations that compose, add temporal constraints and allow program execution on behavioural traces to evaluate progress. To build a generative model of goals, the authors learn a fitness function over the infinite set of possible goal programs and sample novel goals with a quality-diversity algorithm. Human evaluators found that model-generated goals, when sampled from partitions of program space occupied by human examples, were indistinguishable from human-created games. The authors also discovered that the model’s internal fitness scores predict games that are evaluated as more fun to play and more human-like.</p>
</li>
<li>
<p><a href="https://scholar.google.com/scholar?cluster=8196745813546421985">A Generalized Earley Parser for Human Activity Parsing and Prediction</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8196745813546421985">All Versions</a>]. Detection, parsing, and future predictions on sequence data (e.g., videos) require the algorithms to capture non-Markovian and compositional properties of high-level semantics. Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. This paper generalizes the Earley parser to parse sequence data which is neither segmented nor labeled. Given the output of an arbitrary probabilistic classifier, this generalized Earley parser finds the optimal segmentation and labels in the language defined by the input grammar. Based on the parsing results, it makes top-down future predictions. The proposed method is generic, principled, and widely applicable. Experiment results clearly show the benefit of the method for both human activity parsing and prediction on three video datasets.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06127-z">Algorithm for optimized mRNA design improves stability and immunogenicity</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=13852796394887781838">All Versions</a>]. Messenger RNA (mRNA) vaccines are being used to combat the spread of COVID-19, but they still exhibit critical limitations caused by mRNA instability and degradation, which are major obstacles for the storage, distribution and efficacy of the vaccine products. Increasing secondary structure lengthens mRNA half-life, which, together with optimal codons, improves protein expression. Therefore, a principled mRNA design algorithm must optimize both structural stability and codon usage. However, owing to synonymous codons, the mRNA design space is prohibitively large---for example, there are around 2.4 × 10632 candidate mRNA sequences for the SARS-CoV-2 spike protein. This poses insurmountable computational challenges. This work provides a simple and unexpected solution using the classical concept of lattice parsing in computational linguistics, where finding the optimal mRNA sequence is analogous to identifying the most likely sentence among similar-sounding alternatives6. The algorithm LinearDesign finds an optimal mRNA design for the spike protein in just 11 minutes, and can concurrently optimize stability and codon usage. LinearDesign substantially improves mRNA half-life and protein expression, and profoundly increases antibody titre by up to 128 times in mice compared to the codon-optimization benchmark on mRNA vaccines for COVID-19 and varicella-zoster virus. This result reveals the great potential of principled mRNA design and enables the exploration of previously unreachable but highly stable and efficient designs. This work is a timely tool for vaccines and other mRNA-based medicines encoding therapeutic proteins such as monoclonal antibodies and anti-cancer drugs7,8.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3386569.3392375">Penrose: from mathematical notation to beautiful diagrams</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7935816003253127595">All Versions</a>]. This work introduces a system called Penrose for creating mathematical diagrams. Its basic functionality is to translate abstract statements written in familiar math-like notation into one or more possible visual representations. Rather than rely on a fixed library of visualization tools, the visual representation is user-defined in a constraint-based specification language; diagrams are then generated automatically via constrained numerical optimization. The system is user-extensible to many domains of mathematics, and is fast enough for iterative design exploration. In contrast to tools that specify diagrams via direct manipulation or low-level graphics programming, Penrose enables rapid creation and exploration of diagrams that faithfully preserve the underlying mathematical meaning. The authors demonstrate the effectiveness and generality of the system by showing how it can be used to illustrate a diverse set of concepts from mathematics and computer graphics.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-030-37933-9_3">LegalLanguage: A Domain-Specific Language for Legal Contexts</a> - <strong><em>EEWC'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12637991160754186719">All Versions</a>]. Nowadays legal ontologies have been used in the legal domain, however, being poorly explored in legislative and production processes. This paper analyses the adoption of legal ontologies as a tool to support these processes, in particular, related to activities span from the submission of bills and their subsequent authoring and ratification. This paper introduces the state of the art of legal (or normative) ontologies; and also discusses some application examples. The analysis of this state of the art allows us to identify some problems, namely regarding the activities involving the authoring and validation of laws that tend to be very human-intensive and error-prone. As a consequence of this analysis, the authors introduce the LegalLanguage, a language particularly suitable for the authoring and specification of law(s) in a more rigorous and systematic way, that would allow to keep track different types of intra and inter-laws relationships (e.g., structural, order or temporal relationships between articles or even between laws). </p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3618351">GarmentCode: Programming Parametric Sewing Patterns</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=9890151343262605598">All Versions</a>]. Garment modeling is an essential task of the global apparel industry and a core part of digital human modeling. Realistic representation of garments with valid sewing patterns is key to their accurate digital simulation and eventual fabrication. However, little-to-no computational tools provide support for bridging the gap between high-level construction goals and low-level editing of pattern geometry, e.g., combining or switching garment elements, semantic editing, or design exploration that maintains the validity of a sewing pattern. This work suggests the first DSL for garment modeling - GarmentCode - that applies principles of object-oriented programming to garment construction and allows designing sewing patterns in a hierarchical, component-oriented manner. The programming-based paradigm naturally provides unique advantages of component abstraction, algorithmic manipulation, and free-form design parametrization. The authors additionally support the construction process by automating typical low-level tasks like placing a dart at a desired location. In the prototype garment configurator, users can manipulate meaningful design parameters and body measurements, while the construction of pattern geometry is handled by garment programs implemented with GarmentCode.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10700937">VMC: A Grammar for Visualizing Statistical Model Checks</a> - <strong><em>IEEE Transactions on Visualization and Computer Graphics</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=884768102185861577">All Versions</a>]. Visualizations play a critical role in validating and improving statistical models. However, the design space of model check visualizations is not well understood, making it difficult for authors to explore and specify effective graphical model checks. VMC defines a model check visualization using four components: (1) samples of distributions of checkable quantities generated from the model, including predictive distributions for new data and distributions of model parameters; (2) transformations on observed data to facilitate comparison; (3) visual representations of distributions; and (4) layouts to facilitate comparing model samples and observed data. This work contributes an implementation of VMC as an R package. The authors validate VMC by reproducing a set of canonical model check examples, and show how using VMC to generate model checks reduces the edit distance between visualizations relative to existing visualization toolkits. The findings of an interview study with three expert modelers who used VMC highlight challenges and opportunities for encouraging exploration of correct, effective model check visualizations.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3414685.3417831">RoboGrammar: graph grammar for terrain-optimized robot design</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7042593841334771437">All Versions</a>]. This work presents RoboGrammar, a fully automated approach for generating optimized robot structures to traverse given terrains. This framework represents each robot design as a graph, and uses a graph grammar to express possible arrangements of physical robot assemblies. Each robot design can then be expressed as a sequence of grammar rules. Using only a small set of rules the grammar can describe hundreds of thousands of possible robot designs. The construction of the grammar limits the design space to designs that can be fabricated. For a given input terrain, the design space is searched to find the top performing robots and their corresponding controllers. The authors introduce Graph Heuristic Search - a novel method for efficient search of combinatorial design spaces. In Graph Heuristic Search, the authors explore the design space while simultaneously learning a function that maps incomplete designs (e.g., nodes in the combinatorial search tree) to the best performance values that can be achieved by expanding these incomplete designs. Graph Heuristic Search prioritizes exploration of the most promising branches of the design space. To test the method the authors optimize robots for a number of challenging and varied terrains. The authors demonstrate that RoboGrammar can successfully generate nontrivial robots that are optimized for a single terrain or a combination of terrains.</p>
</li>
</ul>
<h4 id="logic-dsl-applications">Logic DSL Applications<a class="headerlink" href="#logic-dsl-applications" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Situation_calculus">Situation Calculus</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on Situation Calculus, a logic formalism designed for representing and reasoning about dynamical domains.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/3-540-46767-x_28">Answer Set Programming</a> - <strong><em>ICLPNR'99</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=15267370435063454675">All Versions</a>]. [<a href="http://people.sabanciuniv.edu/~esraerdem/teaching/krr06/asp.pdf">Preprint</a>]. The original paper on Answer Set Programming (ASP), a form of declarative programming oriented towards difficult search problems, on the use of nonmonotonic reasoning in knowledge representation. In ASP solutions to a problem are represented by answer sets (known also as stable models), and not by answer substitutions produced in response to a query, as in conventional logic programming.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F978-3-642-60085-2_16">Action Languages, Answer Sets, and Planning</a> - <strong><em>The Logic Programming Paradigms</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=2045126541850245645">All Versions</a>]. [<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=e58359b3dae3141fd2c85ee3f00c566411134929">Preprint</a>]. This is a discussion of some of the achievements and challenges related to representing actions and the design of planners from the perspective of logic programming. The authors talk about recent work on action languages and translating them into logic programming, on representing possible histories of an action domain by answer sets, on efficient implementations of the answer set semantics and their use for generating plans, and on causal logic and its relation to planning algorithms. Recent progress in these areas may lead to the creation of planners which are based on the ideas of logic programming and combine the use of expressive action description languages with efficient computational procedures.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370286900731">Qualitative Simulation</a> - <strong><em>Artificial Intelligence</em></strong>, 1986. [<a href="https://scholar.google.com/scholar?cluster=4945009733425184345&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://www.cs.utexas.edu/ftp/qsim/papers/Kuipers-aij-86.pdf">Preprint</a>]. This paper presents a precise definition of qualitative structure and behavior descriptions as abstractions of differential equations and continuously differentiable functions. The authors present a new algorithm for qualitative simulation that generalizes the best features of existing algorithms, and allows direct comparisons among alternate approaches. Starting with a set of constraints abstracted from a differential equation, this work proves that the QSIM algorithm is guaranteed to produce a qualitative behavior corresponding to any solution to the original equation. The paper also shows that any qualitative simulation algorithm will sometimes produce spurious qualitative behaviors: ones which do not correspond to any mechanism satisfying the given constraints. These observations suggest specific types of care that must be taken in designing applications of qualitative causal reasoning systems, and in constructing and validating a knowledge base of mechanism descriptions.</p>
</li>
<li>
<p><a href="https://www.cs.utexas.edu/users/qr/QR-book.html">Qualitative Reasoning: Modeling and Simulation with Incomplete Knowledge</a> - <strong><em>MIT Press</em></strong>, 1994. [<a href="https://scholar.google.com/scholar?&amp;cluster=6634684154722677465">All Versions</a>]. This book presents, within a conceptually unified theoretical framework, a body of methods that have been developed over the past fifteen years for building and simulating qualitative models of physical systems - bathtubs, tea kettles, automobiles, the physiology of the body, chemical processing plants, control systems, electrical systems - where knowledge of that system is incomplete. The primary tool for this work is the author's QSIM algorithm, which is discussed in detail. Qualitative models are better able than traditional models to express states of incomplete knowledge about continuous mechanisms. Qualitative simulation guarantees to find all possible behaviors consistent with the knowledge in the model. This expressive power and coverage is important in problem solving for diagnosis, design, monitoring, explanation, and other applications of artificial intelligence.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370297000507">Qualitative and quantitative simulation: bridging the gap</a> - <strong><em>Artificial Intelligence</em></strong>, 1997. [<a href="https://scholar.google.com/scholar?cluster=9033452473914228535">All Versions</a>]. Shortcomings of qualitative simulation and of quantitative simulation motivate combining them to do simulations exhibiting strengths of both. The resulting class of techniques is called semiquantitative simulation. One approach to semi-quantitative simulation is to use numeric intervals to represent incomplete quantitative information. This research demonstrates semi-quantitative simulation using intervals in an implemented semi-quantitative simulator called Q3. Q3 progressively refines a qualitative simulation, providing increasingly specific quantitative predictions which can converge to a numerical simulation in the limit while retaining important correctness guarantees from qualitative and interval simulation techniques.</p>
</li>
<li>
<p><a href="https://pubs.acs.org/doi/10.1021/acssynbio.8b00229">A Logic Programming Language for Computational Nucleic Acid Devices</a> - <strong><em>ACS Synthetic Biology</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3336951672389047784">All Versions</a>]. This paper presents a logic programming language that allows a broad range of computational nucleic acid systems to be designed and analyzed. The language extends standard logic programming with a novel equational theory to express nucleic acid molecular motifs. It automatically identifies matching motifs present in the full system, in order to apply a specified transformation expressed as a logical rule.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41596-021-00675-2">Genetic circuit design automation with Cello 2.0</a> - <strong><em>Nature Protocol</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=7418307542591684967">All Versions</a>]. [<a href="https://www.researchgate.net/profile/Samuel-Oliveira-38/publication/358801979_Genetic_circuit_design_automation_with_Cello_20/links/635debf412cbac6a3e0b19e4/Genetic-circuit-design-automation-with-Cello-20.pdf">Preprint</a>]. Cells interact with their environment, communicate among themselves, track time and make decisions through functions controlled by natural regulatory genetic circuits consisting of interacting biological components. Synthetic programmable circuits used in therapeutics and other applications can be automatically designed by computer-aided tools. The Cello software designs the DNA sequences for programmable circuits based on a high-level software description and a library of characterized DNA parts representing Boolean logic gates. This process allows for design specification reuse, modular DNA part library curation and formalized circuit transformations based on experimental data. This protocol describes Cello 2.0, a freely available cross-platform software written in Java. Cello 2.0 enables flexible descriptions of the logic gates’ structure and their mathematical models representing dynamic behavior, new formal rules for describing the placement of gates in a genome, a new graphical user interface, support for Verilog 2005 syntax and a connection to the SynBioHub parts repository software environment. Collectively, these features expand Cello’s capabilities beyond Escherichia coli plasmids to new organisms and broader genetic contexts, including the genome. Designing circuits with Cello 2.0 produces an abstract Boolean network from a Verilog file, assigns biological parts to each node in the Boolean network, constructs a DNA sequence and generates highly structured and annotated sequence representations suitable for downstream processing and fabrication, respectively. The result is a sequence implementing the specified Boolean function in the organism and predictions of circuit performance. Depending on the size of the design space and users’ expertise, jobs may take minutes or hours to complete.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2502.13372">MoVer: Motion Verification for Motion Graphics Animations</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=527747131334466686">All Versions</a>]. While large vision-language models can generate motion graphics animations from text prompts, they regularly fail to include all of spatio-temporal properties described in the prompt. This work introduces MoVer, a motion verification DSL based on first-order logic that can check spatio-temporal properties of a motion graphics animation. The authors identify a general set of such properties that people commonly use to describe animations (e.g., the direction and timing of motions, the relative positioning of objects, etc.). The authors implement these properties as predicates in MoVer and provide an execution engine that can apply a MoVer program to any input SVG-based motion graphics animation. The authors then demonstrate how MoVer can be used in an LLM-based synthesis and verification pipeline for iteratively refining motion graphics animations. Given a text prompt, the pipeline synthesizes a motion graphics animation and a corresponding MoVer program. Executing the verification program on the animation yields a report of the predicates that failed and the report can be automatically fed back to LLM to iteratively correct the animation.</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=C45YqeBDUM">The KoLMogorov Test: Compression by Code Generation</a> - <strong><em>ICLR'25</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=16809888292456252135">All Versions</a>]. Compression is at the heart of intelligence. A theoretically optimal way to compress any sequence of data is to find the shortest program that outputs that sequence and then halts. However, such Kolmogorov compression is uncomputable, and code generating LLMs struggle to approximate this theoretical ideal, as it requires reasoning, planning and search capabilities beyond those of current models. This work introduces the KoLMogorov-Test (KT), a compression-as-intelligence intelligence test for code generation LLMs. In KT a model is presented with a sequence of data at inference time, and asked to generate the shortest DSL (designed specifically for the task) program that produces the sequence. The authors identify several benefits of KT for both evaluation and training: an essentially infinite number of problem instances of varying difficulty is readily available, strong baselines already exist, the evaluation metric (compression) cannot be gamed, and pretraining data contamination is highly unlikely. To evaluate current models, the authors use audio, text, and DNA data, as well as sequences produced by random synthetic DSL programs. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41598-022-21801-4">Meta-analysis of the functional neuroimaging literature with probabilistic logic programming</a> - <strong><em>Scientific Reports</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=5952076495542489316">All Versions</a>]. Inferring reliable brain-behavior associations requires synthesizing evidence from thousands of functional neuroimaging studies through meta-analysis. However, existing meta-analysis tools are limited to investigating simple neuroscience concepts and expressing a restricted range of questions. This work expands the scope of neuroimaging meta-analysis by designing NeuroLang: a domain-specific language to express and test hypotheses using probabilistic first-order logic programming. By leveraging formalisms found at the crossroads of artificial intelligence and knowledge representation, NeuroLang provides the expressivity to address a larger repertoire of hypotheses in a meta-analysis, while seamlessly modeling the uncertainty inherent to neuroimaging data. The authors demonstrate the language’s capabilities in conducting comprehensive neuroimaging meta-analysis through use-case examples that address questions of structure-function associations. Specifically, the authors infer the specific functional roles of three canonical brain networks, support the role of the visual word-form area in visuospatial attention, and investigate the heterogeneous organization of the frontoparietal control network.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41597-024-03331-y">Prototyping an Ontological Framework for Cellular Senescence Mechanisms: A Homeostasis Imbalance Perspective</a> - <strong><em>Scientific Data</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=16297381933381807246">All Versions</a>]. Although cellular senescence is a key factor in organismal aging, with both positive and negative effects on individuals, its mechanisms remain largely unknown. Thus, integrating knowledge is essential to explain how cellular senescence manifests in tissue damage and age-related diseases. Here, this work proposes an ontological model that organizes knowledge of cellular senescence in a computer-readable form. The authors manually annotated and defined cellular senescence processes, molecules, anatomical structures, phenotypes, and other entities based on the Homeostasis Imbalance Process ontology (HOIP). The authors described the mechanisms as causal relationships of processes and modelled a homeostatic imbalance between stress and stress response in cellular senescence for a unified framework. HOIP was assessed formally, and the relationships between cellular senescence and diseases were inferred for higher-order knowledge processing. The authors visualized cellular senescence processes to support knowledge utilization. This study provides a knowledge base to help elucidate mechanisms linking cellular and organismal aging.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10128752">Knowledge-Based Embodied Question Answering</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=3297900861513340459">All Versions</a>]. This paper proposes a novel Knowledge-based Embodied Question Answering (K-EQA) task, in which the agent intelligently explores the environment to answer various questions with the knowledge. Different from explicitly specifying the target object in the question as existing EQA work, the agent can resort to external knowledge to understand more complicated question such as “Please tell me what are objects used to cut food in the room?”, in which the agent must know the knowledge such as “knife is used for cutting food”. To address this K-EQA problem, a novel framework based on neural program synthesis reasoning is proposed, where the joint reasoning of the external knowledge and 3D scene graph is performed to realize navigation and question answering. Especially, the 3D scene graph can provide the memory to store the visual information of visited scenes, which significantly improves the efficiency for the multi-turn question answering. Experimental results have demonstrated that the proposed framework is capable of answering more complicated and realistic questions in the embodied environment. The proposed method is also applicable to multi-agent scenarios.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9623348">Explainable Robotic Plan Execution Monitoring Under Partial Observability</a> - <strong><em>IEEE Transactions on Robotics</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8699998059430654104">All Versions</a>]. Successful plan generation for autonomous systems is necessary but not sufficient to guarantee reaching a goal state by an execution of a plan. Various discrepancies between an expected state and the observed state may occur during the plan execution (e.g., due to unexpected exogenous events, changes in the goals, or failure of robot parts) and these discrepancies may lead to plan failures. For that reason, autonomous systems should be equipped with execution monitoring algorithms so that they can autonomously recover from such discrepancies. This work introduces a plan execution monitoring algorithm that operates under partial observability. This algorithm relies on novel formal methods for hybrid prediction, diagnosis and explanation generation, and planning. The prediction module generates an expected state after the execution of a part of the plan from an incomplete state to check for discrepancies. The diagnostic reasoning module generates meaningful hypotheses to explain failures of robot parts. Unlike the existing diagnosis methods, the previous hypotheses can be revised, based on new partial observations, increasing the accuracy of explanations as further information becomes available. The replanning module considers these explanations while computing a new plan that would avoid such failures. All these reasoning modules are hybrid in that they combine high-level logical reasoning with low-level feasibility checks based on probabilistic methods. The authors experimentally show that these hybrid formal reasoning modules improve the performance of plan execution monitoring.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10494680">LogSay: An Efficient Comprehension System for Log Numerical Reasoning</a> - <strong><em>IEEE Transactions on Computers</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=17053857493674669692">All Versions</a>]. With the growth of smart systems and applications, high volume logs are generated that record important data for system maintenance. System developers are usually required to analyze logs to track the status of the system or applications. Therefore, it is essential to find the answers in large-scale logs when they have some questions. This work designs a multi-step “Retriever-Reader” question-answering system, namely LogSay, which aims at predicting answers accurately and efficiently. The proposed system can not only answers simple questions, such as a segment log or span, but also can answer complex logical questions through numerical reasoning. LogSay has two key components: Log Retriever and Log Reasoner, and the authors designed five operators to implement them. Log Retriever aims at retrieving some relevant logs based on a question. Then, Log Reasoner performs numerical reasoning to infer the final answer. In addition, due to the lack of available question-answering datasets for system logs, the authors constructed question-answering datasets based on three public log datasets and will make them publicly available. The evaluation results show that LogSay outperforms the state-of-the-art works in terms of accuracy and efficiency.</p>
</li>
</ul>
<h4 id="dsl-program-synthesis">DSL Program Synthesis<a class="headerlink" href="#dsl-program-synthesis" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2018/hash/6788076842014c83cedadbe6b0ba0314-Abstract.html">Learning to Infer Graphics Programs from Hand-Drawn Images</a> - <strong><em>NeurIPS'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=14065112485794121024">All Versions</a>]. The method learns a model that uses program synthesis techniques to recover a graphics program from drawing primitives. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3571207">babble: Learning Better Abstractions with E-Graphs and Anti-unification</a> - <strong><em>POPL'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=7935064016901049715">All Versions</a>]. This paper proposes library learning modulo theory (LLMT), a new library learning algorithm that additionally takes as input an equational theory for a given problem domain. LLMT uses e-graphs and equality saturation to compactly represent the space of programs equivalent modulo the theory, and uses a novel e-graph anti-unification technique to find common patterns in the corpus more directly and efficiently.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3571234">Top-Down Synthesis for Library Learning</a> - <strong><em>POPL'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=12324277007659029766">All Versions</a>]. This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus.</p>
</li>
<li>
<p><a href="https://royalsocietypublishing.org/doi/full/10.1098/rsta.2022.0050">DreamCoder: growing generalizable, interpretable knowledge with wake–sleep Bayesian program learning</a> - <strong><em>Philosophical Transactions of the Royal Society A</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11356436337624711843">All Versions</a>]. [<a href="https://arxiv.org/abs/2006.08381">Preprint</a>]. This paper presents DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating domain-specific programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ‘wake–sleep’ learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-022-32012-w">Synthesizing theories of human language with Bayesian program induction</a> - <strong><em>Nature Communications</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8603772394100237159">All Versions</a>]. Automated, data-driven construction and evaluation of scientific models and theories is a long-standing challenge in artificial intelligence. This work presents a framework for algorithmically synthesizing models of a basic part of human language: morpho-phonology, the system that builds word forms from sounds. The authors integrate Bayesian inference with program synthesis and representations inspired by linguistic theory and cognitive models of learning and discovery. Across 70 datasets from 58 diverse languages, the system synthesizes human-interpretable models for core aspects of each language’s morpho-phonology, sometimes approaching models posited by human linguists. Joint inference across all 70 data sets automatically synthesizes a meta-model encoding interpretable cross-language typological tendencies. Finally, the same algorithm captures few-shot learning dynamics, acquiring new morphophonological rules from just one or a few examples. These results suggest routes to more powerful machine-enabled discovery of interpretable models in linguistics and other scientific domains.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/cd40d0d65bfebb894ccc9ea822b47fa8-Abstract-Conference.html">Grammar Prompting for Domain-Specific Language Generation with Large Language Models</a> - <strong><em>NeurIPS'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11694070042468483715">All Versions</a>]. Grammar prompting is a simple approach to enable LLMs to use external knowledge and domain-specific constraints expressed through a grammar in Backus--Naur Form (BNF) during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2303.14100">Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=8063693456660536915">All Versions</a>]. [<a href="https://github.com/ac-rad/xdl-generation">Project</a>]. [<a href="https://ac-rad.github.io/clairify/">Website</a>]. This paper proposes CLAIRIFY, an approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/full/10.1145/3613904.3642319">PhotoScout: Synthesis-Powered Multi-Modal Image Search</a> - <strong><em>ACM SIGCHI'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=6522231014055730719">All Versions</a>]. This paper explores a new multi-modal image search approach that allows users to conveniently specify and perform semantic image search tasks. With the tool, PhotoScout, the user interactively provides natural language descriptions, positive and negative examples, and object tags to specify their search tasks. Under the hood, PhotoScout is powered by a program synthesis engine that generates visual queries in a domain-specific language and executes the synthesized program to retrieve the desired images.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/54dd9e0cff6d9214e20d97eb2a3bae49-Abstract-Conference.html">Expert-level protocol translation for self-driving labs</a> - <strong><em>NeurIPS'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13997597682274906943">All Versions</a>]. [<a href="https://autodsl.org/procedure/papers/neurips24shi.html">Project</a>]. Recent development in Artificial Intelligence (AI) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, this work proposes a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06924-6">Mathematical discoveries from program search with large language models</a> - <strong><em>Nature</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=5653439474813913484">All Versions</a>]. Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. This work introduces FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. The authors demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. The authors showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3654777.3676357">CoLadder: Manipulating Code Generation via Multi-Level Blocks</a> - <strong><em>UIST'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=6267750825019915015">All Versions</a>]. This paper adopted an iterative design process to gain insights into programmers’ strategies when using LLMs for programming. The authors proposed CoLadder, a novel system that supports programmers by facilitating hierarchical task decomposition, direct code segment manipulation, and result evaluation during prompt authoring. A user study with 12 experienced programmers showed that CoLadder is effective in helping programmers externalize their problem-solving intentions flexibly, improving their ability to evaluate and modify code across various abstraction levels, from their task’s goal to final code implementation.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3272127.3275006">InverseCSG: automatic conversion of 3D models to CSG trees</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3247081749407570405">All Versions</a>]. While computer-aided design is a major part of many modern manufacturing pipelines, the design files typically generated describe raw geometry. Lost in this representation is the procedure by which these designs were generated. This paper presents a method for reverse-engineering the process by which 3D models may have been generated, in the language of constructive solid geometry (CSG). Observing that CSG is a formal grammar, the authors formulate this inverse CSG problem as a program synthesis problem. The solution is an algorithm that couples geometric processing with state-of-the-art program synthesis techniques. In this scheme, geometric processing is used to convert the mixed discrete and continuous domain of CSG trees to a pure discrete domain where modern program synthesizers excel. The authors demonstrate the efficiency and scalability of the algorithm on several different examples, including those with over 100 primitive parts. The authors show that the algorithm is able to find simple programs which are close to the ground truth, and demonstrate the method's applicability in mesh re-editing. Finally, the authors compare the method to prior state-of-the-art. The authors demonstrate that the algorithm dominates previous methods in terms of resulting CSG compactness and runtime, and can handle far more complex input meshes than any previous method.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3220134.3220135">pix2code: Generating Code from a Graphical User Interface Screenshot</a> - <strong><em>ACM SIGCHI Symposium on Engineering Interactive Computing Systems</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8296741513177971931">All Versions</a>]. [<a href="https://github.com/tonybeltramelli/pix2code">Code</a>]. [<a href="https://uizard.io/research/">Website</a>]. This paper shows that deep learning methods can be leveraged to train a model end-to-end to automatically reverse engineer user interfaces and generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3528223.3530133">Free2CAD: parsing freehand drawings into CAD commands</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=5726506085191658159">All Versions</a>]. CAD modeling, despite being the industry-standard, remains restricted to usage by skilled practitioners due to two key barriers. First, the user must be able to mentally parse a final shape into a valid sequence of supported CAD commands; and second, the user must be sufficiently conversant with CAD software packages to be able to execute the corresponding CAD commands. As a step towards addressing both these challenges, this work presents Free2CAD wherein the user can simply sketch the final shape and the system parses the input strokes into a sequence of commands expressed in a simplified CAD language. When executed, these commands reproduce the sketched object. Technically, the authors cast sketch-based CAD modeling as a sequence-to-sequence translation problem, for which the authors leverage the powerful Transformers neural network architecture. Given the sequence of pen strokes as input, the authors introduce the new task of grouping strokes that correspond to individual CAD operations. The authors combine stroke grouping with geometric fitting of the operation parameters, such that intermediate groups are geometrically corrected before being reused, as context, for subsequent steps in the sequence inference. Although trained on synthetically-generated data, the authors demonstrate that Free2CAD generalizes to sketches created from real-world CAD models as well as to sketches drawn by novice users.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3414685.3417812">ShapeAssembly: learning to generate programs for 3D shape structure synthesis</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7510961460353268258">All Versions</a>]. Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable. This work takes a step towards achieving the best of both worlds for novel 3D shape synthesis. The authors propose ShapeAssembly, a domain-specific "assembly-language" for 3D shape structures. ShapeAssembly programs construct shape structures by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. ShapeAssembly functions are parameterized with continuous free variables, so that one program structure is able to capture a family of related shapes. The authors show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then, the authors train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. The approach leverages the strengths of each representation: the program captures the subset of shape variability that is interpretable and editable, and the deep generative model captures variability and correlations across shape collections that is hard to express procedurally.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3450626.3459821">ShapeMOD: macro operation discovery for 3D shape programs</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16783678518681779265">All Versions</a>]. A popular way to create detailed yet easily controllable 3D shapes is via procedural modeling, i.e. generating geometry using programs. Such programs consist of a series of instructions along with their associated parameter values. To fully realize the benefits of this representation, a shape program should be compact and only expose degrees of freedom that allow for meaningful manipulation of output geometry. One way to achieve this goal is to design higher-level macro operators that, when executed, expand into a series of commands from the base shape modeling language. However, manually authoring such macros, much like shape programs themselves, is difficult and largely restricted to domain experts. This paper presents ShapeMOD, an algorithm for automatically discovering macros that are useful across large datasets of 3D shape programs. ShapeMOD operates on shape programs expressed in an imperative, statement-based language. It is designed to discover macros that make programs more compact by minimizing the number of function calls and free parameters required to represent an input shape collection. The authors run ShapeMOD on multiple collections of programs expressed in a domain-specific language for 3D shape structures. The authors show that it automatically discovers a concise set of macros that abstract out common structural and parametric patterns that generalize over large shape collections. The authors also demonstrate that the macros found by ShapeMOD improve performance on downstream tasks including shape generative modeling and inferring programs from point clouds. Finally, the authors conduct a user study that indicates that ShapeMOD's discovered macros make interactive shape editing more efficient.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3592416">ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives</a> - <strong><em>ACM Transactions on Graphics</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=682217944594450121">All Versions</a>]. This work introduces ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across a dataset, so that programs rewritten with these abstractions are more compact, and suppress spurious degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stringent input assumptions. This is principally made possible by two methodological advancements: (a) a shape-to-program recognition network that learns to solve sub-problems and (b) the use of e-graphs, augmented with a conditional rewrite scheme, to determine when abstractions with complex parametric expressions can be applied, in a tractable manner. The authors evaluate ShapeCoder on multiple datasets of 3D shapes, where primitive decompositions are either parsed from manual annotations or produced by an unsupervised cuboid abstraction method. In all domains, ShapeCoder discovers a library of abstractions that captures high-level relationships, removes extraneous degrees of freedom, and achieves better dataset compression compared with alternative approaches. Finally, the authors investigate how programs rewritten to use discovered abstractions prove useful for downstream tasks.</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/full/10.1177/0278364919868279">Learning attribute grammars for movement primitive sequencing</a> - <strong><em>International Journal of Robotics Research</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3308554285811264465">All Versions</a>]. Movement primitives are a well studied and widely applied concept in modern robotics. However, composing primitives out of an existing library has shown to be a challenging problem. This work proposes the use of probabilistic context-free grammars to sequence a series of primitives to generate complex robot policies from a given library of primitives. The rule-based nature of formal grammars allows an intuitive encoding of hierarchically structured tasks. This hierarchical concept strongly connects with the way robot policies can be learned, organized, and re-used. However, the induction of context-free grammars has proven to be a complicated and yet unsolved challenge. The authors exploit the physical nature of robot movement primitives to restrict and efficiently search the grammar space. The grammar is learned by applying a Markov chain Monte Carlo optimization over the posteriors of the grammars given the observations. The proposal distribution is defined as a mixture over the probabilities of the operators connecting the search space. Moreover, the authors present an approach for the categorization of probabilistic movement primitives and discuss how the connectibility of two primitives can be determined. These characteristics in combination with restrictions to the operators guarantee continuous sequences while reducing the grammar space. In addition, a set of attributes and conditions is introduced that augments probabilistic context-free grammars in order to solve primitive sequencing tasks with the capability to adapt single primitives within the sequence. The method was validated on tasks that require the generation of complex sequences consisting of simple movement primitives using a seven-degree-of-freedom lightweight robotic arm.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10710633">LogiCode: An LLM-Driven Framework for Logical Anomaly Detection</a> - <strong><em>IEEE Transactions on Automation Science and Engineering</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=4279830355362202335">All Versions</a>]. This paper presents LogiCode, a novel framework that leverages Large Language Models (LLMs) for identifying logical anomalies in industrial settings, moving beyond the traditional focus on structural inconsistencies. By harnessing LLMs for logical reasoning, LogiCode autonomously generates Python codes to pinpoint anomalies such as incorrect component quantities or missing elements, marking a significant leap forward in anomaly detection technologies. A custom dataset “LOCO-Annotations” and a benchmark “LogiBench” are introduced to evaluate the LogiCode’s performance across various metrics including binary classification accuracy, code generation success rate, and precision in reasoning. Findings demonstrate LogiCode’s enhanced interpretability, significantly improving the accuracy of logical anomaly detection and offering detailed explanations for identified anomalies. This represents a notable shift towards more intelligent, LLM-driven approaches in industrial anomaly detection, promising substantial impacts on industry-specific applications. </p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3385398">Synthesis of Incremental Linear Algebra Programs</a> - <strong><em>ACM Transactions on Database Systems</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=18222415331047706008">All Versions</a>]. This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. This work presents LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying the framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. The authors evaluate the effectiveness of the framework on a wide range of applications from regression models to graph computations.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/0c1e94af650f5c74b1f3da467c2308c2-Abstract-Conference.html">Enhancing Robot Program Synthesis Through Environmental Context</a> - <strong><em>NeurIPS'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17630134586050451683">All Versions</a>]. Program synthesis aims to automatically generate an executable program that conforms to the given specification. Recent advancements have demonstrated that deep neural methodologies and large-scale pretrained language models are highly proficient in capturing program semantics. For robot programming, prior works have facilitated program synthesis by incorporating global environments. However, the assumption of acquiring a comprehensive understanding of the entire environment is often excessively challenging to achieve. This work presents a framework that learns to synthesize a program by rectifying potentially erroneous code segments, with the aid of partially observed environments. To tackle the issue of inadequate attention to partial observations, the authors propose to first learn an environment embedding space that can implicitly evaluate the impacts of each program token based on the precondition. Furthermore, by employing a graph structure, the model can aggregate both environmental and syntactic information flow and furnish smooth program rectification guidance. Extensive experimental evaluations and ablation studies on the partially observed VizDoom domain authenticate that the method offers superior generalization capability across various tasks and greater robustness when encountering noises.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/full/10.1145/3697012">On the Effectiveness of Large Language Models in Domain-Specific Code Generation</a> - <strong><em>ACM Transactions on Software Engineering and Methodology</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=9022735489715766344">All Versions</a>]. Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., Web, game, and math). This work conducts an in-depth study of the LLMs in domain-specific code generation. The results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. The authors further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, the authors further investigate how to effectively incorporate API knowledge into the code generation process. The authors experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. The authors refer to these strategies as a new code generation approach called DomCoder. Experimental results show that all strategies of DomCoder improve the effectiveness of domain-specific code generation under certain settings.</p>
</li>
</ul>
<h4 id="cognitive-foundations">Cognitive Foundations<a class="headerlink" href="#cognitive-foundations" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30174-1">The Child as Hacker</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13128656954836679743">All Versions</a>]. The scope of human learning and development poses a radical challenge for cognitive science. The authors propose that developmental theories can address this challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. The authors specifically propose that children’s learning is analogous to a particular style of programming called hacking, making code better along many dimensions through an open-ended set of goals and activities. By contrast to existing theories, which depend primarily on local search and simple metrics, this view highlights the many features of good mental representations and the multiple complementary processes children use to create them.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-025-02227-0">How laypeople evaluate scientific explanations containing jargon</a> - <strong><em>Nature Human Behavior</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=6467855047925175367">All Versions</a>]. Individuals rely on others’ expertise to achieve a basic understanding of the world. But how can non-experts achieve understanding from explanations that, by definition, they are ill-equipped to assess? Across 9 experiments with 6,698 participants (Study 1A = 737; 1B = 734; 1C = 733; 2A = 1,014; 2B = 509; 2C = 1,012; 3A = 1,026; 3B = 512; 4 = 421), this work addresses this puzzle by focusing on scientific explanations with jargon. The authors identify ‘when’ and ‘why’ the inclusion of jargon makes explanations more satisfying, despite decreasing their comprehensibility. The authors find that jargon increases satisfaction because laypeople assume the jargon fills gaps in explanations that are otherwise incomplete. The authors also identify strategies for debiasing these judgements: when people attempt to generate their own explanations, inflated judgements of poor explanations with jargon are reduced, and people become better calibrated in their assessments of their own ability to explain.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/182aed0379591ebd1d655b2bdc152075-Abstract-Datasets_and_Benchmarks.html">Communicating Natural Programs to Humans and Machines</a> - <strong><em>NeurIPS'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=13381039702346039142">All Versions</a>]. While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. This makes building intelligent systems that can generalize to novel situations such as ARC difficult. Human-generated instructions are referred as “natural programs”. While they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-024-50966-x">Symbolic metaprogram search improves learning efficiency and explains rule learning in humans</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=7670274141609367282">All Versions</a>]. Symbolic models based on program learning successfully explain rule-learning in many domains, but performance degrades quickly as program complexity increases. It remains unclear how to scale symbolic rule-learning methods to model human performance in challenging domains. This work shows that symbolic search over the space of metaprograms—programs that revise programs—dramatically improves learning efficiency. On a behavioral benchmark of 100 algorithmically rich rules, this approach fits human learning more accurately than alternative models while also using orders of magnitude less search. The computation required to match median human performance is consistent with conservative estimates of human thinking time. The results suggest that metaprogram-like representations may help human learners to efficiently acquire rules.</p>
</li>
</ul>
<h3 id="problem-solving">Problem Solving<a class="headerlink" href="#problem-solving" title="Permanent link">&para;</a></h3>
<h4 id="human-level-problem-solving">Human-Level Problem Solving<a class="headerlink" href="#human-level-problem-solving" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://psycnet.apa.org/record/1959-07883-001">Elements of a theory of human problem solving</a> - <strong><em>Psychological Review</em></strong>, 1958. [<a href="https://scholar.google.com/scholar?cluster=6226995019045187501">All Versions</a>]. Herbert Simon's original idea on human problem solving.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1973-10478-000">Human Problem Solving</a> - <strong><em>Englewood Cliffs, NJ: Prentice-hall</em></strong>, 1972. [<a href="https://scholar.google.com/scholar?cluster=3996229083126262536&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Herbert Simon's classic idea of human problem solving as search.</p>
</li>
<li>
<p><a href="http://196.223.158.148/bitstream/handle/123456789/2978/596.pdf?sequence=1&amp;isAllowed=y">Learning to Solve Problems: A Handbook for Designing Problem-Solving Learning Environments</a> - <strong><em>Taylorfrancis</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=13262690779319271809&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.science.org/doi/abs/10.1126/science.185.4157.1124">Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty</a> - <strong><em>Science</em></strong>, 1974. [<a href="https://scholar.google.com/scholar?cluster=17040257859216791312&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Daniel Kahneman's classic idea of prospective theory.</p>
</li>
<li>
<p><a href="https://www.cnbc.cmu.edu/braingroup/papers/sarafyazd_jazayeri_2019.pdf">Hierarchical reasoning by neural circuits in the frontal cortex</a> - <strong><em>Science</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9875733886908769773&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Neuroscience evidence supporting rule switch.</p>
</li>
<li>
<p><a href="https://oar.princeton.edu/rt4ds/file/11875/2161">The importance of mixed selectivity in complex cognitive tasks</a> - <strong><em>Nature</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=2707751672275136220&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper introducing mixed selectivity with high-dimensional neural representations.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-022-04743-9">People construct simplified mental representations to plan</a> - <strong><em>Nature</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=12068944400080889789&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A computational account on rational problem representation in human planning.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661322002819">Goals, usefulness and abstraction in value-based choice</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6256990098976657651&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. A review that outlines the computational and biological principles that enable the brain to compute the usefulness of an option or action by creating abstractions that flexibly adapt to changing goals.</p>
</li>
<li>
<p><a href="https://elifesciences.org/articles/68943">Value signals guide abstraction during learning</a> - <strong><em>eLife</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=10324834842795908439&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/BF00058926">Learning to perceive and act by trial and error</a> - <strong><em>Machine Learning</em></strong>, 1991. [<a href="https://scholar.google.com/scholar?cluster=1987606770603964473&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1801_3">Representations in distributed cognitive tasks</a> - <strong><em>Cognitive Science</em></strong>, 1994. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14781266698447195483">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0364021399800226">The nature of external representations in problem solving</a> - <strong><em>Cognitive Science</em></strong>, 1997. [<a href="https://scholar.google.com/scholar?cluster=10698887231200401430&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cognitivesciencesociety.org/cogsci20/papers/0765/0765.pdf">Abstract strategy learning underlies flexible transfer in physical problem solving</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Abstract+strategy+learning+underlies+flexible+transfer+in+physical+problem+solving.&amp;btnG=">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=CXyZrKPz4CU">Physion: Evaluating Physical Prediction from Vision in Humans and Machines</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8733318111076645893&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2352154620301236">Exploration: from machines to humans</a> - <strong><em>Current Opinion in Behavioral Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8015078432419172621&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2352154620301467">Balancing exploration and exploitation with information and randomization</a> - <strong><em>Current Opinion in Behavioral Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8164388137243077863&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0092867421008369">Hippocampal neurons construct a map of an abstract value space</a> - <strong><em>Cell</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12658820581876003172&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cpilab.org/pubs/Dasgupta2018Learning.pdf">Learning to act by integrating mental simulations and physical experiments</a> - <strong><em>CogSci'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7342920174595829739&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/ishita-dg/SimulationVSAction">Code</a>].</p>
</li>
<li>
<p><a href="https://gershmanlab.com/pubs/Momennejad17.pdf">The successor representation in human reinforcement learning</a> - <strong><em>Nature Human Behavior</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7317529612823134939&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="planning">Planning<a class="headerlink" href="#planning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://jair.org/index.php/jair/article/view/11175">From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning</a> - <strong><em>Journal of Artificial Intelligence Research</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17962480659445514879">All Versions</a>]. This work considers the problem of constructing abstract representations for planning in high-dimensional, continuous environments. The authors assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions. The authors first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. The authors then consider probabilistic planning, which they show requires generalizing from sets of states to distributions over states. Finally, the authors apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from sensorimotor data---point clouds, map locations, and joint angles---and then plans using that representation.</p>
</li>
<li>
<p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-control-091420-084139">Integrated Task and Motion Planning</a> - <strong><em>Annual Review of Control, Robotics, and Autonomous Systems</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=478421650694199529">All Versions</a>]. The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete–continuous mathematical programming, and continuous motion planning and thus cannot be effectively addressed by any of these fields directly. In this article, the authors define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.</p>
</li>
<li>
<p><a href="https://dspace.mit.edu/handle/1721.1/126626">Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning</a> - <strong><em>Robotics: Science and Systems</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10342169019935480143&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://gershmanlab.com/pubs/Dasgupta18_simulation.pdf">Learning to act by integrating mental simulations and physical experiments</a> - <strong><em>CogSci'21</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7342920174595829739&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/ftr/10.1111/cogs.12928">What Is the Model in Model-Based Planning?</a> - <strong><em>Cognitive Science</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10598397017491369972&amp;hl=en&amp;scisbd=1&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2109.11082.pdf">Discovering State and Action Abstractions for Generalized Task and Motion Planning</a> - <strong><em>AAAI'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1054368060554971920">All Versions</a>].</p>
</li>
</ul>
<h4 id="intrinsic-motivation">Intrinsic Motivation<a class="headerlink" href="#intrinsic-motivation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2004/hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html">Intrinsically Motivated Reinforcement Learning</a> - <strong><em>NeurIPS'04</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=9736217847061704054&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on intrinsic reward functions in classic reinforcement learning.</p>
</li>
<li>
<p><a href="https://www.frontiersin.org/articles/10.3389/neuro.12.006.2007/full">What is intrinsic motivation? A typology of computational approaches</a> - <strong><em>Frontiers in Neurorobotics</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11901343819872275353&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.jair.org/index.php/jair/article/view/12087">Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study</a> - <strong><em>Journal of Artificial Intelligence Research</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5309595875334344707&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v70/pathak17a.html">Curiosity-driven Exploration by Self-supervised Prediction</a> - <strong><em>ICML'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=9379743003299559904&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on curiosity as intrinsic motivation.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1706.01502">UCB Exploration via Q-Ensembles</a> - 2017. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=13260404166621290240">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2010.03110">Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4880520597219138666&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2015/hash/e00406144c1e7e35240afed70f34166a-Abstract.html">Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</a> - <strong><em>NeurIPS'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=9262504233068870193&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on empowerment as intrinsic motivation.</p>
</li>
<li>
<p><a href="https://psyarxiv.com/ybs7g/">Intrinsic Exploration as Empowerment in a Richly Structured Online Game</a> - 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12321757821600526668">All Versions</a>].</p>
</li>
<li>
<p><a href="https://gershmanlab.com/pubs/Tomov21.pdf">Multi-task reinforcement learning in humans</a> - <strong><em>Nature Human Behavior</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14589018692074515644&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10778628">JARVIS-1: Open-World Multi-Task Agents With Memory-Augmented Multimodal Language Models</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>. [<a href="https://scholar.google.com/scholar?cluster=12845806504666245406">All Versions</a>]. Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. This work introduces JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, the authors develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. JARVIS-1 is outfitted with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. </p>
</li>
</ul>
<h4 id="reinforcement-learning">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.andrew.cmu.edu/user/rmorina/papers/SuttonBook.pdf">Reinforcement learning: An introduction</a> - <strong><em>MIT Press</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8821915215029978039&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Richard Sutton's comprehensive book on reinforcement learning.</p>
</li>
<li>
<p><a href="https://www.jair.org/index.php/jair/article/view/10166">Reinforcement learning: A survey</a> - <strong><em>Journal of Artificial Intelligence Research</em></strong>, 1996. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=4983604491168613713">All Versions</a>]. Leslie Kaelbling's review on reinforcement learning.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2011.00583.pdf">An overview of multi-agent reinforcement learning from game theoretical perspective</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=16197919002723407603&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Yaodong Yang's review on multi-agent reinforcement learning from the perspective of game theory.</p>
</li>
<li>
<p><a href="https://klab.tch.harvard.edu/academia/classes/Neuro230/ReadingAssignments/MnihEtAlHassibis15NatureControlDeepRL.pdf">Human-level control through deep reinforcement learning</a> - <strong><em>Nature</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=12439121588427761338&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on solving Atari games via Deep Q-Network.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370299000521">Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</a> - <strong><em>Artificial Intelligence</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=1471968208408231068&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on operation reinforcement learning.</p>
</li>
<li>
<p><a href="http://oucsace.cs.ohio.edu/~chelberg/classes/680/paperPresentations/NathanPaperToPresent.pdf">On Monte Carlo Tree Search and Reinforcement Learning</a> - <strong><em>Journal of Artificial Intelligence Research</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=5805718077259491860&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1805.00909">Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review</a> - 2018. [<a href="https://scholar.google.com/scholar?cluster=16437288987337534404&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-15.pdf">Slides</a>]. Sergey Levine's tutorial on treating reinforcement learning probabilisticly.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2019/hash/4a46fbfca3f1465a27b210f4bdfe6ab3-Abstract.html">A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7721047641895252765&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=9SS69KwomAM">Solving Compositional Reinforcement Learning Problems via Task Reduction</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15628616147808752058&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8460689">Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</a> - <strong><em>ICRA'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7155333517647976638&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://academic.oup.com/logcom/article-abstract/28/2/337/4695480">Learning to act: qualitative learning of deterministic action models</a> - <strong><em>Journal of Logic and Computation</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14570482854600886953&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2109.06076">Learning to Act and Observe in Partially Observable Domains</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=2258600434630687063&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2107.06277">Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9640851185758072663&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A formal treatment on the generalization problem in reinforcement learning.</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=r1nTpv9eg">Learning to Perform Physics Experiments via Deep Reinforcement Learning</a> - <strong><em>ICLR'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=13142558595749186250&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9387127">Data-Efficient Learning for Complex and Real-Time Physical Problem Solving Using Augmented Simulation</a> - <strong><em>Robotics and Automation Letters</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=3140653562829320759&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.jmlr.org/papers/volume18/16-634/16-634.pdf">A Survey of Preference-Based Reinforcement Learning Methods</a> - <strong><em>Journal of Machine Learning Research</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=13278778479251450967&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://papers.NeurIPS.cc/paper/2021/file/4079016d940210b4ae9ae7d41c4a2065-Paper.pdf">On the Expressivity of Markov Reward</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4524686816939437211&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A formal treatment of tasks and rewards in reinforcement learning modeling.</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v37/schulman15.html">Trust Region Policy Optimization</a> - <strong><em>ICML'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=4215501129336400677&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper introducing TRPO, a method for optimizing control policies, with guaranteed monotonic improvement.</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf">Constrained Policy Optimization</a> - <strong><em>ICML'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6114366704163518185&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on constrained reinforcement learning (safe reinforcement learning).</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html">When to Trust Your Model: Model-Based Policy Optimization</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4248859125840907707&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://bair.berkeley.edu/blog/2019/12/12/mbpo/">Post</a>].</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v139/lee21g.html">SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8840831494454574191&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/pokaxpoka/sunrise">Code</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2202.13252">The Quest for a Common Model of the Intelligent Decision Maker</a> - <strong><em>Multi-disciplinary Conference on Reinforcement Learning and Decision Making'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=7652784232757502910&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Richard Sutton's perspective on the future directions of reinforcement learning research.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.5555/3491440.3492111">Automatic curriculum learning for deep RL: a short survey</a> - <strong><em>IJCAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10660055557098312214&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v139/romac21a.html">TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11016662361926634008&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/flowersteam/TeachMyAgent">Project</a>].</p>
</li>
</ul>
<h4 id="inverse-reinforcement-learning">Inverse Reinforcement Learning<a class="headerlink" href="#inverse-reinforcement-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/1015330.1015430">Apprenticeship Learning via Inverse Reinforcement Learning</a> - <strong><em>ICML'04</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=10260011060619377707&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Pieter Abbeel and Andrew Ng's original paper on inverse reinforcement learning (IRL).</p>
</li>
<li>
<p><a href="https://www.ijcai.org/Proceedings/07/Papers/416.pdf">Bayesian Inverse Reinforcement Learning</a> - <strong><em>IJCAI'07</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=4154724070362583557&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Bayesian account on classic inverse reinforcement learning.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1902.07742">From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following</a> - <strong><em>ICLR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9128320307925997063&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1904.06317.pdf">Few-shot Bayesian imitation learning with logical program policies.</a> - <strong><em>AAAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5103854692762145813&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://export.arxiv.org/pdf/2011.09854">Generalized Inverse Planning: Learning Lifted non-Markovian Utility for Generalizable Task Representation</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=18369106870663956780&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v139/malik21a.html">Inverse Constrained Reinforcement Learning</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Inverse+Constrained+Reinforcement+Learning+S+Malik&amp;btnG=">All Versions</a>].</p>
</li>
</ul>
<h3 id="system-1-system-2">System 1 &amp; System 2<a class="headerlink" href="#system-1-system-2" title="Permanent link">&para;</a></h3>
<h4 id="dual-coding-theory">Dual-Coding Theory<a class="headerlink" href="#dual-coding-theory" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://zh.pb1lib.org/book/1004349/825277">Mental Representations: A Dual Coding Approach</a> - <strong><em>Oxford University Press</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0,5&amp;q=mental+representations:+a+dual+coding+approach">All Versions</a>]. The original book on dual coding theory, in the neuroscience account of mental representation.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001765">Dual coding of knowledge in the human brain</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11751507203561842501&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Yanchao Bi's review on neuroscience experiments on dual coding theory.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0896627320302798">Two Forms of Knowledge Representations in the Human Brain</a> - <strong><em>Neuron</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16941965185680116049">All Versions</a>]. Illustrating language-derived and sensory-derived knowledge.</p>
</li>
<li>
<p><a href="http://bilab.bnu.edu.cn/paper/2018/Wang_2018_Cerebral_Cortex.pdf">Organizational Principles of Abstract Words in the Human Brain</a> - <strong><em>Cerebral Cortex</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=15272192531353715481">All Versions</a>].</p>
</li>
<li>
<p><a href="http://bilab.bnu.edu.cn/paper/2022/Fu_2022_CC.pdf">Different computational relations in language are captured by distinct brain systems</a> - <strong><em>Cerebral Cortex</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=720215181903530260&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://europepmc.org/article/med/28190038">The Deese-Roediger-McDermott (DRM) task: A simple cognitive paradigm to investigate false memories in the laboratory</a> - <strong><em>Journal of Visualized Experiments</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=10880194606861797581&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://mri-q.com/uploads/3/4/5/7/34572113/gallant_piis0896627312009348.pdf">A continuous semantic space describes the representation of thousands of object and action categories across the human brain</a> - <strong><em>Neuron</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=10348115268396987731&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-021-01259-6">Rational arbitration between statistics and rules in human sequence processing</a> - <strong><em>Nature Human Behavior</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=9856085207409198966&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="neural-symbolic-ai">Neural-Symbolic AI<a class="headerlink" href="#neural-symbolic-ai" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://academic.oup.com/nsr/article/12/10/nwaf339/8237444">How large language models need symbolism</a> - <strong><em>National Science Review</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=9866048591424428891">All Versions</a>]. Advances in artificial intelligence (AI), particularly large language models (LLMs), have achieved remarkable success. This progress stems from ‘scaling laws’---performance improves with greater computation, data and model size. However, this paradigm has a crucial vulnerability: scaling laws are effective only when data are abundant. A foundational process for this is the human ability of ‘quotienting’: creating a compact symbolic space from a vast problem space, in a process akin to forming a mathematical quotient space via equivalence classes. The new strategy uses symbols as vessels for compressed human wisdom, creating maps that guide the powerful statistical intuition of LLMs, which is especially critical for tackling complex problems where usable data are, by nature, sparse. The next frontier for AI will not be conquered by scaling alone. The art of symbolization itself—the crafting of powerful abstractions—is therefore the central task ahead. If scaling laws have given models their powerful intuition, it is the art of the symbol that will provide the compass for genuine discovery.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-642-59789-3_58">Regression Analysis for Interval-Valued Data</a> - <strong><em>Data Analysis, Classification, and Related Methods</em></strong>, 2000. [<a href="https://scholar.google.com/scholar?cluster=9407097855380377791&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on symbolic regression.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-7908-1709-6_20">Symbolic data analysis: what is it?</a> - <strong><em>Proceedings in Computational Statistics</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3730437602749399283&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1805.10872">DeepProbLog: Neural Probabilistic Logic Programming</a> - <strong><em>NeurIPS'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6079567413300944995&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on neuro-symbolic probabilistic programming.</p>
</li>
<li>
<p><a href="https://www.jair.org/index.php/jair/article/view/11172">Learning Explanatory Rules from Noisy Data</a> - <strong><em>Journal of Artificial Intelligence Research</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2553893814364678772&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper for differential Inductive Logic Programming.</p>
</li>
<li>
<p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/aaai17lasin.pdf">Combining Logical Abduction and Statistical Induction: Discovering Written Primitives with Human Knowledge</a> - <strong><em>AAAI'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14477085725208589393&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1904.10729.pdf">Neural Logic Reinforcement Learning</a> - <strong><em>ICML'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=18074632043038701502&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://papers.NeurIPS.cc/paper/8548-bridging-machine-learning-and-logical-reasoning-by-abductive-learning">Bridging Machine Learning and Logical Reasoning by Abductive Learning.</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1518342375288126288&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://daiwz.net/org/slides/ABL-meetup.html#/slide-title">Slides</a>]. [<a href="https://github.com/AbductiveLearning/ABL-HED">Code</a>]. The original paper on Abductive Learning, a derivative-free approach for neuro-symbolic learning.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11432-018-9801-4">Abductive learning: towards bridging machine learning and logical reasoning</a> - <strong><em>Science China Information Sciences</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8541635351775190855&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2010.03514.pdf">Abductive Knowledge Induction From Raw Data</a> - <strong><em>IJCAI'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7027142960863064076&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/hash/df7e148cabfd9b608090fa5ee3348bfe-Abstract.html">Fast Abductive Learning by Similarity-based Consistency Optimization</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8539963460239876225&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. An approach for accelerating the convergence of Abductive Learning.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2019/file/c20a7ce2a627ba838cfbff082db35197-Paper.pdf">Learning by Abstraction: The Neural State Machine</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7361406080192630148&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370220301855">Making sense of sensory input</a> - <strong><em>Artificial Intelligence</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11875529139573472578&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2103.14230v1.pdf">Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution</a> - <strong><em>CVPR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4172146500538799638&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=SJlh8CEYDB">Learn to explain efﬁciently via neural logic inductive learning</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4550874980727321525&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/gblackout/NLIL">Project</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2006.06649">Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning</a> - <strong><em>ICML'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9257372000778020812&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2003.08978">Generating new concepts with hybrid neuro-symbolic models.</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1912020791698331044">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2006.14448">Learning Task-General Representations with Generative Neuro-Symbolic Modeling</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1335404082385789329">All Versions</a>].</p>
</li>
<li>
<p><a href="http://clgiles.ist.psu.edu/IST597/materials/slides/papers-memory/2016-graves.pdf">Hybrid computing using a neural network with dynamic external memory</a> - <strong><em>Nature</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=8100274942961380405&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/sciadv.aay2631">AI Feynman: A physics-inspired method for symbolic regression</a> - <strong><em>Science Advances</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3655502646441210453">All Versions</a>]. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, the authors develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. The authors apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, this work improves the state-of-the-art success rate from 15 to 90%.</p>
</li>
<li>
<p><a href="http://papers.NeurIPS.cc/paper/8546-classification-by-components-probabilistic-modeling-of-reasoning-over-a-set-of-components.pdf">Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12691103404451941071&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2006.11524.pdf">Neuro-Symbolic Visual Reasoning: Disentangling “Visual” from “Reasoning”</a> - <strong><em>ICML'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13160160974887139307&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/file/0d82627e10660af39ea7eb69c3568955-Paper.pdf">Understanding Deep Architectures with Reasoning Layer</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=937882599430270789&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1905.10307.pdf">An Explicitly Relational Neural Network Architecture</a> - <strong><em>ICML'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=37732747764322837&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2103.01937.pdf">Neural Production Systems</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15299280949648915581&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Yoshua Bengio's perspective on slot attention model as a general production system.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2008.06662.pdf">Compositional Generalization via Neural-Symbolic Stack Machines</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15612498612943317331&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=H1eSS3CcKX">Stochastic Optimization of Sorting Networks via Continuous Relaxations</a> - <strong><em>ICLR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=10619362619006891050&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=BkxUvnEYDH">Program Guided Agent</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://openreview.net/forum?id=BkxUvnEYDH">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/hash/7a685d9edd95508471a9d3d6fcace432-Abstract.html">Learning Compositional Rules via Neural Program Synthesis</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3160670555314650508&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2006.11287">Discovering Symbolic Models from Deep Learning with Inductive Biases</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9452091824686227240&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1904.11694.pdf">Neural Logic Machines</a> - <strong><em>ICLR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4525183211642569463&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1904.12584.pdf">The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a> - <strong><em>ICLR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8837128214653317831&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://papers.NeurIPS.cc/paper/2019/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf">Visual Concept-Metaconcept Learning</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1888051343232298875&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2103.16564">Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16735976343684307244&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://jiajunwu.com/papers/toqnet_ijcai.pdf">Temporal and Object Quantification Networks</a> - <strong><em>IJCAI'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17251222943638414124&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2009.01719.pdf">Grounded Language Learning Fast and Slow</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17735027444431750346&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/deepmind/dm_fast_mapping?s=05">Project</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s10994-022-06142-7">Detect, Understand, Act: A Neuro-symbolic Hierarchical Reinforcement Learning Framework</a> - <strong><em>Machine Learning</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=10321228117236432485&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A neuro-symbolic framework that integrates meta-policy learning in inductive logic programming.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9338352">Semi-Supervised Abductive Learning and Its Application to Theft Judicial Sentencing</a> - <strong><em>ICDM'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16646246740380524224">All Versions</a>]. [<a href="https://www.lamda.nju.edu.cn/huangyx/src/ICDM20-SSABL.pdf">Preprint</a>]. In many practical tasks, there are usually two kinds of common information: cheap unlabeled data and domain knowledge in the form of symbols. There are some attempts using one single information source, such as semi-supervised learning and abductive learning. However, there is little work to use these two kinds of information sources at the same time, because it is very difficult to combine symbolic logical representation and numerical model optimization effectively. The learning becomes even more challenging when the domain knowledge is insufficient. This paper presents an attempt-Semi-Supervised ABductive Learning (SS-ABL) framework. In this framework, semi-supervised learning is trained via pseudo labels of unlabeled data generated by abductive learning, and the background knowledge is refined via the label distribution predicted by semi-supervised learning. The above framework can be optimized iteratively and can be naturally interpretable. The effectiveness of the framework has been fully verified in the theft judicial sentencing of real legal documents. In the case of missing sentencing elements and mixed legal rules, the framework is apparently superior to many existing baseline practices, and provides explanatory assistance to judicial sentencing.</p>
</li>
</ul>
<h3 id="explainability">Explainability<a class="headerlink" href="#explainability" title="Permanent link">&para;</a></h3>
<h4 id="trustworthy-ai">Trustworthy AI<a class="headerlink" href="#trustworthy-ai" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.pnas.org/doi/full/10.1073/pnas.2111547119">Bayesian modeling of human–AI complementarity</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=15735143859968841009&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Bayesian framework for combining the predictions and different types of confidence scores from humans and machines.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/10.1126/scirobotics.aay4663">A tale of two explanations: Enhancing human trust by explaining robot behavior</a> - <strong><em>Science Robotics</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3985046411399524590">All Versions</a>]. [<a href="https://yzhu.io/publication/openbottle2019scirob/paper.pdf">Preprint</a>]. The ability to provide comprehensive explanations of chosen actions is a hallmark of intelligence. Lack of this ability impedes the general acceptance of AI and robot systems in critical tasks. This paper examines what forms of explanations best foster human trust in machines and proposes a framework in which explanations are generated from both functional and mechanistic perspectives. The robot system learns from human demonstrations to open medicine bottles using (i) an embodied haptic prediction model to extract knowledge from sensory feedback, (ii) a stochastic grammar model induced to capture the compositional structure of a multistep task, and (iii) an improved Earley parsing algorithm to jointly leverage both the haptic and grammar models. The robot system not only shows the ability to learn from human demonstrators but also succeeds in opening new, unseen bottles. Using different forms of explanations generated by the robot system, we conducted a psychological experiment to examine what forms of explanations best foster human trust in the robot. The authors found that comprehensive and real-time visualizations of the robot’s internal decisions were more effective in promoting human trust than explanations based on summary text descriptions. In addition, forms of explanation that are best suited to foster trust do not necessarily correspond to the model components contributing to the best task performance. This divergence shows a need for the robotics community to integrate model components to enhance both task execution and human trust in machines.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1909.06907">X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust</a> - <strong><em>CVPR XAI Workshop'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7751326666821697923">All Versions</a>]. This work presents a new explainable AI (XAI) framework aimed at increasing justified human trust and reliance in the AI machine through explanations. The authors pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, the machine generates sequence of explanations in a dialog which takes into account three important aspects at each dialog turn: (a) human's intention (or curiosity); (b) human's understanding of the machine; and &copy; machine's understanding of the human user. To do this, the authors use Theory of Mind (ToM) which helps us in explicitly modeling human's intention, machine's mind as inferred by the human as well as human's mind as inferred by the machine. In other words, these explicit mental representations in ToM are incorporated to learn an optimal explanation policy that takes into account human's perception and beliefs. Furthermore, the authors also show that ToM facilitates in quantitatively measuring justified human trust in the machine by comparing all the three mental representations.
We applied our framework to three visual recognition tasks, namely, image classification, action recognition, and human body pose estimation. The authors argue that our ToM based explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex machine learning models. This is the first work to derive explanations using ToM. Extensive human study experiments verify our hypotheses, showing that the proposed explanations significantly outperform the state-of-the-art XAI methods in terms of all the standard quantitative and qualitative XAI evaluation metrics including human trust, reliance, and explanation satisfaction. </p>
</li>
<li>
<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/5643">CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines</a> - <strong><em>AAAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17443137068166403183&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2589004221015510">CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models</a> - <strong><em>iScience</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17526041764295337444">All Versions</a>]. This work proposes CX-ToM, short for counterfactual explanations with theory-of-mind, a new explainable AI (XAI) framework for explaining decisions made by a deep convolutional neural network (CNN). In contrast to the current methods in XAI that generate explanations as a single shot response, the authors pose explanation as an iterative communication process, i.e., dialogue between the machine and human user. More concretely, this CX-ToM framework generates a sequence of explanations in a dialogue by mediating the differences between the minds of the machine and human user. To do this, the authors use Theory of Mind (ToM) which helps us in explicitly modeling the human’s intention, the machine’s mind as inferred by the human, as well as human's mind as inferred by the machine. Moreover, most state-of-the-art XAI frameworks provide attention (or heat map) based explanations. In this work, the authors show that these attention-based explanations are not sufficient for increasing human trust in the underlying CNN model. In CX-ToM, the authors instead use counterfactual explanations called fault-lines which are defined as follows: given an input image I for which a CNN classification model M predicts class cpred, a fault-line identifies the minimal semantic-level features (e.g., stripes on zebra), referred to as explainable concepts, that need to be added to or deleted from I to alter the classification category of I by M to another specified class calt. Extensive experiments verify the hypotheses, demonstrating that CX-ToM significantly outperforms the state-of-the-art XAI models.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-023-00692-8">Explaining machine learning models with interactive natural language conversations using TalkToModel</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=7044008493489695982">All Versions</a>]. Practitioners increasingly use machine learning (ML) models, yet models have become more complex and harder to understand. To understand complex models, researchers have proposed techniques to explain model predictions. However, practitioners struggle to use explainability methods because they do not know which explanation to choose and how to interpret the explanation. This work addresses the challenge of using explainability methods by proposing TalkToModel: an interactive dialogue system that explains ML models through natural language conversations. TalkToModel consists of three components: an adaptive dialogue engine that interprets natural language and generates meaningful responses; an execution component that constructs the explanations used in the conversation; and a conversational interface. In real-world evaluations, 73% of healthcare workers agreed they would use TalkToModel over existing systems for understanding a disease prediction model, and 85% of ML professionals agreed TalkToModel was easier to use, demonstrating that TalkToModel is highly effective for model explainability.</p>
</li>
</ul>
<h4 id="strong-machine-learning">Strong Machine Learning<a class="headerlink" href="#strong-machine-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://link.springer.com/article/10.1007/s10994-018-5707-3">Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP</a> - <strong><em>Machine Learning</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17551060457946144913">All Versions</a>]. During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell’s, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper the authors provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. The authors present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment the authors directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie’s sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. The authors believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007%2Fs10994-020-05941-0">Beneficial and harmful explanatory machine learning</a> - <strong><em>Machine Learning</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16983722694047294963">All Versions</a>]. Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. The quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.</p>
</li>
<li>
<p><a href="https://www.ijcai.org/Proceedings/2017/497">Deep Forest: Towards An Alternative to Deep Neural Networks</a> - <strong><em>IJCAI'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7391596872731517007">All Versions</a>]. [<a href="https://github.com/LAMDA-NJU/Deep-Forest">Project</a>]. This paper proposes gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train; even when it is applied to different data across different domains in the experiments, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient, and users can control training cost according to computational resource available. The efficiency may be further enhanced because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. </p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=mCLVeEpplNE">NBDT: Neural-Backed Decision Trees</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1902399007162005819">All Versions</a>]. [<a href="https://github.com/alvinwan/neural-backed-decision-trees">Code</a>]. Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. This work forgoes this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, the surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging.</p>
</li>
</ul>
<h4 id="explainable-deep-learning">Explainable Deep Learning<a class="headerlink" href="#explainable-deep-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://github.com/jacobgil/pytorch-grad-cam">pytorch-grad-cam</a> - 2021. Class Activation Map methods implemented in Pytorch, with many elegant features.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/8099837">Network dissection: Quantifying interpretability of deep visual representations</a> - <strong><em>CVPR'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=18069685615852396783&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="http://netdissect.csail.mit.edu/">Project</a>]. [<a href="http://places2.csail.mit.edu/index.html">Dataset: Places365</a>]. The original paper on visualizing the class activation maps to explain convolutional neural networks.</p>
</li>
<li>
<p><a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a> - <strong><em>Distill</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9053581372570691569&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A perspective on treating neural networks as circuits.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/hash/c74956ffb38ba48ed6ce977af6727275-Abstract.html">Compositional Explanations of Neurons</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15725346730266402738&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/jayelm/compexp">Project</a>]. A concept-composition version of network dissection.</p>
</li>
<li>
<p><a href="http://papers.NeurIPS.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition.pdf">This Looks Like That: Deep Learning for Interpretable Image Recognition</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9461838581952136719&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2006.09994.pdf">Noise or Signal: The Role of Backgrounds in Image Classification</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14729938011425134088&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/MadryLab/backgrounds_challenge">Code &amp; Data</a>]. [<a href="https://gradientscience.org/background/">Project</a>]. A perspective on image background provides strong clue for foreground classification.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2018/hash/5fc34ed307aac159a30d81181c99847e-Abstract.html">Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation</a> - <strong><em>NeurIPS'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=401428033641216502&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Maching the learned pattern of neurons in different neural networks.</p>
</li>
</ul>
<h3 id="embodied-intelligence">Embodied Intelligence<a class="headerlink" href="#embodied-intelligence" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/embodied-cognition/">Embodied Cognition</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Embodied Cognition, which emphasizes the significance of an agent's physical body in cognitive abilities.</p>
</li>
<li>
<p><a href="https://www.researchgate.net/profile/David-Woods-19/publication/242545872_Cognitive_Engineering_Human_Problem_Solving_with_Tools/links/542becf70cf29bbc126ac097/Cognitive-Engineering-Human-Problem-Solving-with-Tools.pdf">Cognitive engineering: Human problem solving with tools</a> - <strong><em>Human Factors</em></strong>, 1988. [<a href="https://scholar.google.com/scholar?cluster=14194840995416222723&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original idea of investigating huamn tool use in problem solving.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1993-97340-000">Tools, language and cognition in human evolution</a> - <strong><em>Cambridge University Press</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=6046350461147957220&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A classic perspective correlating human tool use with the evolution of civilization.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661303003231">The neural bases of complex tool use in humans</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=3612212926196611828&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A neuroscience account of human tool use.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0960982207017708">Spontaneous Metatool Use by New Caledonian Crows</a> - <strong><em>Current Biology</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=9263531730425342443&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A piece of evidence that intelligent animals can take advantage of matatools to make tools for problem solving.</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/abs/10.1177/0956797610371962">Rapid Assimilation of External Objects Into the Body Schema</a> - <strong><em>Psychological Science</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=854636910326733489&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.eva.mpg.de/documents/Cambridge/Tennie_Cultural_BehBrainSci_2012_1566208.pdf">The cognitive bases of human tool use</a> - <strong><em>Behavioral and Brain Sciences</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=4648150119820414671&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00214/full">The embodied mind extended: using words as social tools</a> - <strong><em>Frontiers in Psychology</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=14719988081062606352&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0408">Tool use as adaptation</a> - <strong><em>Philosophical Transactions of the Royal Society B: Biological Sciences</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=8060841461200774807&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0028393214000232">Intensive tool-practice and skillfulness facilitate the extension of body representations in humans</a> - <strong><em>Neuropsychologia</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=10578024091098127929&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Frev0000027">Tool use and affordance: Manipulation-based versus reasoning-based approaches</a> - <strong><em>Psychological Review</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3284942486402374505&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A classic review on human tool use and affordance.</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/5gf0m7x3">Meta-strategy learning in physical problem-solving: the effect of embodied experience</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=9713842177532954702">All Versions</a>].</p>
</li>
<li>
<p><a href="https://yzhu.io/publication/tool2015cvpr/paper.pdf">Understanding Tools: Task-Oriented Object Modeling, Learning and Recognition</a> - <strong><em>CVPR'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=4609926671953500969&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://yzhu.io/publication/tool2015cvpr/">Project</a>]. The original paper introducing affordance and physically-grounded tool use into computer vision.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2106.05654.pdf">Visual scoping operations for physical assembly</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7238090583833839&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.cc.gatech.edu/ai/robot-lab/online-publications/StoytchevICRA2005.pdf">Behavior-grounded representation of tool affordances</a> - <strong><em>ICRA'05</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=6115815663915603675&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-642-38812-5_1">A Relational Approach to Tool-Use Learning in Robots</a> - <strong><em>ILP'12</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=18374178227592386332&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s10514-017-9637-x">Relational affordances for multiple-object manipulation</a> - <strong><em>Autonomous Robots</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6357646940615855682&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://m.roboticsproceedings.org/rss15/p01.pdf">Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</a> - <strong><em>RSS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4316276917607326251&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/5gf0m7x3">Meta-strategy learning in physical problem-solving: the effect of embodied experience</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9713842177532954702">All Versions</a>]. This paper focuses on how natural embodied experience affects what kinds of abstract physical problem-solving strategies people use in a virtual task. The findings suggest that differences in embodied experience drive the acquisition of different meta-strategies for balancing acting with thinking, deciding what kinds of actions to try, and deciding how persistent to be with a current action plan.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/scirobotics.aar4043">Humanoid robotics—History, current state of the art, and challenges</a> - <strong><em>Science Robotics</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=10429657872677563981">All Versions</a>]. Humanoids represent one of the ultimate goals of robotics: to synthesize advances from many disciplines.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2002.06289">3D dynamic scene graphs: Actionable spatial perception with places, objects, and humans</a> - <strong><em>RSS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4428742298455436054">All Versions</a>]. This paper presents a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). The second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. The authors integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. This is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, the authors provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. The third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where the authors assess its robustness and expressiveness. Finally, the authors discuss the implications of their proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-025-01005-x">Embodied large language models enable robots to complete complex tasks in unpredictable environments</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=4507358987058849097">All Versions</a>]. Completing complex tasks in unpredictable settings challenges robotic systems, requiring a step change in machine intelligence. Sensorimotor abilities are considered integral to human intelligence. Thus, biologically inspired machine intelligence might usefully combine artificial intelligence with robotic sensorimotor capabilities. This work reports an embodied large-language-model-enabled robot (ELLMER) framework, utilizing GPT-4 and a retrieval-augmented generation infrastructure, to enable robots to complete long-horizon tasks in unpredictable settings. The method extracts contextually relevant examples from a knowledge base, producing action plans that incorporate force and visual feedback and enabling adaptation to changing conditions. The authors tested ELLMER on a robot tasked with coffee making and plate decoration; these tasks consist of a sequence of sub-tasks from drawer opening to pouring, each benefiting from distinct feedback types and methods. The authors show that the ELLMER framework allows the robot to complete the tasks. This demonstration marks progress towards scalable, efficient and ‘intelligent robots’ able to complete complex tasks in uncertain environments.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10056320">The Design, Education and Evolution of a Robotic Baby</a> - <strong><em>IEEE Transactions on Robotics</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=16298301812039402477">All Versions</a>]. Inspired by Alan Turing's idea of a child machine, this article introduces the formal definition of a robotic baby, an integrated system with minimal world knowledge at birth, capable of learning incrementally and interactively, and adapting to the world. Within the definition, fundamental capabilities and system characteristics of the robotic baby are identified and presented as the system-level requirements. As a minimal viable prototype, the Baby architecture is proposed with a systems engineering design approach to satisfy the system-level requirements, which has been verified and validated with simulations and experiments on a robotic system. The authors demonstrate the capabilities of the robotic baby in natural language acquisition and semantic parsing in English and Chinese, as well as in natural language grounding, natural language reinforcement learning, natural language programming, and system introspection for explainability. The education and evolution of the robotic baby are illustrated with real-world robotic demonstrations. Inspired by the genetic inheritance in human beings, knowledge inheritance in robotic babies and its benefits regarding evolution are discussed.</p>
</li>
</ul>
<h3 id="evolutionary-intelligence">Evolutionary Intelligence<a class="headerlink" href="#evolutionary-intelligence" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="http://websites.umich.edu/~zhanglab/clubPaper/06_08_2012.pdf">Evolutionary trade-offs, Pareto optimality, and the geometry of phenotype space</a> - <strong><em>Science</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=16162252507845975080&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A classic paper correlating biological trade-offs with the evolution of pareto optimality.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/BF01442131">Pareto optimality in multiobjective problems</a> - <strong><em>Applied Mathematics and Optimization</em></strong>, 1977. [<a href="https://scholar.google.com/scholar?cluster=11305142600366783354&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on the pareto optimality in multiobjective problems.</p>
</li>
<li>
<p><a href="http://www.soft-computing.de/SMC0805.pdf">Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies</a> - <strong><em>IEEE Transactions on Systems, Man, and Cybernetics</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=11308312498510305429&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on the application of pareto optimality to multiobjective machine learning.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-019-1153-z">Phylogenetic evidence for Sino-Tibetan origin in northern China in the Late Neolithic</a> - <strong><em>Nature</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13913123623752818925&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Bayesian phylogenetic analysis on two competing hypotheses of the origin of the Sino-Tibetan language family suggests that the initial expansion of Sino-Tibetan languages occurred approximately 4,000–6,000 years before present (BP; taken as AD 1950) in the Yellow River basin of northern China, and that this expansion is associated with the development of the Yangshao and/or Majiayao Neolithic cultures. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-021-04108-8">Triangulation supports agricultural spread of the Transeurasian languages</a> - <strong><em>Nature</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1183005894965630508&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://www.nature.com/articles/d41586-021-03037-w">Nature News</a>]. A triangulation of linguistic, archaeological and genetic data suggests that the Transeurasian language family originated in a population of grain farmers in China around 9,000 years ago, and that agriculture underpinned its spread.</p>
</li>
</ul>
<h3 id="methodologies-for-experiments">Methodologies for Experiments<a class="headerlink" href="#methodologies-for-experiments" title="Permanent link">&para;</a></h3>
<h4 id="quantitative-analysis">Quantitative Analysis<a class="headerlink" href="#quantitative-analysis" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="http://www.jakebowers.org/ITVExperiments/angristimbensrubin96.pdf">Identification of Causal Effects Using Instrumental Variables</a> - <strong><em>Journal of the American Statistical Association</em></strong>, 1996. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17166265099721941605">All Versions</a>]. The original paper on Instrumental Variables for natural sociology studies.</p>
</li>
<li>
<p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-psych-122414-033702">Experiments with More Than One Random Factor: Designs, Analytic Models, and Statistical Power</a> - <strong><em>Annual Review of Psychology</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6652444619934494760&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review of the quantitative analysis techniques for behavioral studies.</p>
</li>
<li>
<p><a href="https://mpra.ub.uni-muenchen.de/4823/1/MPRA_paper_4823.pdf">With or Without U? The Appropriate Test for a U-Shaped Relationship</a> - <strong><em>Oxford Bulletin of Economics and Statistics</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=1574723532506536904&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original method for testing U-shape relation from the data, which is distinctive from the quadratic regression test.</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/pdf/10.1177/2515245918805755">Two lines: A valid alternative to the invalid testing of U-shaped relationships with quadratic regressions</a> - <strong><em>Advances in Methods and Practices in Psychological Science</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=12010185803500406162&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. An alternative method to test the statistical significance of U-shaped relationships.</p>
</li>
</ul>
<h4 id="scaling-up-behavioral-studies">Scaling Up Behavioral Studies<a class="headerlink" href="#scaling-up-behavioral-studies" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://osf.io/wksv8">Scaling up experimental social, behavioral, and economic science</a> - <strong><em>Open Science Foundation Preprints</em></strong>. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Scaling+up+experimental+social%2C+behavioral%2C+and+economic+science&amp;btnG=">All Versions</a>]. A white paper on scaling up social, behavioral, and econimic experiments.</p>
</li>
<li>
<p><a href="https://scholar.harvard.edu/files/henrich/files/henrich_heine_norenzayan_2010-2.pdf">The weirdest people in the world?</a> - <strong><em>Brain and Behavioral Sciences</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=3129419557801277936&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on rethinking and tackling the sample bias in behaivoral studies, where most subjects are drawn from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies.</p>
</li>
<li>
<p><a href="https://www.pnas.org/doi/10.1073/pnas.1915841117">Scaling up psychology via Scientific Regret Minimization</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8011895688226766944&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The statistical and ecological basis for scaling up behavioral studies.</p>
</li>
<li>
<p><a href="https://cpb-us-w2.wpmucdn.com/web.sas.upenn.edu/dist/a/511/files/2021/06/Bhatia-He-Science.pdf">Machine-generated theories of human decision-making</a> - <strong><em>Science</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7065547001880027350&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/jpeterson/papers/peterson2021-science.pdf">Using large-scale experiments and machine learning to discover theories of human decision-making</a> - <strong><em>Science</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7456250222852859810&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A piece of evidence for the merits brought by large-scale behavioral studies in social science.</p>
</li>
<li>
<p><a href="http://jakehofman.com/pdfs/integrating-prediction-and-explanation.pdf">Integrating explanation and prediction in computational social science</a> - <strong><em>Nature</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=288245575125750925&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/josh/papers/griffiths-largeimagedatabases-topics2016.pdf">Exploring human cognition using large image databases</a> - <strong><em>Topics in Cognitive Sciences</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3629906005701226294&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://web.archive.org/web/20170809024454id_/http://www.kevinjing.com/visual_search_at_pinterest.pdf">Visual Search at Pinterest</a> - <strong><em>KDD'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=2051024301293529405&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Large scale user study in the development of the recommendations system by Pinterest.</p>
</li>
</ul>
<h4 id="decision-making">Decision Making<a class="headerlink" href="#decision-making" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="https://link.springer.com/article/10.3758/s13428-022-01789-5">A computational process-tracing method for measuring people’s planning strategies and how they change over time</a> - <strong><em>Behavior Research Methods</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=10405935000926098041">All Versions</a>]. Model-based strategy identification.</li>
</ul>
<h4 id="question-answering">Question Answering<a class="headerlink" href="#question-answering" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://cogsci.mindmodeling.org/2016/papers/0122/paper0122.pdf">Searching large hypothesis spaces by asking questions</a> - <strong><em>CogSci'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3398849603439166012&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A behavioral study for the 20 questions game.</p>
</li>
<li>
<p><a href="https://gureckislab.org/papers/RotheLakeGureckis-2016cogsci.pdf">Asking and evaluating natural language questions</a> - <strong><em>CogSci'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=34641833161282231&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A behavioral study for the battleship game.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s42113-018-0005-5">Do People Ask Good Questions?</a> - <strong><em>Computational Brain &amp; Behavior</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=14595996621617337270&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://nyuccl.org/papers/Rothe-Lake-Gureckis-2019-Cogsci.pdf">Asking goal-oriented questions and learning from answers</a> - <strong><em>CogSci'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=14185546187726917682&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="human-machine-comparison">Human-Machine Comparison<a class="headerlink" href="#human-machine-comparison" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://psycnet.apa.org/record/1973-00249-001">Elimination by aspects: A theory of choice</a> - <strong><em>Psychological Review</em></strong>, 1972. [<a href="https://scholar.google.com/scholar?cluster=1633792484482810297&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Herbert Simon's early experiments on computer aided behavioral studies.</p>
</li>
<li>
<p><a href="https://stacks.stanford.edu/file/druid:qv796fc9687/qv796fc9687.pdf">Problem Solving and Rule Induction: A Unified View</a> - <strong><em>Knowledge and cognition</em></strong>, 1974. [<a href="https://scholar.google.com/scholar?cluster=12943734683291006234&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1901.04587">Human few-shot learning of compositional instructions</a> - <strong><em>CogSci'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12841163907815018136&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2103.05823.pdf">Fast and flexible: Human program induction in abstract reasoning tasks</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5294483826040237516&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v80/dubey18a.html">Investigating Human Priors for Playing Video Games</a> - <strong><em>ICML'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2202192690517876762&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2352154619300622">Tasks for aligning human and machine planning</a> - <strong><em>Current Opinion in Behavioral Sciences</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8308872468787875598&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://perception.jhu.edu/files/PDFs/19_Adversarial_Deciphering/ZhouFirestone-AdversarialDeciphering.pdf">Humans can decipher adversarial images</a> - <strong><em>Nature Communications</em></strong>. 2019. [<a href="https://scholar.google.com/scholar?cluster=4423950118844131054&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41593-022-01026-4.pdf">Shared computational principles for language processing in humans and deep language models</a> - <strong><em>Nature Neuroscience</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=16078004657063602593&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="association-test">Association Test<a class="headerlink" href="#association-test" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Implicit-association_test">Implicit Association Test</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on the Implicit Association Test, a controversial assessment intended to detect subconscious associations between mental representations of objects (concepts) in memory.</p>
</li>
<li>
<p><a href="http://faculty.fortlewis.edu/burke_b/Senior/BLINK%20replication/IAT.pdf">Measuring Individual Differences in Implicit Cognition: The Implicit Association Test</a> - <strong><em>Journal of Personality and Social Psychology</em></strong>, 1998. [<a href="https://scholar.google.com/scholar?cluster=302378224541015580&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper introducing the Implicit Association Test.</p>
</li>
<li>
<p><a href="http://faculty.washington.edu/agg/pdf/Gwald_Nosek_ZEITSCHR_2001.OCR.pdf">Health of the Implicit Association Test at age 3</a> - <strong><em>Zeitschrift für Experimentelle Psychologie</em></strong>, 2001. [<a href="https://scholar.google.com/scholar?cluster=10868478693422595588&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The 3<sup>rd</sup> year review for the IAT.</p>
</li>
<li>
<p><a href="https://faculty.washington.edu/agg/pdf/Nosek%20&amp;%20al.IATatage7.2007.pdf">The Implicit Association Test at Age 7: A Methodological and Conceptual Review</a> - <strong><em>Social psychology and the unconscious: The automaticity of higher mental processes (pp. 265–292), Psychology Press</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=16189750920013376566&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The 7<sup>th</sup> year review for the IAT.</p>
</li>
<li>
<p><a href="http://faculty.washington.edu/agg/IATmaterials/PDFs/Hofmann%20&amp;%20al%20(PSPB,2005).pdf">A Meta-Analysis on the Correlation Between the Implicit Association Test and Explicit Self-Report Measures</a> - <strong><em>Personality and Social Psychology Bulletin</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=4888328728717829047&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="virtual-reality">Virtual Reality<a class="headerlink" href="#virtual-reality" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.nature.com/articles/nn948">Virtual reality in behavioral neuroscience and beyond</a> - <strong><em>Nature Neuroscience</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=12168354203281280346&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A classic review on the early applications of Virtual Reality to behavioral studies.</p>
</li>
<li>
<p><a href="https://stanfordvr.com/mm/2009/fox-jmp-vr-survival.pdf">Virtual reality: A survival guide for the social scientist</a> - <strong><em>Journal of Media Psychology</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=17318470193315023264&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2022-60836-006">The psychology of virtual reality</a> - <strong><em>The psychology of technology: Social science research in the age of Big Data (pp. 155–193), American Psychological Association</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=11535480055596209683&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2021">All Versions</a>]. Jeremy Bailenson's review on the applications of Virtual Reality to behavioral studies.</p>
</li>
<li>
<p><a href="https://stanfordvr.com/mm/2015/cummings-mp-how-immersive.pdf">How Immersive Is Enough? A Meta-Analysis of the Effect of Immersive Technology on User Presence</a> - <strong><em>Media Psychology</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=9218122072360464558&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A meta-analysis on the extent to which technologies need to be immersive in order to generate a sense of presence.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/10108427">Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving</a> - <strong><em>VR'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11228377215337222005&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3139131.3139152">Agent: automatic generation of experimental protocol runtime</a> - <strong><em>VRST'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=3511549412244980073">All Versions</a>]. This paper proposes the use of Domain-Specific Languages (DSLs) to ease the description and generation of VR experiments, thus letting experiment designers focus on their core tasks: designing, conducting, and reporting experiments.</p>
</li>
</ul>
<h3 id="meta-level-considerations">Meta-Level Considerations<a class="headerlink" href="#meta-level-considerations" title="Permanent link">&para;</a></h3>
<h4 id="meta-learning">Meta Learning<a class="headerlink" href="#meta-learning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/2201.03916.pdf">Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</a> - 2022. [<a href="https://scholar.google.com/scholar?cluster=9025378857688824887&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on AutoRL.</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v70/finn17a/finn17a.pdf">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> - <strong><em>ICML'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17278604844873996878">All Versions</a>]. [<a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">Post</a>]. Chelsea Finn's original paper on Model-Agnostic Meta-Learning (MAML). </p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2018/hash/e1021d43911ca2c1845910d84f40aeae-Abstract.html">Bayesian Model-Agnostic Meta-Learning</a> - <strong><em>NeurIPS'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7370333111335795917&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Bayesian account on MAML.</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=SJeD3CEFPH">Meta-Q-Learning</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2865388954464396222&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The milestone paper on context Meta-RL.</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v97/rakelly19a.html">Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</a> - <strong><em>ICML'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15379570585451726919&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=TQt98Ya7UMP">Balancing Constraints and Rewards with Meta-Gradient D4PG</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=2805226315118298313&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=Bk8BvDqex">Metacontrol for Adaptive Imagination-Based Optimization</a> - <strong><em>ICLR'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=16728474512617398730&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/hash/1e4d36177d71bbb3558e43af9577d70e-Abstract.html">On Effective Scheduling of Model-based Reinforcement Learning</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11128521607771619105&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="marrs-levels-of-analysis">Marr's Levels of Analysis<a class="headerlink" href="#marrs-levels-of-analysis" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://usa1lib.org/book/1223444/8e5ca8">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</a> - <strong><em>MIT Press</em></strong>, 1982. [<a href="https://scholar.google.com/scholar?cluster=14386368570811483142&amp;hl=en&amp;as_sdt=0,44">All Versions</a>]. David Marr's original book on the levels of analysis.</p>
</li>
<li>
<p><a href="https://dspace.mit.edu/bitstream/handle/1721.1/5782/AIM-357.pdf?sequence=2">From understanding computation to understanding neural circuitry</a> - <strong><em>Neuroscience Research Program Bulletin</em></strong>, 1979. [<a href="https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=0,5&amp;cluster=11150567121969913334">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/tom/papers/LabPublications/BridgingLevelsAnalysis.pdf">Bridging Levels of Analysis for Probabilistic Models of Cognition</a> - <strong><em>Current Directions in Psychological Science</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=5063382112136991296&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Marr's paradigm account on probabilistic models.</p>
</li>
<li>
<p><a href="https://people.csail.mit.edu/pkrafft/papers/krafft-griffiths-levels-css.pdf">Levels of Analysis in Computational Social Science</a> - <strong><em>CogSci'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10178929388985626844&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Marr's paradigm account on computational social science.</p>
</li>
<li>
<p><a href="https://baicsworkshop.github.io/pdf/BAICS_6.pdf">Levels of Analysis for Machine Learning</a> - <strong><em>ICLR'20 Bridging AI and Cognitive Science Workshop</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13819038971626384115&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Marr's paradigm account on machine learning.</p>
</li>
</ul>
<h4 id="gestalt">Gestalt<a class="headerlink" href="#gestalt" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://psycnet.apa.org/record/2007-10344-001">Gestalt theory</a> - <strong><em>A source book of Gestalt psychology</em></strong>, 1938. [<a href="https://scholar.google.com/scholar?cluster=18133275659218646817&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original book on Gestalt psychology.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/BF00422382">Gestalt Psychology</a> - <strong><em>Psychologische Forschung</em></strong>, 1967. [<a href="https://scholar.google.com/scholar?cluster=16023098380090751616&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Wolfgang Köhler's review on Gestalt psychology.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9450.1984.tb01001.x">Restructuring revisited I. Summary and critique of the Gestalt theory of problem solving</a> - <strong><em>Scandinavian Journal of Psychology</em></strong>, 1984. [<a href="https://scholar.google.com/scholar?cluster=1540079499182933565&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9450.1984.tb01005.x">Restructuring revisited II. An information processing theory of restructuring and insight</a> - <strong><em>Scandinavian Journal of Psychology</em></strong>, 1984. [<a href="https://scholar.google.com/scholar?cluster=1821980539002417470&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1993-36184-001">Thoughts beyond words: When language overshadows insight</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=13773440938721955384&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/1244721/20ddc5">Deep Learning: How the Mind Overrides Experience</a> - <strong><em>Cambridge University Press</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=231021877034210140">All Versions</a>]. </p>
</li>
</ul>
<h4 id="the-aha-moment">The Aha! Moment<a class="headerlink" href="#the-aha-moment" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Eureka_effect">Eureka Effect</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on Eureka effect (a.k.a. Aha! moment, insight, and epiphany), the common human experience of suddenly understanding a previously incomprehensible problem or concept.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Insight">Insight</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on insight.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Epiphany_(feeling)">Epiphany</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on epiphany, the "feeling" when the Aha! moment comes.</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/54x8v354">A computational model of scientific insight</a> - <strong><em>The nature of creativity: Contemporary psychological perspectives</em></strong>, 1988. [<a href="https://scholar.google.com/scholar?cluster=13633357571064621019&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A computational account on insights for scientific discovery.</p>
</li>
<li>
<p><a href="https://www.researchgate.net/profile/Thomas-Ormerod/publication/8909475_What_Makes_an_Insight_Problem_The_Roles_of_Heuristics_Goal_Conception_and_Solution_Recoding_in_Knowledge-Lean_Problems/links/00b7d5159f3c057eb5000000/What-Makes-an-Insight-Problem-The-Roles-of-Heuristics-Goal-Conception-and-Solution-Recoding-in-Knowledge-Lean-Problems.pdf">What Makes an Insight Problem? The Roles of Heuristics, Goal Conception, and Solution Recoding in Knowledge-Lean Problems</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17529631069707671285&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2003-10949-002">APA</a>]. </p>
</li>
<li>
<p><a href="https://www.hf.uni-koeln.de/data/fgpsych/File/Haider/Knoblich_etal_1999.pdf">Constraint relaxation and chunk decomposition in insight problem solving</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=8057214169831054227&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://psycnet.apa.org/record/1999-01477-011">APA</a>].</p>
</li>
<li>
<p><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=818fec7c896ea3716eeb637da095293e9e6d1806">Dynamics and constraints in insight problem solving</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=12067671710370549516&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2002-01361-014">APA</a>].</p>
</li>
<li>
<p><a href="https://bpb-us-e1.wpmucdn.com/sites.northwestern.edu/dist/a/699/files/2015/11/Salvi_etal_Insight-is-right_TR2016-2n3ns9l.pdf">Insight solutions are correct more often than analytic solutions</a> - <strong><em>Thinking &amp; Reasoning</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=883561570778414219&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01424/full">Insight Is Not in the Problem: Investigating Insight in Problem Solving across Task Types</a> - <strong><em>Frontiers in Psychology</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=4564128114316001308&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.researchgate.net/profile/Trina-Kershaw/publication/8909474_Multiple_Causes_of_Difficulty_in_Insight_The_Case_of_the_Nine-Dot_Problem/links/55dca27e08aeb38e8a8d23b6/Multiple-Causes-of-Difficulty-in-Insight-The-Case-of-the-Nine-Dot-Problem.pdf">Multiple Causes of Difficulty in Insight: The Case of the Nine-Dot Problem</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=15600199808825346018&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2003-10949-001">APA</a>].</p>
</li>
<li>
<p><a href="https://www.researchgate.net/profile/Gary-Jones-14/publication/23152585_Investigating_the_Effect_of_Mental_Set_on_Insight_Problem_Solving/links/0fcfd50abb767b1102000000/Investigating-the-Effect-of-Mental-Set-on-Insight-Problem-Solving.pdf">Investigating the effect of Mental Set on Insight Problem Solving</a> - <strong><em>Experimental Psychology</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=11054712671934144981&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="rationality">Rationality<a class="headerlink" href="#rationality" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/bounded-rationality/">Bounded Rationality</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Bounded Rationality, an elementary hypothesis of human intelligence in psychology and ecology.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/rationality-instrumental/">Instrumental Rationality</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Instrumental Rationality, a dabate on whether an agent's decision is made intentionally or out of rational coherence.</p>
</li>
<li>
<p><a href="https://www.taylorfrancis.com/books/mono/10.4324/9781315083223/study-thinking-jerome-bruner-jacqueline-goodnow-george-austin">A Study of Thinking</a> - <strong><em>Routledge</em></strong>, 1956. [<a href="https://scholar.google.com/scholar?cluster=17466297915128086930">All Versions</a>]. This book is a pioneering account of how human beings achieve a measure of rationality in spite of the constraints imposed by time and ignorance. </p>
</li>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(03)00028-7?large_figure=true&amp;mobileUi=0">Task switching</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2003. [<a href="https://scholar.google.com/scholar?cluster=676255515965300942&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="http://psychfiles.net/experimental/Monsell_2003.pdf">Preprint</a>]. The original paper on ``switch cost'', where subjects' responses are substantially slower and, usually, more error-prone immediately after a task switch.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12086">Computational Rationality: Linking Mechanism and Behavior Through Bounded Utility Maximization</a> - <strong><em>Topics in Cognitive Science</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=15813211310327194798&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Introducing the computational rationality framework for including information-processing bounds in rational analyses, which emphasizes the incorporation of computational mechanism into the definition of rational action. </p>
</li>
<li>
<p><a href="https://gershmanlab.com/pubs/GershmanHorvitzTenenbaum15.pdf">Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</a> - <strong><em>Science</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=7744057022238735461&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on the rationality of Bayesian computational models.</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/papers/lieder_resource.pdf">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</a> - <strong><em>Behavioral and Brain Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1642626865293965288&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A resource-rational account on interpreting human intelligence.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12142">Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic</a> - <strong><em>Topics in Cognitive Science</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=16305499937147933368&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. An earlier version of the paper above.</p>
</li>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30215-1">Understanding Human Intelligence through Human Limitations</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=6469796133334580403">All Versions</a>]. [<a href="https://cocosci.princeton.edu/papers/griffiths_understanding.pdf">Preprint</a>]. Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. The author argues that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. The author claims that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.</p>
</li>
<li>
<p><a href="https://eccl.mit.edu/s/Pelz_Foundations-of-intuitive-power-analyses-in-children-and-adults.pdf">Foundations of intuitive power analyses in children and adults</a> - <strong><em>Nature Human Behavior</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=4370839893505978405&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Evidences support that people have some of the foundations for 'intuitive power analyses', which help people use intuitive statistical reasoning and metacognitive strategies to estimate how much information they might need to solve different discrimination problems.</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/papers/ho2022cognitive.pdf">Cognitive Science as a Source of Forward and Inverse Models of Human Decisions for Robotics and Control</a> - <strong><em>Annual Review of Control, Robotics, and Autonomous Systems</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=14055765901243029337">All Versions</a>]. The review focuses on how cognitive science can provide forward models of human decision-making and inverse models of how humans think about others’ decision-making. The authors highlight relevant recent developments, including approaches that synthesize black box and theory-driven modeling, accounts that recast heuristics and biases as forms of bounded optimality, and models that characterize human theory of mind and communication in decision-theoretic terms.</p>
</li>
</ul>
<h4 id="cognitive-architecture">Cognitive Architecture<a class="headerlink" href="#cognitive-architecture" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/epistemology/">Epistemology</a> - <strong><em>Plato Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001285">The secret life of predictive brains: what's spontaneous activity for?</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=719229834892860829&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A neuroscience account on brain as a generative model.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370287900506">SOAR: An architecture for general intelligence</a> - <strong><em>Artificial Intelligence</em></strong>, 1987. [<a href="https://scholar.google.com/scholar?cluster=10873259207109132615&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370205001530">Metacognition in computation: A selected research review</a> - <strong><em>Artificial Intelligence</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=4240334051245008914&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010027718301604">Basic functional trade-offs in cognition: An integrative framework</a> - <strong><em>Cognition</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11475742130443069967&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://doi.org/10.1126/SCIENCE.AAN8871">What is consciousness, and could machines have it?</a> - <strong><em>Science</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6932714857132107942&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A perspective on the two levels of consciousness in machine intelligence.</p>
</li>
<li>
<p><a href="https://www.worldscientific.com/doi/abs/10.1142/S2705078521500028">A Theoretical Computer Science Perspective on Consciousness</a> - <strong><em>Journal of Artificial Intelligence and Consciousness</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16430561748075101972&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h3 id="science-logology">Science Logology<a class="headerlink" href="#science-logology" title="Permanent link">&para;</a></h3>
<h4 id="philosophy-of-science">Philosophy of Science<a class="headerlink" href="#philosophy-of-science" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www-inst.eecs.berkeley.edu/~cs298-7/fa20/readings/kuhn.pdf">The structure of scientific revolutions</a> - <strong><em>University of Chicago Press: Chicago</em></strong>, 1970. [<a href="https://scholar.google.com/scholar?cluster=8909475038284903063&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Thomas Kuhn's original book on the emergence and the shift of scientific paradigms.</p>
</li>
<li>
<p><a href="https://jamacoartney.net/Abend%20(2008).pdf">The Meaning of "Theory"</a> - <strong><em>Sociological Theory</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=4876642889050563131&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A philosophical account on the definition of "theory" in social science (also can be generalized to natural science).</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/pdf/10.4256/mio.2013.015">The blind men and the elephant: A metaphor to illuminate the role of researchers and reviewers in social science</a> - <strong><em>Methodological Innovations Online</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=1654629562068006152&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3576896">A Computational Inflection for Scientific Discovery</a> - <strong><em>Communications of the ACM</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=1756108647531090189&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="science-of-science">Science of Science<a class="headerlink" href="#science-of-science" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Metascience">Metascience</a> - <strong><em>Wikipedia</em></strong>.</p>
</li>
<li>
<p><a href="http://ctbergstrom.com/publications/pdfs/2018Science.pdf">Science of Science</a> - <strong><em>Science</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6471468823556848055&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive large-scale review on the science of science.</p>
</li>
<li>
<p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.0307752101">Finding Scientific Topics</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17382767110929995134&amp;hl=zh-CN&amp;as_sdt=0,5">All Versions</a>]. Thomas L. Griffiths's analysis of scientific topics using Bayesian model.</p>
</li>
<li>
<p><a href="https://www.pnas.org/doi/10.1073/pnas.1618569114">Meta-assessment of Bias in Science</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14575889060982308028&amp;hl=zh-CN&amp;as_sdt=0,5">All Verisions</a>]. An analysis of bias patterns and risk factors in science.</p>
</li>
<li>
<p><a href="https://www.pnas.org/doi/10.1073/pnas.2021636118">Slowed Canonical Progress in Large Fields of Science</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7541922918797308487&amp;hl=zh-CN&amp;as_sdt=0,5">All Verisions</a>]. An analysis of why too many papers published each year in a field can lead to stagnation rather than advance. </p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/2858036.2858283">HCI Research as Problem-Solving</a> - <strong><em>ACM SIGCHI'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3206201064123443333&amp;as_sdt=0,5">All Versions</a>]. This essay contributes a meta-scientific account of human-computer interaction (HCI) research as problem-solving. We build on the philosophy of Larry Laudan, who develops problem and solution as the foundational concepts of science. We argue that most HCI research is about three main types of problem: empirical, conceptual, and constructive.</p>
</li>
</ul>
<h4 id="literature-mining">Literature Mining<a class="headerlink" href="#literature-mining" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.nature.com/articles/s41467-024-45563-x">Structured information extraction from scientific text with large language models</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13694008040033857249">All Versions</a>]. This paper presents a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models can be fine-tuned to extract useful records of complex scientific knowledge. The authors test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-020-17266-6">Automated extraction of chemical synthesis actions from experimental procedures</a> - <strong><em>Nature Communications</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1626689948540815082">All Versions</a>]. This paper presents a method to convert unstructured experimental procedures written in English to structured synthetic steps (action sequences) reflecting all the operations needed to successfully conduct the corresponding chemical reactions. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-021-22951-1">Inferring experimental procedures from text-based representations of chemical reactions</a> - <strong><em>Nature Communications</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15772647675166217556">All Versions</a>]. This paper presents data-driven models for predicting the entire sequence of synthesis steps starting from a textual representation of a chemical equation, for application in batch organic chemistry.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-023-43836-5">Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis</a> - <strong><em>Nature Communications</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8186755371438552520">All Versions</a>]. This paper introduces a transformer model for automated synthesis protocol analysis in catalyst discovery, exemplified using single-atom heterogeneous catalysts (SACs), a rapidly expanding catalyst family. The model adeptly converts SAC protocols into action sequences, and this output is used to facilitate statistical inference of their synthesis trends and applications, potentially expediting literature review and analysis.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41598-025-02643-2">An intelligent guided troubleshooting method for aircraft based on HybirdRAG</a> - <strong><em>Scientific Reports</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=4924119792997395046">All Versions</a>]. To enhance aircraft fault diagnosis efficiency, this paper proposes HybridRAG, an intelligent-guided troubleshooting framework that integrates knowledge graphs and large language models (LLMs). Unlike conventional retrieval-augmented generation (RAG) methods that rely on single-modal retrieval, HybridRAG adopts a multi-dimensional retrieval strategy, combining graph-based reasoning with both vector-based and BM25-based text retrieval techniques. This hybrid approach ensures comprehensive extraction of relevant information from both unstructured text and structured fault graphs, enhancing diagnostic precision, relevance, and robustness. Experimental results demonstrate that HybridRAG achieves an F1 score improvement of at least 4% and reduces hallucination rates by over 7% compared to mainstream RAG baselines. These advancements, combined with its unique integration of multi-modal retrieval, position HybridRAG as a novel framework for addressing complex aircraft maintenance challenges. Additionally, the paper presents an agent-based intelligent troubleshooting assistant that supports more interactive, adaptive, and flexible diagnostic Q&amp;A, providing maintenance personnel with a significant advanced intelligent, context-aware diagnostic tool.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41598-025-00724-w">Dual retrieving and ranking medical large language model with retrieval augmented generation</a> - <strong><em>Scientific Reports</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=9572966919397577345">All Versions</a>]. Recent advancements in large language models (LLMs) have significantly enhanced text generation across various sectors; however, their medical application faces critical challenges regarding both accuracy and real-time responsiveness. To address these dual challenges, this work proposes a novel two-step retrieval and ranking retrieval-augmented generation (RAG) framework that synergistically combines embedding search with Elasticsearch technology. Built upon a dynamically updated medical knowledge base incorporating expert-reviewed documents from leading healthcare institutions, the hybrid architecture employs ColBERTv2 for context-aware result ranking while maintaining computational efficiency. Experimental results show a 10% improvement in accuracy for complex medical queries compared to standalone LLM and single-search RAG variants, while acknowledging that latency challenges remain in emergency situations requiring sub-second responses in an experimental setting, which can be achieved in real-time using more powerful hardware in real-world deployments. This work establishes a new paradigm for reliable medical AI assistants that successfully balances accuracy and practical deployment considerations.</p>
</li>
<li>
<p><a href="https://galactica.org/static/paper.pdf">Galactica: A Large Language Model for Science</a> - <strong><em>Meta AI</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=15782429788006956926&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A large language model trained on large-scale scientific corpus.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2205.03512">CORWA: A Citation-Oriented Related Work Annotation Dataset</a> - <strong><em>NAACL'22</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=14605899782190710454&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://aclanthology.org/2021.acl-demo.14/">ESRA: Explainable Scientific Research Assistant</a> - <strong><em>ACL'21 Demo Track</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4387915912582172679&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A tool for constructing and visualizing the knowledge graph of a query keyword in literature retrieving.</p>
</li>
<li>
<p><a href="https://matthewberger.github.io/papers/cite2vec.pdf">cite2vec: Citation-Driven Document Exploration via Word Embeddings</a> - <strong><em>IEEE Transactions on Visualization and Computer Graphics</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=6949650208780085923&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/zeyuli2020.pdf">Galex: Exploring the evolution and intersection of disciplines</a> - <strong><em>IEEE Transactions on Visualization and Computer Graphics</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13313104491218225635&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h4 id="scientific-writing">Scientific Writing<a class="headerlink" href="#scientific-writing" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="http://library.lol/main/8036CBB1CCC448CA7E036774D810EBC0">The uses of argument</a> - <strong><em>Cambridge University Press</em></strong>, 1958. [<a href="https://scholar.google.com/scholar?cluster=12052408655432810103&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Stephen Toulmin's introduction to the Toulmin argument pattern, which is generally consist of a claim, a justification, and a rebuttal. </p>
</li>
<li>
<p><a href="https://www.jstor.org/stable/355200">A tagmemic approach to paragraph analysis</a> - <strong><em>College Composition and Communication</em></strong>, 1965. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+Tagmemic+Approach+to+Paragraph+Analysis+AL+Becker&amp;btnG=">All Versions</a>]. The original paper on analyzing the structure of expository paragraphs, with the two patterns---the Topic-Restriction-Illustration pattern and the Problem-Solution pattern.</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/abs/10.1177/0741088398015002004">The uses and complexity of argument structures in expert and student persuasive writing</a> - <strong><em>Written Communication</em></strong>, 1998. [<a href="https://scholar.google.com/scholar?cluster=3218190258774062869&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A behaviorial study revealing the argument structures exploited by people in argumentative writing.</p>
</li>
<li>
<p><a href="https://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/viewFile/3940/4244">Speech Acts of Argumentation: Inference Anchors and Peripheral Cues in Dialogue</a> - <strong><em>AAAI'12</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9761955212933152906&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper introducing the Information Anchoring Theory (IAT) as an alternate for AIF. </p>
</li>
</ul>
<h4 id="science-education">Science Education<a class="headerlink" href="#science-education" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="https://aclanthology.org/2023.acl-demo.2/">PersLEARN: Research Training through the Lens of Perspective Cultivation</a> - <strong><em>ACL'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6242389165210232890">All Versions</a>]. Scientific research is inherently shaped by its authors’ perspectives, influenced by various factors such as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. To address the problem, this paper introduces PersLEARN, a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework.</li>
</ul>
<h4 id="democratization-of-science">Democratization of Science<a class="headerlink" href="#democratization-of-science" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.1250475">Reproducibility</a> - <strong><em>Science</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=676974831306442279&amp;hl=en&amp;as_sdt=0,10">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41557-024-01470-8">Bridging the information gap in organic chemical reactions</a> - <strong><em>Nature Chemistry</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=5365091261196953334">All Versions</a>]. This perspective article formulates eight principles to improve data management in scientific publications relating to data standardization, reproducibility and evaluation, and encourage scientists to go beyond current publication standards. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-016-0021">A manifesto for reproducible science</a> - <strong><em>Nature Human Behavior</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=9515807942859203900&amp;hl=en&amp;as_sdt=0,10">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/533452a">1,500 scientists lift the lid on reproducibility</a> - <strong><em>Nature</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=11479406257389837824&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4204808/">How to Make More Published Research True</a> - <strong><em>PLoS Medicine</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=10945341175996677908">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/d42473-019-00004-y">Six factors affecting reproducibility in life science research and how to handle them</a> - <strong><em>Nature Advertisement</em></strong>.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/d41586-021-02428-3">Five keys to writing a reproducible lab protocol</a> - <strong><em>Nature</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=13259206850261301938">All Versions</a>]. This interviewing paper introduces five ways to increase the reproducibility of experimental protocols: (i) documenting protocols as the experiment goes; (ii) providing video illustrations in addition to written protocols; (iii) using electronic lab notebooks (ELNs) for managing experimental resources digitally; (iv) depositing and documenting reagents with understanding the rationale behind every step; and (v) exploiting online platforms to share tips, extensions, methods, and data among researchers.</p>
</li>
<li>
<p><a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003779">The Experimental Design Assistant</a> - <strong><em>PLoS Biology</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=12481490526120919925">All Versions</a>]. [<a href="https://www.nature.com/articles/nmeth.4462">Nature Methods Correspondence</a>]. [<a href="https://eda.nc3rs.org.uk/">EDA Website</a>]. The EDA is a web-based tool that guides the in vivo researcher through the experimental design and analysis process, providing automated feedback on the proposed design and generating a graphical summary that aids communication with colleagues, funders, regulatory authorities, and the wider scientific community.</p>
</li>
</ul>
<h4 id="laboratory-automation">Laboratory Automation<a class="headerlink" href="#laboratory-automation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.aat0650">Reconfigurable system for automated optimization of diverse chemical reactions</a> - <strong><em>Science</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3076614068291119943">All Versions</a>]. [<a href="https://www.science.org/doi/pdf/10.1126/science.aat0650">Preprint</a>]. This paper describes a plug-and-play, continuous-flow chemical synthesis system that mitigates this challenge with an integrated combination of hardware, software, and analytics. The system software controls the user-selected reagents and unit operations (reactors and separators), processes reaction analytics (high-performance liquid chromatography, mass spectrometry, vibrational spectroscopy), and conducts automated optimizations.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.abc2986">A universal system for digitization and automatic execution of the chemical synthesis literature</a> - <strong><em>Science</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13909991218383718512">All Versions</a>]. [<a href="https://www.chem.gla.ac.uk/cronin/images/pubs/Mehr-ScienceOct2020.pdf">Preprint</a>]. [<a href="https://croningroup.gitlab.io/chemputer/xdl/index.html">XDL Documentation</a>]. [<a href="https://zenodo.org/records/3955107">XDL Schema Database</a>]. This paper reports a software platform that uses natural language processing to translate the organic chemistry literature directly into editable code, which in turn can be compiled to drive automated synthesis of the compound in the laboratory.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/science.abo0058">Digitization and validation of a chemical synthesis literature database in the ChemPU</a> - <strong><em>Science</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17368503277308594977">All Versions</a>]. [<a href="https://www.researchgate.net/profile/Aamir-Khan/publication/361857872_Digitization_and_validation_of_a_chemical_synthesis_literature_database_in_the_ChemPU/links/62cd356d00d0b451104cbfe9/Digitization-and-validation-of-a-chemical-synthesis-literature-database-in-the-ChemPU.pdf">Preprint</a>]. This paper presents an automatically executable chemical reaction database of 100 molecules representative of the range of reactions found in contemporary organic synthesis. The chemical reaction codes or χDLs for the reactions have been stored in a database for version control, validation, collaboration, and data mining. Of these syntheses, more than 50 entries from the database have been downloaded and robotically run in seven modular chemputers with yields and purities comparable to those achieved by an expert chemist.</p>
</li>
<li>
<p><a href="https://pubs.acs.org/doi/full/10.1021/jacsau.1c00303">Chemputation and the Standardization of Chemical Informatics</a> - <strong><em>Journal of the American Chemical Society (Au)</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=3884902150148113559">All Versions</a>]. This paper describes a standard hardware (the chemical processing programming architecture --- the ChemPU) to encompass all chemical synthesis, an approach which unifies all chemistry automation strategies, from solid-phase peptide synthesis, to HTE flow chemistry platforms, while at the same time establishing a publication standard so that researchers can exchange chemical code (χDL) to ensure reproducibility and interoperability.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41557-022-01016-w">An autonomous portable platform for universal chemical synthesis</a> - <strong><em>Nature Chemistry</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=4484997534431409967">All Versions</a>]. [<a href="https://eprints.gla.ac.uk/275574/">Preprint</a>]. This paper presents a portable suitcase-sized chemical synthesis platform containing all the modules required for synthesis and purification. The system uses a chemical programming language coupled to a digital reactor generator to produce reactors and executable protocols based on text-based literature syntheses. Simultaneously, the platform generates a reaction pressure fingerprint, used to monitor processes within the reactors and remotely perform a protocol quality control.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-020-2442-2">A mobile robotic chemist</a> - <strong><em>Nature</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13216902493789027324&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://strathprints.strath.ac.uk/74759/1/Burger_etal_Nature_2020_A_mobile_robotic.pdf">Preprint</a>]. This work uses a mobile robot to search for improved photocatalysts for hydrogen production from water. The robot operated autonomously over eight days, performing 688 experiments within a ten-variable experimental space, driven by a batched Bayesian search algorithm. This autonomous search identified photocatalyst mixtures that were six times more active than the initial formulations, selecting beneficial components and deselecting negative ones.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06734-w">An autonomous laboratory for the accelerated synthesis of novel materials</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17944003281308189532">All Versions</a>]. This paper introduces the A-Lab, an autonomous laboratory for the solid-state synthesis of inorganic powders. This platform uses computations, historical data from the literature, machine learning (ML) and active learning to plan and interpret the outcomes of experiments performed using robotics. Over 17 days of continuous operation, the A-Lab realized 41 novel compounds from a set of 58 targets including a variety of oxides and phosphates that were identified using large-scale ab initio phase-stability data from the Materials Project and Google DeepMind.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/542125a">The Internet of Things comes to the lab</a> - <strong><em>Nature</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7747117198956166976&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The emergence of connected instruments and equipment promises to untether researchers from the laboratory --- letting them fine-tune experiments and analyse data remotely. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-023-44599-9">A dynamic knowledge graph approach to distributed self-driving laboratories</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=7070798385652764751">All Versions</a>]. This work employs ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. The architecture is built upon the World Avatar project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.</p>
</li>
<li>
<p><a href="https://www.cell.com/trends/chemistry/fulltext/S2589-5974(23)00249-6">Balancing act: when to flex and when to stay fixed</a> - <strong><em>Trends in Chemistry</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=14208571639305934551">All Versions</a>]. This perspective article provides essential insights into the decision-making process for choosing automation platforms, highlighting the suitability of fixed automation for standardized tasks and the strategic use of flexible automation in dynamic research settings.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2590238522006385">What is a minimal working example for a self-driving laboratory?</a> - <strong><em>Matter</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=1612804023616680548">All Versions</a>]. This paper proposes SDL-Demo: a low-cost “Hello, World!” for self-driving laboratories that combines “Hello, World!” tasks from electronics, physics-based simulations, and optimization. SDL-Demo is modular and extensible, making it an ideal candidate for low-cost teaching and prototyping of self-driving laboratory concepts.</p>
</li>
<li>
<p><a href="https://elifesciences.org/articles/77007">Robotic search for optimal cell culture in regenerative medicine</a> - <strong><em>eLife</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=1330075145723138159">All Versions</a>]. This paper develops a robotic AI system with a batch Bayesian optimization algorithm that autonomously induces the differentiation of induced pluripotent stem cell-derived retinal pigment epithelial (iPSC-RPE) cells. From 200 million possible parameter combinations, the system performed cell culture in 143 different conditions in 111 days, resulting in 88% better iPSC-RPE production than that obtained by the pre-optimized culture in terms of the pigmentation scores.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s43588-025-00769-x">Balancing autonomy and expertise in autonomous synthesis laboratories</a> - <strong><em>Nature Computational Science</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=2487456887760587329">All Versions</a>]. Autonomous synthesis laboratories promise to streamline the plan–make–measure–analyze iteration loop. Here, the authors comment on the barriers in the field, the promise of a human on-the-loop approach, and strategies for optimizing accessibility, accuracy, and efficiency of autonomous laboratories.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-023-37139-y">AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning</a> - <strong><em>Nature Communications</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=14092546584501434626">All Versions</a>]. Closed-loop, autonomous experimentation enables accelerated and material-efficient exploration of large reaction spaces without the need for user intervention. However, autonomous exploration of advanced materials with complex, multi-step processes and data sparse environments remains a challenge. In this work, the authors present AlphaFlow, a self-driven fluidic lab capable of autonomous discovery of complex multi-step chemistries. AlphaFlow uses reinforcement learning integrated with a modular microdroplet reactor capable of performing reaction steps with variable sequence, phase separation, washing, and continuous in-situ spectral monitoring. To demonstrate the power of reinforcement learning toward high dimensionality multi-step chemistries, the authors use AlphaFlow to discover and optimize synthetic routes for shell-growth of core-shell semiconductor nanoparticles, inspired by colloidal atomic layer deposition (cALD). Without prior knowledge of conventional cALD parameters, AlphaFlow successfully identified and optimized a novel multi-step reaction route, with up to 40 parameters, that outperformed conventional sequences. Through this work, the authors demonstrate the capabilities of closed-loop, reinforcement learning-guided systems in exploring and solving challenges in multi-step nanoparticle syntheses, while relying solely on in-house generated data from a miniaturized microfluidic platform. Further application of AlphaFlow in multi-step chemistries beyond cALD can lead to accelerated fundamental knowledge generation as well as synthetic route discoveries and optimization.</p>
</li>
</ul>
<h4 id="ai-assisted-research">AI Assisted Research<a class="headerlink" href="#ai-assisted-research" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06221-2">Scientific discovery in the age of artificial intelligence</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11962817646389491592">All Versions</a>]. A review article that examines breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency.</p>
</li>
<li>
<p><a href="https://pubs.acs.org/doi/full/10.1021/jacs.4c00338">Artificial Intelligence for Retrosynthetic Planning Needs Both Data and Expert Knowledge</a> - <strong><em>Journal of the American Chemical Society</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=10595951443492961310">All Versions</a>]. The development of AI synthesis planners trained solely on reaction-example-data has stagnated and is not on par with the performance of “hybrid” algorithms combining AI with expert knowledge. This Perspective examines possible causes of these shortcomings, extending beyond the established reasoning of insufficient quantities of reaction data. Drawing attention to the intricacies and data biases that are specific to the domain of synthetic chemistry, the authors advocate augmenting the unique capabilities of AI with the knowledge base and the reasoning strategies of domain experts. By actively involving synthetic chemists, who are the end users of any synthesis planning software, into the development process, the authors envision to bridge the gap between computer algorithms and the intricate nature of chemical synthesis.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2311.07361">The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4</a> - <strong><em>Microsoft Research AI4Science</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?%2C5&amp;q=The+Impact+of+Large+Language+Models+on+Scientific+Discovery%3A+a+Preliminary+Study+using+GPT-4&amp;btnG=">All Versions</a>]. [<a href="https://github.com/microsoft/LLM4ScientificDiscovery">Project</a>]. A survey on the performance of LLMs within the context of scientific discovery, focusing on GPT-4.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/full/10.1126/sciadv.aay4275">Machine learning-assisted molecular design and efficiency prediction for high-performance organic photovoltaic materials</a> - <strong><em>Science Advances</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12392230644945701722">All Versions</a>]. In the process of finding high-performance materials for organic photovoltaics (OPVs), it is meaningful if one can establish the relationship between chemical structures and photovoltaic properties even before synthesizing them. This work first establishes a database containing over 1700 donor materials reported in the literature. Through supervised learning, our machine learning (ML) models can build up the structure-property relationship and, thus, implement fast screening of OPV materials. The authors explore several expressions for molecule structures, i.e., images, ASCII strings, descriptors, and fingerprints, as inputs for various ML algorithms. It is found that fingerprints with length over 1000 bits can obtain high prediction accuracy. The reliability of the approach is further verified by screening 10 newly designed donor materials. Good consistency between model predictions and experimental outcomes is obtained. The result indicates that ML is a powerful tool to prescreen new OPV materials, thus accelerating the development of the OPV field. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41598-018-34533-1">Design of metalloproteins and novel protein folds using variational autoencoders</a> - <strong><em>Scientific Reports</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=18126187509308242959">All Versions</a>]. The design of novel proteins has many applications but remains an attritional process with success in isolated cases. Meanwhile, deep learning technologies have exploded in popularity in recent years and are increasingly applicable to biology due to the rise in available data. This work attempts to link protein design and deep learning by using variational autoencoders to generate protein sequences conditioned on desired properties. Potential copper and calcium binding sites are added to non-metal binding proteins without human intervention and compared to a hidden Markov model. In another use case, a grammar of protein structures is developed and used to produce sequences for a novel protein topology. One candidate structure is found to be stable by molecular dynamics simulation. The ability of the model to confine the vast search space of protein sequences and to scale easily has the potential to assist in a variety of protein design tasks.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a> - <strong><em>Nature</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=6286436358625670901">All Versions</a>]. This paper provides the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. This approach is a canonical application of observation- and explanation- based method for protein structure prediction instead of first-principle-based methods. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-05773-7">Human–machine collaboration for improving semiconductor process development</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10295771969614897767">All Versions</a>]. [<a href="https://www.nature.com/articles/d41586-023-01353-x">Nature News</a>]. This work studies Bayesian optimization algorithms to investigate how artificial intelligence (AI) might decrease the cost of developing complex semiconductor chip processes. In particular, this work create a controlled virtual process game to systematically benchmark the performance of humans and computers for the design of a semiconductor fabrication process. The authors find that human engineers excel in the early stages of development, whereas the algorithms are far more cost-efficient near the tight tolerances of the target. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06555-x">A foundation model for generalizable disease detection from retinal images</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=3139988207343394501">All Versions</a>]. This paper presents RETFound, a foundation model for retinal images that learns generalizable representations from unlabelled retinal images and provides a basis for label-efficient model adaptation in several applications. Specifically, RETFound is trained on 1.6 million unlabelled retinal images by means of self-supervised learning and then adapted to disease detection tasks with explicit labels.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06185-3">Accurate medium-range global weather forecasting with 3D neural networks</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=7198604620204619820">All Versions</a>]. This paer introduces an artificial-intelligence-based method for accurate, medium-range global weather forecasting. It shows that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, the program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts.</p>
</li>
<li>
<p><a href="https://www.science.org/doi/10.1126/science.adi2336">Learning skillful medium-range global weather forecasting</a> - <strong><em>Science</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=269756601245477923&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06184-4">Skilful nowcasting of extreme precipitation with NowcastNet</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17837864391812838009&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-023-06792-0">Autonomous chemical research with large language models</a> - <strong><em>Nature</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8097577445064259203">All Versions</a>]. An artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-024-00832-8">Augmenting large language models with chemistry tools</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=9291969834799338362">All Versions</a>]. [<a href="https://arxiv.org/abs/2304.05376">Preprint</a>]. This paper introduces ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery and materials design. By integrating 18 expert-designed tools and using GPT-4 as the LLM, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. The agent autonomously planned and executed the syntheses of an insect repellent and three organocatalysts and guided the discovery of a novel chromophore.</p>
</li>
<li>
<p><a href="https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5">Empowering biomedical discovery with AI agents</a> - <strong><em>Cell</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=16382920795308615613">All Versions</a>]. The authors envision “AI scientists” as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate AI models and biomedical tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI’s ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are poised to be proficient in various tasks, planning discovery workflows and performing self-assessment to identify and mitigate gaps in their knowledge. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from virtual cell simulation, programmable control of phenotypes, and the design of cellular circuits to developing new therapies.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-025-57430-4">DrBioRight 2.0: an LLM-powered bioinformatics chatbot for large-scale cancer functional proteomics analysis</a> - <strong><em>Nature Communications</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=4899547492027803190">All Versions</a>]. [<a href="https://drbioright.org">Project</a>]. Functional proteomics provides critical insights into cancer mechanisms, facilitating the discovery of novel biomarkers and therapeutic targets. The authors have developed a comprehensive cancer functional proteomics resource using reverse phase protein arrays, incorporating data from nearly 8000 patient samples from The Cancer Genome Atlas and approximately 900 samples from the Cancer Cell Line Encyclopedia. The dataset includes a curated panel of nearly 500 high-quality antibodies, covering all major cancer hallmark pathways. To enhance the accessibility and analytic power of this resource, this work introduces DrBioRight 2.0, an intuitive bioinformatic platform powered by state-of-the-art large language models. DrBioRight enables researchers to explore protein-centric cancer omics data, perform advanced analyses, visualize results, and engage in interactive discussions using natural language. By streamlining complex proteogenomic analyses, this tool accelerates the translation of large-scale functional proteomics data into meaningful biomedical insights.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-025-09442-9">The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies</a> - <strong><em>Nature</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=9384650162839457416">All Versions</a>]. Science frequently benefits from teams of interdisciplinary researchers, but many scientists do not have easy access to experts from multiple fields. Although large language models (LLMs) have shown an impressive ability to aid researchers across diverse domains, their uses have been largely limited to answering specific scientific questions rather than performing open-ended research. Here the authors expand the capabilities of LLMs for science by introducing the Virtual Lab, an artificial intelligence (AI)–human research collaboration to perform sophisticated, interdisciplinary science research. The Virtual Lab consists of an LLM Principal Investigator agent guiding a team of LLM scientist agents through a series of research meetings, with a human researcher providing high-level feedback. The authors applied the Virtual Lab to design nanobody binders to recent variants of SARS-CoV-2. The Virtual Lab created a novel computational nanobody design pipeline that incorporates the protein language model ESM, the protein folding model AlphaFold-Multimer and the computational biology software Rosetta and designed 92 new nanobodies. Experimental validation reveals a range of functional nanobodies with promising binding profiles across SARS-CoV-2 variants. In particular, two new nanobodies exhibit improved binding to the recent JN.1 or KP.3 variants while maintaining strong binding to the ancestral viral spike protein, suggesting that these are suitable candidates for further investigation. This work demonstrates how the Virtual Lab can rapidly make an impactful, real-world scientific discovery.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2023.emnlp-main.162/">BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</a> - <strong><em>EMNLP'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=1222312709622462659">All Versions</a>]. [<a href="https://github.com/bioplanner/bioplanner">Project</a>]. This paper presents an automatic evaluation framework for the task of planning experimental protocols, and introduces BioProt: a dataset of biology protocols with corresponding pseudocode representations.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s11432-024-4485-0">From intention to implementation: automating biomedical research via LLMs</a> - <strong><em>Science China Information Sciences</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=15681173257674232428">All Versions</a>]. Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly large language models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols, on average, outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers’ workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-024-47997-9">A human-machine interface for automatic exploration of chemical reaction networks</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13306522324804014261">All Versions</a>]. Autonomous reaction network exploration algorithms offer a systematic approach to explore mechanisms of complex chemical processes. However, the resulting reaction networks are so vast that an exploration of all potentially accessible intermediates is computationally too demanding. This paper introduces a STEERING WHEEL to guide an otherwise unbiased automated exploration. The STEERING WHEEL algorithm is intuitive, generally applicable, and enables one to focus on specific regions of an emerging network. It also allows for guiding automated data generation in the context of mechanism exploration, catalyst design, and other chemical optimization challenges.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-025-63303-7">Active learning accelerates electrolyte solvent screening for anode-free lithium metal batteries</a> - <strong><em>Nature Communications</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=3974490115470252870">All Versions</a>]. Anode-free or ‘zero-excess’ lithium metal batteries offer high energy density compared to current lithium-ion batteries but require electrolyte innovation to extend cycle life. Due to the lack of universal design principles, electrolyte development for anode-free lithium metal batteries is slow and incremental and mainly driven by trial-and-error. This work demonstrates the use of active learning as an alternative approach to accelerate electrolyte discovery for anode-free lithium metal batteries. Unlike conventional data-intensive frequentist machine learning techniques, the active learning framework employs sequential Bayesian experimental design with Bayesian model averaging to efficiently identify optimal candidates in typical data-scarce and noisy label settings. Using capacity retention in real Cu||LiFePO4 cells as the target property, the approach integrates experimental feedback to iteratively refine predictions. Starting with just 58 data points from an in-house cycling dataset, the active learning framework explored a virtual search space of 1 million electrolytes, rapidly converging on optimal candidates. After seven active learning campaigns with about ten electrolytes tested in each, four distinct electrolyte solvents are identified that rival state-of-the-art electrolytes in performance. This work showcases the promise of active learning approaches in navigating large electrolyte chemical spaces for next-generation batteries.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-024-50779-y">PatCID: an open-access dataset of chemical structures in patent documents</a> - <strong><em>Nature Communications</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=329287456191953845">All Versions</a>]. The automatic analysis of patent publications has potential to accelerate research across various domains, including drug discovery and material science. Within patent documents, crucial information often resides in visual depictions of molecule structures. PatCID (Patent-extracted Chemical-structure Images database for Discovery) allows to access such information at scale. It enables users to search which molecules are displayed in which documents. PatCID contains 81M chemical-structure images and 14M unique chemical structures. This work compares PatCID with state-of-the-art chemical patent-databases. On a random set, PatCID retrieves 56.0% of molecules, which is higher than automatically-created databases, Google Patents (41.5%) and SureChEMBL (23.5%), as well as manually-created databases, Reaxys (53.5%) and SciFinder (49.5%). Leveraging state-of-the-art methods of document understanding, PatCID high-quality data outperforms currently available automatically-generated patent-databases. PatCID even competes with proprietary manually-created patent-databases. This enables promising applications for automatic literature review and learning-based molecular generation methods.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-025-00994-z">Large language models for scientific discovery in molecular property prediction</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2025. [<a href="https://scholar.google.com/scholar?cluster=12679596096313341977">All Versions</a>]. Large language models (LLMs) are a form of artificial intelligence system encapsulating vast knowledge in the form of natural language. These systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization and computer code generation. Although LLMs have seen initial applications in natural sciences, their potential for driving scientific discovery remains largely unexplored. This work introduces LLM4SD, a framework designed to harness LLMs for driving scientific discovery in molecular property prediction by synthesizing knowledge from literature and inferring knowledge from scientific data. LLMs synthesize knowledge by extracting established information from scientific literature, such as molecular weight being key to predicting solubility. For inference, LLMs identify patterns in molecular data, particularly in Simplified Molecular Input Line Entry System-encoded structures, such as halogen-containing molecules being more likely to cross the blood–brain barrier. This information is presented as interpretable knowledge, enabling the transformation of molecules into feature vectors. By using these features with interpretable models such as random forest, LLM4SD can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. The authors foresee it providing interpretable and potentially new insights, aiding scientific discovery in molecular property prediction.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-023-38851-5">Retrosynthesis prediction using an end-to-end graph generative architecture for molecular graph editing</a> - <strong><em>Nature Communications</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8204732491203332966">All Versions</a>]. Retrosynthesis planning, the process of identifying a set of available reactions to synthesize the target molecules, remains a major challenge in organic synthesis. Recently, computer-aided synthesis planning has gained renewed interest and various retrosynthesis prediction algorithms based on deep learning have been proposed. However, most existing methods are limited to the applicability and interpretability of model predictions, and further improvement of predictive accuracy to a more practical level is still required. In this work, inspired by the arrow-pushing formalism in chemical reaction mechanisms, the authors present an end-to-end architecture for retrosynthesis prediction called Graph2Edits. Specifically, Graph2Edits is based on graph neural network to predict the edits of the product graph in an auto-regressive manner, and sequentially generates transformation intermediates and final reactants according to the predicted edits sequence. This strategy combines the two-stage processes of semi-template-based methods into one-pot learning, improving the applicability in some complicated reactions, and also making its predictions more interpretable. Evaluated on the standard benchmark dataset USPTO-50k, the model achieves the state-of-the-art performance for semi-template-based retrosynthesis with a promising 55.1% top-1 accuracy.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2311.00176">ChipNeMo: Domain-Adapted LLMs for Chip Design</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=5962372610489019326">All Versions</a>]. ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, the authors instead adopt the following domain adaptation techniques: domain-adaptive tokenization, domain-adaptive continued pretraining, model alignment with domain-specific instructions, and domain-adapted retrieval models. The authors evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Evaluations demonstrate that domain-adaptive pretraining of language models, can lead to superior performance in domain related downstream tasks compared to their base LLaMA2 counterparts, without degradations in generic capabilities. In particular, the largest model, ChipNeMo-70B, outperforms the highly capable GPT-4 on two of the use cases, namely engineering assistant chatbot and EDA scripts generation, while exhibiting competitive performance on bug summarization and analysis. These results underscore the potential of domain-specific customization for enhancing the effectiveness of large language models in specialized applications. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-021-22048-9">Single-atom alloy catalysts designed by first-principles calculations and artificial intelligence</a> - <strong><em>Nature Communications</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=6593978922251447907">All Versions</a>]. This paper addresses the problem of new Single-atom-alloy catalysts (SAACs) discovery by applying a compressed-sensing data-analytics approach parameterized with density-functional inputs.</p>
</li>
<li>
<p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2016239118">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15181490380139888639&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s00449-016-1659-9">Comparability of automated human induced pluripotent stem cell culture: a pilot study</a> - <strong><em>Bioprocess and Biosystems Engineering</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=14666375402220991095&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41591-019-0539-7">An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis</a> - <strong><em>Nature Medicine</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3280260879383275625">All Versions</a>]. The microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of image digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. This work proposes a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/10059206">Optimizing Spaced Repetition Schedule by Capturing the Dynamics of Memory</a> - <strong><em>IEEE Transactions on Knowledge and Data Engineering</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=949715967083833369&amp;hl=en&amp;as_sdt=0,10">All Versions</a>].</p>
</li>
<li>
<p><a href="https://aclanthology.org/2020.findings-emnlp.261/">LEGAL-BERT: The Muppets straight out of Law School</a> - <strong><em>EMNLP'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11254432523766039890&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Generating answers to legal questions, analyze contracts, and summarizing legal documents, making legal knowledge more accessible to non-experts.</p>
</li>
<li>
<p><a href="https://academic.oup.com/bioinformatics/article/36/4/1234/5566506">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</a> - <strong><em>Bioinformatics</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2783127196632783403&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Answering medical questions, identifying relevant clinical trials, and diagnosing diseases based on symptoms, making medical information more accessible to the general public.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.5555/3491440.3492062">Finbert: A pre-trained financial language representation model for financial text mining</a> - <strong><em>IJCAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17844713837232165872&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Predicting stock market trends, analyzing financial documents, and generating summaries of economic news articles, helping to disseminate financial knowledge.</p>
</li>
<li>
<p><a href="https://aclanthology.org/D19-1371/">SciBERT: A Pretrained Language Model for Scientific Text</a> - <strong><em>EMNLP'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7377999893003631695&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Searching and synthesizing scientific literature, aiding researchers in hypothesis generation, and assisting with experimental design, making scientific knowledge more accessible.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2020.findings-emnlp.139/">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a> - <strong><em>EMNLP'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9055786889913621082&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Completing code, generating programming documentation, and providing technical support, making programming knowledge more accessible to non-experts.</p>
</li>
</ul>
<h3 id="theory-of-mind">Theory of Mind<a class="headerlink" href="#theory-of-mind" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Theory_of_mind">Theory of Mind</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on Theory of Mind (ToM), a cognitive capability that estimating others' goal, belief, and desire.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/intentionality/">Intentionality</a> - <strong><em>Plato Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/mental-imagery/">Mental Imagery</a> - <strong><em>Plato Stanford</em></strong>.</p>
</li>
</ul>
<!--* [Cognitive Science](https://plato.stanford.edu/entries/cognitive-science/) - ***Plato Stanford***.

* [The Mind/Brain Identity Theory](https://plato.stanford.edu/entries/mind-identity/) - ***Plato Stanford***.

* [Mental Representation](https://plato.stanford.edu/entries/mental-representation/) - ***Plato Stanford***.

* [Temporal Consciousness](https://plato.stanford.edu/entries/consciousness-temporal/) - ***Plato Stanford***.

* [The Experience and Perception of Time](https://plato.stanford.edu/entries/time-experience/) - ***Plato Stanford***.

* [Practical Reason](https://plato.stanford.edu/entries/practical-reason/) - ***Plato Stanford***.

* [Memory](https://plato.stanford.edu/entries/memory/) - ***Plato Stanford***.-->

<!-- * [The Computational Theory of Mind](https://plato.stanford.edu/entries/computational-mind/) - ***Plato Stanford***. A computational philosophy account on ToM. -->

<ul>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(16)30053-5">The naïve utility calculus: Computational principles underlying commonsense psychology</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=6894095575934067763">All Versions</a>]. [<a href="http://sll.stanford.edu/docs/2016_JaraEttinger_Gweon_Schulz_Tenenbaum_TiCS.pdf">Preprint</a>]. This review article proposes that human social cognition is structured around a basic understanding of ourselves and others as intuitive utility maximizers: from a young age, humans implicitly assume that agents choose goals and actions to maximize the rewards they expect to obtain relative to the costs they expect to incur. This ‘naïve utility calculus’ allows both children and adults observe the behavior of others and infer their beliefs and desires, their longer-term knowledge and preferences, and even their character: who is knowledgeable or competent, who is praiseworthy or blameworthy, who is friendly, indifferent, or an enemy. </p>
</li>
<li>
<p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(22)00185-1">Planning with theory of mind</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8461125353366208047">All Versions</a>]. [<a href="https://saxelab.mit.edu/sites/default/files/publications/HoSaxeCushman2022.pdf">Preprint</a>]. A perspective on understanding Theory of Mind through planning that consists of abstract structured causal representations and supports efficient search and selection from innumerable possible actions. Planning requires that Theory of Mind consists of abstract structured causal representations and supports efficient search and selection from innumerable possible actions. Theory of Mind contrasts with less cognitively demanding alternatives: statistical predictive models of other people’s actions, or model-free reinforcement of actions by their effects on other people. Theory of Mind is likely used to plan novel interventions and predict their effects, for example, in pedagogy, emotion regulation, and impression management.</p>
</li>
<li>
<p><a href="https://psyarxiv.com/f692k/">The Signature of All Things: Children Infer Knowledge States from Static Images</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=12380982112592086477&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2017">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661316301565?via%3Dihub">Bayesian Brains without Probabilities</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=13076510377612067772&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A perspective on human probabilistic modeling without explicit probabilistic computation.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-017-0064">Rational quantitative attribution of beliefs, desires and percepts in human mentalizing</a> - <strong><em>Nature Human Behavior</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=9377509910551057835">All Versions</a>]. [<a href="https://cbmm.mit.edu/sites/default/files/publications/article.pdf">Preprint</a>]. This paper presents a model of core mentalizing computations: inferring jointly an actor’s beliefs, desires and percepts from how they move in the local spatial environment. The proposed Bayesian theory of mind (BToM) model is based on probabilistically inverting artificial-intelligence approaches to rational planning and state estimation, which extend classical expected-utility agent models to sequential actions in complex, partially observable domains.</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v80/rabinowitz18a.html">Machine theory of mind</a> - <strong><em>ICML'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6267278380616425333">All Versions</a>]. Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. This work proposes a Theory of Mind neural network --- a ToMnet --- which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2352154618302055?via%3Dihub">Theory of mind as inverse reinforcement learning</a> - <strong><em>Current Opinion in Behavioral Sciences</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=14959443239271810913">All Versions</a>]. This paper reviews the idea that Theory of Mind --- humans' ability to reason about other people's mental states --- can be formalized as inverse reinforcement learning. Under this framework, expectations about how mental states produce behavior are captured in a reinforcement learning (RL) model. Predicting other people’s actions is achieved by simulating a RL model with the hypothesized beliefs and desires, while mental-state inference is achieved by inverting this model. Although many advances in inverse reinforcement learning (IRL) did not have human Theory of Mind in mind, this paper focuses on what they reveal when conceptualized as cognitive theories.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12371">Computational Models of Emotion Inference in Theory of Mind: A Review and Roadmap</a> - <strong><em>Topics in Cognitive Science</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15919410726494658168">All Versions</a>]. This paper proposes an intuitive theory framework to studying affective cognition—how humans reason about emotions—and derive a taxonomy of inferences within affective cognition. Using this taxonomy, the authors review formal computational modeling work on such inferences, including causal reasoning about how others react to events, reasoning about unseen causes of emotions, reasoning with multiple cues, as well as reasoning from emotions to other mental states. This framework proposes unifying these various types of reasoning as Bayesian inference within a common “intuitive Theory of Emotion.”</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010028520300633">The Naïve Utility Calculus as a unified, quantitative framework for action understanding</a> - <strong><em>Cognitive Psychology</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10366690800692546587">All Versions</a>]. [<a href="http://www.github.com/julianje/bishop">Project</a>]. This paper presents a formal theory of the Naïve Utility Calculus as a probabilistic generative model, which highlights the role of cost and reward tradeoffs in a Bayesian framework for action-understanding. The model predicts with quantitative accuracy how people infer agents’ subjective costs and rewards based on their observable actions. By distinguishing between desires, goals, and intentions, the model extends to complex action scenarios unfolding over space and time in scenes with multiple objects and multiple action episodes.</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v139/shu21a.html">AGENT: A Benchmark for Core Psychological Reasoning</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9729067071974484204">All Versions</a>]. Inspired by cognitive development studies on intuitive psychology, this paper presents a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology. The results suggest that to pass the designed tests of core intuitive psychology at human levels, a model must acquire or have built-in representations of how agents plan, combining utility computations and core knowledge of objects and physics. </p>
</li>
<li>
<p><a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-psych-081420-110718">Experimental Games and Social Decision Making</a> - <strong><em>Annual Review of Psychology</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4713510112126264116">All Versions</a>]. Experimental games model situations in which the future outcomes of individuals and groups depend on their own choices and on those of other (groups of) individuals. Games are a powerful tool to identify the neural and psychological mechanisms underlying interpersonal and group cooperation and coordination. This review article discusses recent developments in how experimental games are used and adapted, with an increased focus on repeated interactions, partner control through sanctioning, and partner (de)selection for future interactions.</p>
</li>
<li>
<p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4574">Theory of Minds: Understanding Behavior in Groups through Inverse Planning</a> - <strong><em>AAAI'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6755247312077985817">All Versions</a>]. Towards the goal of building machine-learning algorithms with human-like social intelligence, this paper develops a generative model of multiagent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. This work uses CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. </p>
</li>
<li>
<p><a href="https://psycnet.apa.org/fulltext/2019-58384-001.pdf?auth_token=0859666184839448b848053cd7bdceb2bdf2745a">Leveraging Facial Expressions and Contextual Information to Investigate Opaque Representations of Emotion</a> - <strong><em>Emotion</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9634378462684744548&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://linkinghub.elsevier.com/retrieve/pii/S0010027712002235">Waiting and weighting: Information sampling is a balance between efficiency and error-reduction</a> - <strong><em>Cognition</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=12787722822882067638&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0896627313005503?via%3Dihub">Natural scene statistics account for the representation of scene categories in human visual cortex</a> - <strong><em>Neuron</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=14030885492052338412&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41598-018-23618-6">Using human brain activity to guide machine learning</a> - <strong><em>Scientific Report</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=12987955253653036948&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2019-27729-001">Unit of visual working memory: A Boolean map provides a better account than an object does</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14909735035752892020&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dspace.mit.edu/bitstream/handle/1721.1/112291/ivc_full_preprint.pdf?sequence=1&amp;isAllowed=y">Ten-month-old infants infer the value of goals from the costs of actions</a> - <strong><em>Science</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=11862940312128630925&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A piece of evidence for children's capability on ToM.</p>
</li>
<li>
<p><a href="https://static1.squarespace.com/static/595a9f155016e1f7ead6edf1/t/61eeb3e7bbc41a23cd288f8a/1643033708945/Gandhi_etal_2021.pdf">Baby Intuitions Benchmark (BIB): Discerning the goals, preferences, and actions of others</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16514364601966350574">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2011.05558.pdf">Intentonomy: a Dataset and Study towards Human Intent Understanding</a> - <strong><em>CVPR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5268870345003195142&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A large-scale database on human intentionally-posted images on social media.</p>
</li>
<li>
<p><a href="https://www.tshu.io/HeiderSimmel/CogSci20/Flatland_CogSci20.pdf">Adventures in Flatland: Perceiving Social Interactions Under Physical Dynamics</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1928005249823745390&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16167">PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception</a> - <strong><em>AAAI'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15536873427310696150&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://tshu.io/PHASE/">Project</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=w_7JMpGZRh0">Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration</a> - <strong><em>ICLR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16340001407726295133">All Versions</a>].</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/2j53v5nv">Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities</a> - <strong><em>CogSci'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=902767361177896884">All Versions</a>]. This work eveloped a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). The approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns. Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities.</p>
</li>
</ul>
<h3 id="analogy">Analogy<a class="headerlink" href="#analogy" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/metaphor/">Metaphor</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Metaphor, a poetically or rhetorically ambitious use of words, a figurative as opposed to literal use.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/reasoning-analogy/">Analogy and Analogical Reasoning</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Analogy, a comparison between two objects, or systems of objects, that highlights respects in which they are thought to be similar.</p>
</li>
<li>
<p><a href="https://1lib.net/book/1165963/e9aa3d">A Cognitive Theory of Metaphor</a> - <strong><em>MIT Press</em></strong>, 1985. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=a+cognitive+theory+of+metaphor&amp;btnG=">All Versions</a>]. A cognitive account on Metaphor.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370289900775">The structure-mapping engine: Algorithm and examples</a> - <strong><em>Artificial Intelligence</em></strong>, 1989. [<a href="https://scholar.google.com/scholar?cluster=16104901325436513899&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A computational implementation of analogy.</p>
</li>
<li>
<p><a href="https://cogsci.ucsd.edu/~coulson/203/gentner-markman-97.pdf">Structure mapping in analogy and similarity</a> - <strong><em>American Psychologist</em></strong>, 1997. [<a href="https://scholar.google.com/scholar?cluster=3497411606978611830&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A perspective unifying analogy and similarity judgement.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2022-26663-001">A theory of relation learning and cross-domain generalization</a> - <strong><em>Psychological Review</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8559821723107269122&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on the perspective of treating analogy as cross-domain generalization.</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v97/allen19a.html">Analogies Explained: Towards Understanding Word Embeddings</a> - <strong><em>ICML'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15445529659618849253&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Explaining the analogy capability in word embeddings.</p>
</li>
<li>
<p><a href="https://aclanthology.org/P17-1007/">Skip-Gram − Zipf + Uniform = Vector Additivity</a> - <strong><em>ACL'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=11732363456979525246&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.iiia.csic.es/~enric/papers/generalize_and_blend.pdf">Generalize and Blend: Concept Blending Based on Generalization, Analogy, and Amalgams</a> - <strong><em>ICCC'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=11073359237116879862&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v28/juhwang13.pdf">Analogy-preserving Semantic Embedding for Visual Object Categorization</a> - <strong><em>ICML'13</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=9332855910734484101&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The first application of analogy to machine learning.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2015/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf">VISALOGY: Answering Visual Analogy Questions</a> - <strong><em>NeurIPS'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=7665427758655324654&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/9010418">Detecting Unseen Visual Relations Using Analogies</a> - <strong><em>CVPR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=16686853801653819556&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370218301863">Analogy between concepts</a> - <strong><em>Artificial Intelligence</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1397905953174123757&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A mathematical account on analogy.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1902.00120">Learning to Make Analogies by Contrasting Abstract Relational Structure</a> - <strong><em>ICLR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15521573039503233138&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://aclanthology.org/2020.figlang-1.pdf#page=140">Sky + Fire = Sunset. Exploring Parallels between Visually Grounded Metaphors and Image Classifiers</a> - <strong><em>ACL'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5747285277687442001&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2006.04156.pdf">Analogy as Nonparametric Bayesian Inference over Relational Systems</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1798148167130120057&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.cs.jhu.edu/~alanlab/Pubs21/ichien2021visual.pdf">Visual Analogy: Deep Learning Versus Compositional Models</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1187822306970312749&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A human-deep-learning comparison on similarity judgement.</p>
</li>
<li>
<p><a href="https://pcl.sitehost.iu.edu/rgoldsto/pdfs/simdiff.pdf">Similarity involving attributes and relations: Judgments of similarity and difference are not inverses</a> - <strong><em>Psychological Science</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?cluster=13205938250772079784&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h3 id="causality">Causality<a class="headerlink" href="#causality" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Causality">Causality</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on causality, which is influence by which one event, process, state, or object (a cause) contributes to the production of another event, process, state, or object (an effect) where the cause is partly responsible for the effect, and the effect is partly dependent on the cause.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/causal-models/">Causal Models</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Causal models, which are mathematical models representing causal relationships within an individual system or population.</p>
</li>
<li>
<p><a href="http://www.jakebowers.org/ITVExperiments/angristimbensrubin96.pdf">Identification of Causal Effects Using Instrumental Variables</a> - <strong><em>Journal of the American Statistical Association</em></strong>, 1996. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17166265099721941605">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.psych.uni-goettingen.de/de/cognition/publikationen-dateien-waldmann/1992_predictive_vs_diagnostic.pdf">Predictive and Diagnostic Learning Within Causal Models: Asymmetries in Cue Competition</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 1992. [<a href="https://scholar.google.com/scholar?cluster=9614241045842043939&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Experimental evidences for distincting causality and association.</p>
</li>
<li>
<p><a href="https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195376746.001.0001/oxfordhb-9780195376746-e-46">Causal Reasoning</a> - <strong><em>The Oxford Handbook of Cognitive Psychology</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=11361740093816709089&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ftp.cs.ucla.edu/pub/stat_ser/R265.pdf">Reasoning with cause and effect</a> - 1998. Judea Pearl's tutorials on causal reasoning with operations on Bayesian networks.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3241036">The Seven Tools of Causal Inference, with Reflections on Machine Learning</a> - <strong><em>Communications of the ACM</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13296019510897277617&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Judea Pearl's review on causal inference in probabilistic graph models.</p>
</li>
<li>
<p><a href="https://cardiacmr.hms.harvard.edu/files/cardiacmr/files/toward_causal_representation_learning.pdf">Toward Causal Representation Learning</a> - <strong><em>Proceedings of the IEEE</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15629454810797806102&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Yoshua Bengio's review on the perspective of treating causal inference as a representation learning problem.</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/tom/papers/tbci.pdf">Theory-Based Causal Induction</a> - <strong><em>Psychological Review</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=13980129728092173387&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Thomas Griffiths' review on causal Bayesian theory induction.</p>
</li>
<li>
<p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/5483">Theory-Based Causal Transfer: Integrating Instance-Level Induction and Abstract-Level Structure Learning</a> - <strong><em>AAAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9411622427165139667&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A computatinoal account on causal transfer.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2703_6">Inferring causal networks from observations and interventions</a> - <strong><em>Cognitive Science</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=12050301037347772984&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cogsci.mindmodeling.org/2015/papers/0418/paper0418.pdf">Constraints on Hypothesis Selection in Causal Learning</a> - <strong><em>CogSci'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=2005&amp;sciodt=0%2C5&amp;cites=16920774374067505248&amp;scipsc=&amp;q=Constraints+on+hypothesis+selection+in+causal+learning&amp;btnG=">All Versions</a>].</p>
</li>
<li>
<p><a href="http://cocolab.stanford.edu/papers/GerstenbergEtAl17_PsychScience.pdf">Eye-tracking causality</a> - <strong><em>Psychological Science</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17518200401109470519">All Versions</a>].</p>
</li>
<li>
<p><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=d0TfP8EAAAAJ&amp;sortby=pubdate&amp;citation_for_view=d0TfP8EAAAAJ:S16KYo8Pm5AC">What happened? Reconstructing the past through vision and sound</a> - 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12975579257004398798">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s42113-021-00124-z">How do people generalize causal relations over objects? A non-parametric Bayesian account</a> - <strong><em>Computational Brain &amp; Behavior</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=3364672295201228487">All Versions</a>]. [<a href="https://psyarxiv.com/x57hf/">Preprint</a>]. How do people decide how general a causal relationship is, in terms of the entities or situations it applies to? What features do people use to decide whether a new situation is governed by a new causal law or an old one? How can people make these difficult judgments in a fast, efficient way? This paper addresses these questions in two experiments that ask participants to generalize from one (Experiment 1) or several (Experiment 2) causal interactions between pairs of objects. In each case, participants see an agent object act on a recipient object, causing some changes to the recipient.</p>
</li>
<li>
<p><a href="https://www.psych.uni-goettingen.de/de/cognition/publikationen-dateien-waldmann/2006_science.pdf">Causal Reasoning in Rats</a> - <strong><em>Science</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=17987039255457850949&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A piece of evidence for the capability of causal reasoning in intelligent animals.</p>
</li>
<li>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.183.4674&amp;rep=rep1&amp;type=pdf">Do New Caledonian crows solve physical problems through causal reasoning?</a> - <strong><em>Proceedings of the Royal Society B: Biological Sciences</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=18374985546068164189&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A piece of evidence for the capability of causal reasoning in intelligent animals.</p>
</li>
<li>
<p><a href="http://fitelson.org/woodward/leslie.pdf">Do six-month-old infants perceive causality?</a> - <strong><em>Cognition</em></strong>, 1987. [<a href="https://scholar.google.com/scholar?cluster=14270905342434182186&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h3 id="commonsense">Commonsense<a class="headerlink" href="#commonsense" title="Permanent link">&para;</a></h3>
<h4 id="intuitive-physics">Intuitive Physics<a class="headerlink" href="#intuitive-physics" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://github.com/lishiqianhugh/Intuitive_Physics_Reading_List">Intuitive Physics Reading List</a> - <strong><em>GitHub</em></strong>. A reading list on intuitive physics, maintained actively by Shiqian Li.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661317301262">Intuitive Physics: Current Research and Controversies</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=0,5&amp;cluster=12085981794958916203">All Versions</a>]. Hongjing Lu's review on intuitive physics.</p>
</li>
<li>
<p><a href="https://www.pnas.org/doi/pdf/10.1073/pnas.1610344113">Functional neuroanatomy of intuitive physical inference</a> - <strong><em>Proceedings of the National Academy of Sciences</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=1792195093536891402&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A piece of evidence for the functional part of intuitive physics in human brain.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661317301134">Mind Games: Game Engines as an Architecture for Intuitive Physics</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14527964477161848029&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. Tomer Ullman's review on simulation-based intuitive physics.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0010028517301822">Learning physical parameters from dynamic scenes</a> - <strong><em>Cognitive Psychology</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=5103729321433959736&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010028521000190">Limits on Simulation Approaches in Intuitive Physics</a> - <strong><em>Cognitive Psychology</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=6329029167380621767">All Versions</a>]. Ernest Davis's perspective against intuitive physics, that physcial reasoning is logical reasoning instead of intuition.</p>
</li>
<li>
<p><a href="https://psyarxiv.com/y4a8x/download?format=pdf">Partial Mental Simulation Explains Fallacies in Physical Reasoning</a> - <strong><em>Cognitive Neuropsychology</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=15541954459060383152&amp;hl=en&amp;as_sdt=2005">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41562-022-01394-8">Intuitive physics learning in a deep-learning model inspired by developmental psychology</a> - <strong><em>Nature Human Behavior</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=13803979681049451699&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A machine-learning dataset designed to evaluate conceptual understanding of intuitive physics, adopting the violation-of-expectation (VoE) paradigm from developmental psychology; a deep-learning system that learns intuitive physics directly from visual data, inspired by studies of visual cognition in children.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2019/hash/4191ef5f6c1576762869ac49281130c9-Abstract.html">PHYRE: A New Benchmark for Physical Reasoning</a> - <strong><em>NeurIPS'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9555658528231205655&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A benchmark for AI physical reasoning.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s42256-022-00583-4">Phy-Q as a measure for physical reasoning intelligence</a> - <strong><em>Nature Machine Intelligence</em></strong>, 2023. [<a href="https://www.nature.com/articles/s42256-019-0072-x">NMI Challenge</a>]. An interactive benchmark for AI physical reasoning.</p>
</li>
</ul>
<h4 id="ai-commonsense-reasoning">AI Commonsense Reasoning<a class="headerlink" href="#ai-commonsense-reasoning" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.sciencedirect.com/book/9781483207704/representations-of-commonsense-knowledge">Representations of Commonsense Knowledge</a> - <strong><em>Morgan Kaufmann</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?cluster=8861902735724600978">All Versions</a>]. A classic book on commonsense knowledge.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F3-540-53487-3_59">Towards a theory of commonsense visual reasoning</a> - <strong><em>FSTTCS</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?cluster=13178231862265713961&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on visual commonsense.</p>
</li>
<li>
<p><a href="http://cs.wellesley.edu/~cs125/reading/commonsenseAI.pdf">Commonsense reasoning and commonsense knowledge in artificial intelligence</a> - <strong><em>Communications of the ACM</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=13786590180441485203&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Gary Marcus's review on commonsense knowledge in AI.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8953217">From Recognition to Cognition: Visual Commonsense Reasoning</a> - <strong><em>CVPR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15467433880059136365&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="http://visualcommonsense.com/">Project</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1911.11641.pdf">PIQA: Reasoning about Physical Commonsense in Natural Language</a> - <strong><em>AAAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10110424163152713144&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9156347">Visual Commonsense R-CNN</a> - <strong><em>CVPR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6886229776034162585&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=Byg1v1HKDB">Abductive Commonsense Reasoning</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16544200144479839958&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Abductive commonsense reasoning on large language models.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58558-7_30">VisualCOMET: Reasoning About the Dynamic Context of a Still Image</a> - <strong><em>ECCV'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7681600847940772451&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://aclanthology.org/2024.naacl-long.469/">UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations</a> - <strong><em>NAACL'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=470445696014235795">All Versions</a>]. This paper explores the task of uncommonsense abductive reasoning. Given a piece of context with an unexpected outcome, this task requires reasoning abductively to generate an explanation that makes the unexpected outcome more likely in the context.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2020.emnlp-main.703.pdf">Experience Grounds Language</a> - <strong><em>EMNLP'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3734668471751920487&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A perspective on the furture of computational linguistics research---commonsense-driven and embodied language.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2021.emnlp-main.162/">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</a> - <strong><em>EMNLP'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12305856131717604775&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.charleskemp.com/papers/hanrpk_humanlikepropertyinductionisachallengeforlargelanguagemodels.pdf">Human-like property induction is a challenge for large language models</a> - <strong><em>CogSci'22</em></strong>, 2022. </p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2305.17390">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</a> - <strong><em>NeurIPS'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=3844178012869500706&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://swiftsage.github.io/">Project</a>].</p>
</li>
</ul>
<h4 id="commonsense-knowledgebase">Commonsense Knowledgebase<a class="headerlink" href="#commonsense-knowledgebase" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><a href="https://www.wikihow.com/Main-Page">wikiHow</a> - <strong><em>wikiHow.com</em></strong>. wikiHow is on website hosting step-by-step "How-to" procedural instructions across various domains and topics.</p>
</li>
<li>
<p><a href="https://theworldavatar.io/">The World Avatar</a> - <strong><em>The World Avatar™</em></strong>. A large-scale dynamic knowledge graph connecting concepts with relations to digitalize molecules, buildings, cities, and countries.</p>
</li>
<li>
<p><a href="https://faculty.cc.gatech.edu/~isbell/classes/reading/papers/lenat95cyc.pdf">CYC: A Large-Scale Investment in Knowledge Infrastructure</a> - <strong><em>Communications of the ACM</em></strong>, 1995. [<a href="https://scholar.google.com/scholar?cluster=6505009388871605141&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The first attempt to build large-scale commonse knoweldgebase from human knowledge.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1612.03975.pdf">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</a> - <strong><em>AAAI'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7089916805257737701&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Latest version of ConceptNet. </p>
</li>
<li>
<p><a href="https://www.aaai.org/Library/Symposia/Spring/2002/ss02-09-011.php">The Public Acquisition of Commonsense Knowledge</a> - <strong><em>Proceedings of AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Access</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=12533779219524472080&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The first attempt for acquring commonsense knowlege from humans' activities on the internet.</p>
</li>
<li>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.914&amp;rep=rep1&amp;type=pdf">Open Mind Common Sense: Knowledge Acquisition from the General Public</a> - <strong><em>OTM Confederated International Conferences'02</em></strong>, 2002. [<a href="https://scholar.google.com/scholar?cluster=11431785236825227404&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]..</p>
</li>
<li>
<p><a href="http://www.aladdin.cs.cmu.edu/papers/pdfs/y2006/verbosity.pdf">Verbosity: A Game for Collecting Common-Sense Facts</a> - <strong><em>CHI'06</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=7793704394155465847&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/fullHtml/10.1145/1378704.1378719">Designing games with a purpose</a> - <strong><em>Communications of the ACM</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=18332117920150730595&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://people.mpi-inf.mpg.de/~ntandon/papers/aaai-2014-tandon.pdf">Acquiring Comparative Commonsense Knowledge from the Web</a> - <strong><em>AAAI'14</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=16641273554706459553&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/abstract/document/9904017">Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale</a> - <strong><em>IEEE Transactions on Visualization and Computer Graphics</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10724509334112758172&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. This paper presents Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts.</p>
</li>
</ul>
<h3 id="inductive-logic-program-synthesis">Inductive Logic &amp; Program Synthesis<a class="headerlink" href="#inductive-logic-program-synthesis" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-inductive/">Inductive Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Inductive Logic, which is a logic of evidential support.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/modeltheory-fo/">First-order Model Theory</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on First-order Model Theory, which is a branch of mathematics that deals with the relationships between descriptions in first-order languages and the structures that satisfy these descriptions.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-paraconsistent/">Paraconsistent Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Paraconsistent Logic, where any logic is paraconsistent as long as it is not explosive.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logical-consequence/">Logical Consequence</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Logical Consequence, which is about the relation between premises and conclusions in valid arguments.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logical-pluralism/">Logic Pluralism</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Logic Pluralism, which is the view that there is more than one correct logic.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-firstorder-emergence/">The Emergence of First-Order Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on the emergence of first-order logic, mainly about first-order logic is natural retrospect.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-higher-order/">Second-order and Higher-order Logic</a> - <strong><em>Plato Stanford</em></strong>. </p>
</li>
<li>
<p><a href="https://www.ijcai.org/Proceedings/83-1/Papers/109.pdf">The Discovery of the Equator or Concept Driven Learning</a> - <strong><em>IJCAI'83</em></strong>, 1983. [<a href="https://scholar.google.com/scholar?cluster=15712225225140903169&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on second-order metarules.</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F3-540-44797-0_10">Towards combining inductive logic programming with Bayesian networks</a> - <strong><em>ILP'01</em></strong>, 2001. [<a href="https://scholar.google.com/scholar?cluster=2904180673047700407&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.doc.ic.ac.uk/~shm/Papers/metagol_gram.pdf">Meta-interpretive learning: application to grammatical inference</a> - <strong><em>Machine Learning</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=17075313112718885592&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Stephen Muggleton's original paper on Meta-Interpretive Learning (MIL).</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/ijcai15-metagolo.pdf">Learning Efficient Logical Robot Strategies Involving Composable Objects</a> - <strong><em>IJCAI'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=5109851972354087162&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/ijcai16-metafunc.pdf">Learning Higher-Order Logic Programs through Abstraction and Invention</a> - <strong><em>IJCAI'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=10945054943203858325&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F978-3-319-99960-9_3">How Much Can Experimental Cost Be Reduced in Active Learning of Agent Strategies?</a> - <strong><em>ILP'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8152380236842970357&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s10994-018-5710-8">Meta-Interpretive Learning from noisy images</a> - <strong><em>Machine Learning</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=5719375383968868329&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/mlj18-metaopt.pdf">Learning efficient logic programs</a> - <strong><em>Machine Learning</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17955696870252443734&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/mlj19-metaho.pdf">Learning higher-order logic programs</a> - <strong><em>Machine Learning</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6723896359456002413&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/mlj19-reduce.pdf">Logical reduction of metarules</a> - <strong><em>Machine Learning</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4577603126537024540&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/ijcai19-playgol.pdf">Playgol: Learning Programs Through Play</a> - <strong><em>IJCAI'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=556522464212000763&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007%2Fs00354-019-00054-2">Machine Discovery of Comprehensible Strategies for Simple Games Using Meta-interpretive Learning</a> - <strong><em>New Generation Computing</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=11019349634035542991&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://andrewcropper.com/pubs/aaai20-forgetgol.pdf">Forgetting to Learn Logic Programs</a> - <strong><em>AAAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13676986733133377042&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.ijcai.org/proceedings/2020/673">Turning 30: New Ideas in Inductive Logic Programming</a> - <strong><em>IJCAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17980870844719684257&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2008.07912">Inductive logic programming at 30: a new introduction</a> - <strong><em>Journal of Artificial Intelligence Research</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=317114056670544302&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A 30-year comprehensive review on Inductive Logic Programming.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2005.02259.pdf">Learning programs by learning from failures</a> - <strong><em>Machine Learning</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6797200487935462023&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.ijcai.org/proceedings/2020/320">Complete Bottom-Up Predicate Invention in Meta-Interpretive Learning</a> - <strong><em>IJCAI'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6085183078630665234&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2106.07464.pdf">Meta-Interpretive Learning as Metarule Specialisation</a> - <strong><em>Machine Learning</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14684315775211086859&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370204000591">Qualitative choice logic</a> - <strong><em>Artificial Intelligence</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=1586187056162326386&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.ijcai.org/Proceedings/16/Papers/278.pdf">Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings</a> - <strong><em>IJCAI'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=15955040483290586781&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/S0024610704006106">Finitely Generated Groups and First-Order Logic</a> - <strong><em>Journal of The London Mathematical Society-second Series</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=3457158221419711506&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://vigilworkshop.github.io/static/papers-2021/25.pdf">Leveraging Language for Abstraction and Program Search</a> - <strong><em>ICML'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Leveraging+Language+for+Abstraction+and+Program+Search&amp;btnG=">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/hash/f7e2b2b75b04175610e5a00c1e221ebb-Abstract.html">Program Synthesis Guided Reinforcement Learning</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17353674428642875269&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cogtoolslab.github.io/pdf/wang_cogsci_2021a.pdf">Learning Part-Based Abstractions for Visual Object Concepts</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+Part-Based+Abstractions+for+Visual+Object+Concepts&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2108.07732">Program Synthesis with Large Language Models</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=15213050540818392833">All Versions</a>]. This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3571249">Combining Functional and Automata Synthesis to Discover Causal Reactive Programs</a> - <strong><em>POPL'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10470162446663474225&amp;as_sdt=0,5">All Versions</a>]. A new algorithm that synthesizes functional reactive programs from observation data, which iterates between a functional synthesis step, which attempts to generate a transition function over observed states, and an automata synthesis step, which adds any additional latent state necessary to fully account for the observations.</p>
</li>
<li>
<p><a href="http://cap.csail.mit.edu/sites/default/files/research-pdfs/Synthesizing%20theories%20of%20human%20language%20with%20Bayesian%20program%20induction.pdf">Synthesizing theories of human language with Bayesian program induction</a> - <strong><em>Nature Communications</em></strong>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8603772394100237159&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2306.12672">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=13778788929096574993">All Versions</a>]. Rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. Linguistic meaning is framed as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. </p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v139/hong21a.html">Latent Programmer: Discrete Latent Codes for Program Synthesis</a> - <strong><em>ICML'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9789877360194738968">All Versions</a>]. Paper introducing the Latent Programmer, a two-level program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language.</p>
</li>
<li>
<p><a href="https://proceedings.mlr.press/v202/gao23f">PAL: Program-aided Language Models</a> - <strong><em>ICML'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=14898051625978777315&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Paper presenting an approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2023.acl-long.411/">Large Language Models Meet NL2Code: A Survey</a> - <strong><em>ACL'23</em></strong>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11868015824802341463&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://nl2code.github.io/">NL2Code Website</a>]. A paper presenting a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics, suggesting that the key factors contributing to the success of large language models for NL2Code are “Large Size, Premium Data, Expert Tuning”.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3597503.3608128">A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges</a> - <strong><em>ICSE'24</em></strong>, 2024. [<a href="https://scholar.google.com/scholar?cluster=3696356619002071917&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A survey finding that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. </p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2308.10620">Large Language Models for Software Engineering: A Systematic Literature Review</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=10466731638053452642&amp;as_sdt=0,5">All Versions</a>]. A systematic literature review on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes.  </p>
</li>
</ul>
<h3 id="knowledge-representation">Knowledge Representation<a class="headerlink" href="#knowledge-representation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://1lib.net/book/511192/9eab86">Handbook of Knowledge Representation</a> - <strong><em>Elsevier</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=14732064619564679879&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A pragmatical handbook for all kinds of knowledge representation modes.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-ontology/">Logic and Ontology</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on logic and ontology, mainly about the intersections of logic and ontology in many significant philosophy problems.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/language-thought/">The Language of Thought Hypothesis</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on the laugnage of though hypothesis, which proposes that thinking occurs in a mental language.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/knowledge-analysis/">The Analysis of Knowledge</a> - <strong><em>Plato Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/scientific-representation/">Scientific Representation</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on scientific representation, focusing on how scientific models represent their target systems.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/self-knowledge/">Self-Knowledge</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on self-knowledge, which standardly refers to knowledge of one's own mental states—that is, of what one is feeling or thinking, or what one believes or desires.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/common-knowledge/">Common Knowledge</a> - <strong><em>Plato Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/sense-data/">Sense-Data</a> - <strong><em>Plato Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/supervenience/">Supervenience</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on supervenience, where a set of properties A supervenes upon another set B just in case no two things can differ with respect to A-properties without also differing with respect to their B-properties.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-dialogical/">Dialogical Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on dialogical logic, which is a dialogue-based approach to logic and argumentation rooted in a research tradition that goes back to dialectics in Greek Antiquity, when problems were approached through dialogues in which opposing parties discussed a thesis through questions and answers.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-temporal/">Temporal Logic</a> - <strong><em>Plato Stanford</em></strong>. </p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-modal/">Modal Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Modal Logic, which is the study of the deductive behavior of the expressions 'it is necessary that' and 'it is possible that'.</p>
</li>
<li>
<p><a href="https://plato.stanford.edu/entries/logic-epistemic/">Epistemic Logic</a> - <strong><em>Plato Stanford</em></strong>. A computational philosophy account on Epistemic Logic, which is a subfield of epistemology concerned with logical approaches to knowledge, belief and related notions.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Epistemic_modal_logic">Epistemic Modal Logic</a> - <strong><em>Wikipedia</em></strong>.</p>
</li>
<li>
<p><a href="https://perception.jhu.edu/files/PDFs/21_Relations/HafriFirestone_2021_SeeingRelations_TiCS.pdf">The Perception of Relations</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12190078466818849725&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. Chaz Firestone's review on the perception of relation, in constrast to the conventional reasoning view.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370284900390">Commonsense reasoning about causality: Deriving behavior from structure</a> - <strong><em>Artificial Intelligence</em></strong>, 1984. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14940738362673077704">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1023/B:SYNT.0000024912.56773.5e">Logics for Epistemic Programs</a> - <strong><em>Synthese</em></strong>, 2004. [<a href="https://scholar.google.com/scholar?cluster=11403619699670839488&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1">All Versions</a>].</p>
</li>
<li>
<p><a href="https://tomgruber.org/writing/ontolingua-kaj-1993.pdf">A Translation Approach to Portable Ontology Specifications</a> - <strong><em>Knowledge Acquisition</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=14668658395073605123&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.cs.ox.ac.uk/activities/ieg/e-library/sources/harnad90_sgproblem.pdf">The Symbolic Grounding Problem</a> - <strong><em>Physica D: Nonlinear Phenomena</em></strong>, 1990. [<a href="https://scholar.google.com/scholar?cluster=6279614024681929496&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00585.x?__cf_chl_captcha_tk__=pmd_Q6xVT1AstoEUxA7xS3_10HyDVsk8W_DzWgOPho_Njnw-1635210931-0-gqNtZGzNA1CjcnBszQvl">Learning overhypotheses with hierarchical Bayesian models</a> - <strong><em>Developmental Science</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=18041836774924845900&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1080/03640210701802071">A Rational Analysis of Rule-Based Concept Learning</a> - <strong><em>Cognitive Science</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=7765061503727822620&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.charleskemp.com/papers/KempGT08.pdf">Theory Acquisition and the Language of Thought</a> - <strong><em>CogSci'08</em></strong>, 2008. [<a href="https://scholar.google.com/scholar?cluster=1839916602381147749&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. </p>
</li>
<li>
<p><a href="http://web.mit.edu/tomeru/www/papers/tlss2010.pdf">Theory Acquisition as Stochastic Search</a> - <strong><em>CogSci'10</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=16324634056226561429&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.charleskemp.com/papers/kemptng09.pdf">A probabilistic model of theory formation</a> - <strong><em>Cognition</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=7705799129887482041&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://core.ac.uk/display/78064072">Bootstrapping in a language of thought: A formal model of numerical concept learning</a> - <strong><em>Cognition</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=13046606910781656302&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://cbmm-dev.mit.edu/sites/default/files/publications/CBMM-Memo-010.pdf">Concepts in a Probabilistic Language of Thought</a> - <strong><em>Center for Brains, Minds, and Machines MEMO No.010</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14593712389828476130">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.charleskemp.com/papers/kemp_exploringtheconceptualuniverse.pdf">Exploring the Conceptual Universe</a> - <strong><em>Psychological Review</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=17824067813343816306&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.charleskemp.com/papers/kempj_ataxonomyofinductiveproblems.pdf">A taxonomy of inductive problems</a> - <strong><em>Psychonomic Bulletin &amp; Review</em></strong>, 2014. [<a href="https://scholar.google.com/scholar?cluster=2571009743105592927&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://colala.berkeley.edu/papers/piantadosi2016logical.pdf">The Logical Primitives of Thought: Empirical Foundations for Compositional Cognitive Models</a> - <strong><em>Psychological Review</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=5316027496661813145&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12580">The Emergence of Organizing Structure in Conceptual Representation</a> - <strong><em>Cognitive Science</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=4986316323923233074&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cogtoolslab.github.io/pdf/wang_cogsci_2021b.pdf">Theory Acquisition as Constraint-Based Program Synthesis</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=525148607069840280&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://escholarship.org/uc/item/9j00x928">Connecting perceptual and procedural abstractions in physical construction</a> - <strong><em>CogSci'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Connecting+perceptual+and+procedural+abstractions+in+physical+construction&amp;btnG=">All Versions</a>].</p>
</li>
<li>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.4086&amp;rep=rep1&amp;type=pdf">Introduction to The Fluent Calculus</a> - <strong><em>Linkoeping University Electronic Press</em></strong>, 1998. [<a href="https://scholar.google.com/scholar?cluster=12069059079023496731&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0004370299000338">From situation calculus to fluent calculus: State update axioms as a solution to the inferential frame problem</a> - <strong><em>Artificial Intelligence</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=10854895617698839149&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.stat.ucla.edu/~sczhu/papers/Conf_2013/Learning_AoG_NeurIPS_2013.pdf">Unsupervised Structure Learning of Stochastic And-Or Grammars</a> - <strong><em>NeurIPS'13</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=4354984630817844670">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psyarxiv.com/ysndt">Algorithms of Adaptation in Inductive Inference</a> - <strong><em>Cognitive Psychology</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16222039361294164246&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/0010027795006743">A representational analysis of numeration systems</a> - <strong><em>Cognition</em></strong>, 1995. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=8852566070856662412">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2205.07455">Reasoning about Procedures with Natural Language Processing: A Tutorial</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=11364086808527515615&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h3 id="cognitive-development">Cognitive Development<a class="headerlink" href="#cognitive-development" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://arxiv.org/abs/1810.07528">Machine Common Sense Concept Paper</a> - <strong><em>DARPA</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=1603121108181262769&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. DARPA's perspective on integrating core knowledge from development psychology into machine intelligence systems.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Cognitive_development">Cognitive Development</a> - <strong><em>Wikipedia</em></strong>. </p>
</li>
<li>
<p><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+Development%3A+an+information+processing+approach&amp;btnG=">Cognitive development: An information processing approach</a> - <strong><em>B.Blackwell</em></strong>, 1991. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+development%3A+An+information+processing+approach&amp;btnG=">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2012-12791-001">Reconstructing constructivism: Causal models, Bayesian learning mechanisms, and the theory theory</a> - <strong><em>Psychological Bulletin</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=11218217347365817167&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Alison Gopnik's review on the constructivism idea of developmental research.</p>
</li>
<li>
<p><a href="https://doi.apa.org/doiLanding?doi=10.1037/rev0000153">Towards a rational constructivist theory of cognitive development</a> - <strong><em>Psychological Review</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3294824172745724080&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Fei Xu's review extending Gopnik's view of constructivism, with the rationality as constraint.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1364661312001301">The origins of inquiry: inductive inference and exploration in early childhood</a> - <strong><em>Trends in Cognitive Sciences</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=5189329081728071335&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Laura Schulz's review on children's exploratory play.</p>
</li>
<li>
<p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-devpsych-070120-014806">Play, Curiosity, and Cognition</a> - <strong><em>Annual Review of Developmental Psychology</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10278208468154249192&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>]. Laura Schulz's review on children's exploratory play, which proposes a new perspective on exploratory play to explain the emergence of irrational behaviors in play.</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1981-32566-001">From exploration to play: A cross-sectional study of infant free play behavior</a> - <strong><em>Developmental Psychology</em></strong>, 1981. [<a href="https://scholar.google.com/scholar?cluster=15547331535034599545&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/1467-8624.00224">Detecting Blickets: How Young Children Use Information about Novel Causal Powers in Categorization and Induction</a> - <strong><em>Children Development</em></strong>, 2003. [<a href="https://scholar.google.com/scholar?cluster=9049737233568227380&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://eccl.scripts.mit.edu/papers/bonawitzandschulzseriousfun.pdf">Serious fun: Preschoolers engage in more exploratory play when evidence is confounded</a> - <strong><em>Developmental Psychology</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=3033619407322882147&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://stahla.pages.tcnj.edu/files/2015/08/Stahl_Feigenson_Science_2015.pdf">Observing the unexpected enhances infants' learning and exploration</a> - <strong><em>Science</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?start=10&amp;hl=en&amp;as_sdt=0,5&amp;cluster=9247917261616759689">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/2008-12114-008">Word, thought, and deed: the role of object categories in children's inductive inferences and exploratory play</a> - <strong><em>Developmental Psychology</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=13947689064550390312&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010027711000916">Where science starts: Spontaneous experiments in preschoolers' exploratory play</a> - <strong><em>Cognition</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16321989770180281706">All Versions</a>].</p>
</li>
<li>
<p><a href="http://alisongopnik.com/Papers_Alison/Scientific%20Thinking%20in%20young%20Children.pdf">Scientific thinking in young children: Theoretical advances, empirical research, and policy implications</a> - <strong><em>Science</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9103846738385460508&amp;hl=en&amp;as_sdt=2005">All Versions</a>].</p>
</li>
<li>
<p><a href="http://eccl.scripts.mit.edu/papers/Finding%20New%20Facts_%20Thinking%20New%20Thoughts.pdf">Finding New Facts; Thinking New Thoughts</a> - <strong><em>Advances in Child Development and Behavior</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Finding+new+facts%3B+thinking+new+thoughts&amp;btnG=">All Versions</a>]. </p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0885201412000445">Theory learning as stochastic search in the language of thought</a> - <strong><em>Cognitive Development</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=8036476579458645432&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.science.org/doi/abs/10.1126/science.aan2317">Infants make more attempts to achieve a goal when they see adults persist</a> - <strong><em>Science</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=2617011825272996810&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://cognitivesciencesociety.org/cogsci20/papers/0716/0716.pdf">Knowing when to quit: Children consider access to solutions when deciding whether to persist</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15997297570269958414&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psyarxiv.com/aq3rp/">Bayesian Models of Conceptual Development: Learning as Building Models of the World</a> - <strong><em>Annual Review of Developmental Psychology</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=646614032563248495&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12765">Sticking to the Evidence? A Behavioral and Computational Case Study of Micro-Theory Change in the Domain of Magnetism</a> - <strong><em>Cognitive Science</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4409900195679222965&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://junyichu.mit.edu/sites/default/files/documents/2018-05-14%20CogSci%20Final.pdf">Cognitive pragmatism: Children flexibly choose between facts and conjectures</a> - <strong><em>CogSci'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6978944437676543728&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psyarxiv.com/9yra2/">Exploratory play, rational action, and efficient search</a> - <strong><em>CogSci'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17529638197045429028&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/cdev.13647?saml_referrer">Children selectively endorse speculative conjectures</a> - <strong><em>Child Development</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5672344544260882286&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/buy/2017-12497-003">Learning higher-order generalizations through free play: Evidence from 2- and 3-year-old children</a> - <strong><em>Developmental Psychology</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=4386474921214936914&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0502">Childhood as a solution to explore–exploit tensions</a> - <strong><em>Philosophical Transactions of the Royal Society B: Biological Sciences</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11960188575664977017&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41467-021-23431-2">Children's exploratory play tracks the discriminability of hypotheses</a> - <strong><em>Nature Communications</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12389351553206792907&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2020">All Versions</a>].</p>
</li>
<li>
<p><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8624.2010.01499.x?saml_referrer">A Developmental Perspective on Executive Function</a> - <strong><em>Child Development</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?cluster=11347590808138984649&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://journals.sagepub.com/doi/pdf/10.1177/1745691620904771">Rethinking Executive Function and Its Development</a> - <strong><em>Psychological Science</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16570230278367237499&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.3758/s13428-012-0210-4">Age-of-acquisition ratings for 30,000 English words</a> - <strong><em>Behavior Research Methods</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=6752414178722956940&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="http://crr.ugent.be/archives/806">Project</a>]. A database for age-of-acquisition ratings for over 30k English words.</p>
</li>
</ul>
<h3 id="learning-in-the-open-world">Learning in the Open World<a class="headerlink" href="#learning-in-the-open-world" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S002224961730010X">Online learning of symbolic concepts</a> - <strong><em>Journal of Mathematical Psychology</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?start=20&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5&amp;cites=8036476579458645432&amp;scipsc=">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8413121">Zero-Shot Learning—A Comprehensive Evaluation of the Good, the Bad and the Ugly</a> - <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11909080239486864961&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on zero-shot learning.</p>
</li>
<li>
<p><a href="https://www.4paradigm.com/upload/file/20210427/20210427225045_12063.pdf">Generalizing from a few examples: A survey on few-shot learning</a> - <strong><em>ACM Computing Survey</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7932202448069313464&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/7298799">Towards Open World Recognition</a> - <strong><em>CVPR'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=856704237994181529&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The first paper introducing the problem of open-world recognition.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780542">Towards Open Set Deep Networks</a> - <strong><em>CVPR'16</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3571743951915089896&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2007.02519.pdf">In the Wild: From ML Models to Pragmatic ML Systems</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15243890330014986346&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive review on incremental machine learning.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2002.04108.pdf">Adversarial Filters of Dataset Biases</a> - <strong><em>ICML'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11617966867048191189&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2009.01797.pdf">A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=2640432662088551010&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2011.12216.pdf">Energy-Based Models for Continual Learning</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7094884707139778576&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://energy-based-model.github.io/Energy-Based-Models-for-Continual-Learning/">Project</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1804.04340v2.pdf">Zero-Shot Object Detection</a> - <strong><em>ECCV'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2027060030559987993&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2103.02603v1.pdf">Towards Open World Object Detection</a> - <strong><em>CVPR'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9715328489246217151&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. [<a href="https://github.com/JosephKJ/OWOD">Project</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3123266.3123323">Learning to Recognise Unseen Classes by A Few Similes</a> - <strong><em>MM'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?q=related:FZZr2BK0U6YJ:scholar.google.com/&amp;scioq=Learning+to+Recognise+Unseen+Classes+by+A+Few+Similes&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.kr.org/2020/87/kr2020-0087-chen-et-al.pdf">Ontology-guided Semantic Composition for Zero-Shot Learning</a> - <strong><em>KR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1825132732653262003&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2102.07339.pdf">OntoZSL: Ontology-enhanced Zero-shot Learning</a> - <strong><em>WWW'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1042573079110416209&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2103.00070">Knowledge-aware Zero-Shot Learning: Survey and Perspective</a> - <strong><em>IJCAI'21</em></strong> 2021. [<a href="https://scholar.google.com/scholar?cluster=2596179801089642923&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/8099612">From Red Wine to Red Tomato: Composition with Context</a> - <strong><em>CVPR'17</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6959320578989247472&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-01246-5_11">Attributes as Operators: Factorizing Unseen Attribute-Object Compositions</a> - <strong><em>ECCV'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11627198158637727139&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/9010671">Learning Compositional Representations for Few-Shot Recognition</a> - <strong><em>CVPR'19</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7363445845219257348&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/9156505">Symmetry and Group in Attribute-Object Compositions</a> - <strong><em>CVPR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16870815556752021056&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/file/1010cedf85f6a7e24b087e63235dc12e-Paper.pdf">A causal view of compositional zero-shot recognition</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2543173389101020482&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/3394171.3413849">Compositional Few-Shot Recognition with Primitive Discovery and Enhancing</a> - <strong><em>MM'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15817839338790433509&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/9156655">Learning Unseen Concepts via Hierarchical Decomposition and Composition</a> - <strong><em>CVPR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14161656227038242300&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h3 id="learning-with-cognitive-plausibility">Learning with Cognitive Plausibility<a class="headerlink" href="#learning-with-cognitive-plausibility" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">Accuracy and Precision</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on the distinctions and the trade-off between accuracy and precision.</p>
</li>
<li>
<p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev.ps.40.020189.003131">Cognitive Science: Definition, Status, and Questions</a> - <strong><em>Annual Review of Psychology</em></strong>, 1989. [<a href="https://scholar.google.com/scholar?cluster=8549671583307260475&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://people.csail.mit.edu/torralba/courses/6.870/papers/Biederman_RBC_1987.pdf">Recognition-by-Components: A Theory of Human Image Understanding</a> - <strong><em>Psychological Review</em></strong>, 1987. [<a href="https://scholar.google.com/scholar?cluster=16522931798979362446&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on the recognition-by-components theory.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/s41586-019-1138-y">Machine Behaviour</a> - <strong><em>Nature</em></strong>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7881171273277686092&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://yzhu.io/publication/dark2020engineering/paper.pdf">Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense</a> - <strong><em>Engineering</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=12292747257300299161&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Yixin Zhu and Song-Chun Zhu's review on visual commonsense.</p>
</li>
<li>
<p><a href="https://cims.nyu.edu/~brenden/papers/OrhanEtAl2020NeurIPS.pdf">Self-supervised Learning Through the eyes of a Child</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5608715260418451299&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Concept learning through near-natural co-occurrence frequency estimation.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1910.01442">CLEVRER: CoLlision Events for Video REpresentation and Reasoning</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4352064462350202338&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/hash/bf15e9bbff22c7719020f9df4badc20a-Abstract.html">BONGARD-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9164011458889391917&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/1143844.1143874">The relationship between Precision-Recall and ROC curves</a> - <strong><em>ICML'06</em></strong>, 2006. [<a href="https://scholar.google.com/scholar?cluster=10708180947310062390&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="http://export.arxiv.org/pdf/2009.08092">Distributional Generalization: A New Kind of Generalization</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=6190621467796247477&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0010027793900584">Learning and development in networks: The importance of starting small.</a> - <strong><em>Cognition</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?cluster=5133345254007462915&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on the idea of curriculum learning.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0010027799000311">Language acquisition in the absence of explicit negative evidence: how important is starting small?</a> - <strong><em>Cognition</em></strong>, 1999. [<a href="https://scholar.google.com/scholar?cluster=11813578367725362166&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/1553374.1553380">Curriculum Learning</a> - <strong><em>ICML'09</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=8740915934335425405&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper applying the idea of curriculum learning to machine learning.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/6126279">Parsing video events with goal inference and intent prediction</a> - <strong><em>ICCV'11</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?cluster=5979196784405021658&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/6751387">Inferring "Dark Matter" and "Dark Energy" from Videos</a> - <strong><em>ICCV'13</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=3467068307444498624&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. The original paper on latent state discovery from videos.</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/hash/4c26774d852f62440fc746ea4cdd57f6-Abstract.html">Attention over Learned Object Embeddings Enables Complex Visual Reasoning</a> - <strong><em>NeurIPS'21</em></strong>, 2021. [<a href="https://scholar.google.com/scholar?cluster=127829313460149801&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://papers.NeurIPS.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> - <strong><em>NeurIPS'13</em></strong>, 2013. [<a href="https://scholar.google.com/scholar?cluster=2410615501856807729&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9197172">Motion Reasoning for Goal-Based Imitation Learning</a> - <strong><em>ICRA'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7519230802512388210&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2020/file/64dcf3c521a00dbb4d2a10a27a95a9d8-Paper.pdf">Refactoring Policy for Compositional Generalizability using Self-Supervised Object Proposals</a> - <strong><em>NeurIPS'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2255457416066730255&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2106.13884">Multimodal Few-Shot Learning with Frozen Language Models</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=16154696122208258147&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5206772">Describing Objects by their Attributes</a> - <strong><em>CVPR'09</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=6853730684095116174&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2108.07783">Panoramic Learning with A Standardized Machine Learning Formalism</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=14222434793711614257&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://psycnet.apa.org/record/1996-10319-001">Graininess of judgment under uncertainty: An accuracy-informativeness trade-off</a> - <strong><em>Journal of Experimental Psychology</em></strong>, 1995. [<a href="https://scholar.google.com/scholar?cluster=15366302654259490472&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://openreview.net/forum?id=GFsU8a0sGB">Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms</a> - <strong><em>ICLR'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2486025806014234529&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<!--
### Tasks & Environments

#### Dataset Aggregation
  * [A Dataset and Architecture for Visual Reasoning with a Working Memory](https://link.springer.com/chapter/10.1007%2F978-3-030-01249-6_44) - ***ECCV'18***, 2018. [[Project](https://github.com/google/cog)].
  * [PHYRE: A New Benchmark for Physical Reasoning](https://research.fb.com/wp-content/uploads/2019/08/PHYRE-A-New-Benchmark-for-Physical-Reasoning-v4.pdf) - ***NeurIPS'19***, 2019.
  * [CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning](https://openreview.net/forum?id=HJgzt2VKPB) - ***ICLR'20***, 2020. [[Project](https://rohitgirdhar.github.io/CATER/)].
  * [CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning](https://arxiv.org/abs/2010.04296), 2020.

#### Embodied AI Environment
  * [ThreeDWorld](http://www.threedworld.org/) - ***MIT-IBM***. [[Paper](https://arxiv.org/abs/2007.04954)].
  * [Rearrangement: A Challenge for Embodied AI](https://arxiv.org/pdf/2011.01975.pdf), 2020.
  * [iGibson](http://svl.stanford.edu/igibson/) - ***Stanford***. [[Paper](https://ieeexplore.ieee.org/document/8954627)].
  * [AI2-THOR](https://ai2thor.allenai.org/ithor) - ***Allen Institute***. [[Paper](https://arxiv.org/abs/1712.05474)].
  * [Robo-THOR](https://ai2thor.allenai.org/robothor) - ***Allen Institute***. [[Paper](https://arxiv.org/abs/2004.06799)].
  * [Manipula-THOR](https://ai2thor.allenai.org/manipulathor) - ***Allen Institute***. [[Paper](https://arxiv.org/abs/2104.11213)].
  * [RLBench](https://sites.google.com/view/rlbench) - ***Imperial College***. [[Paper](https://ieeexplore.ieee.org/document/9001253)].

#### First-Person Vision
  * [First-Person Vision](https://ieeexplore.ieee.org/document/6232429) - ***Proceedings of the IEEE***, 2012.
  * [The Evolution of First Person Vision Methods: A Survey](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7055926) - ***Trans. CSVT***, 2015.
  * [Understanding the Nature of First-Person Videos: Characterization and Classification using Low-Level Features](http://vijaychan.github.io/Publications/2014%20CVPR%20Workshop%20-%20Understanding%20the%20Nature%20of%20First-Person%20Videos.pdf) - ***CVPR'14***, 2014.
  * [Pooled Motion Features for First-Person Videos](https://openaccess.thecvf.com/content_cvpr_2015/papers/Ryoo_Pooled_Motion_Features_2015_CVPR_paper.pdf) - ***CVPR'15***, 2015.
  * [Actor and Observer: Joint Modeling of First and Third-Person Videos](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf) - ***CVPR'18***, 2018.
  * [Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video](https://link.springer.com/chapter/10.1007/978-3-030-58452-8_41) - ***ECCV'20***, 2020.
  * [Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9088213) - ***Trans. PAMI***, 2020.
  * [View-Action Representation Learning for Active First-Person Vision](https://ieeexplore.ieee.org/document/9064828) - ***Trans. CSVT***, 2021.
  * [Design and Use Paradigms for Gazebo, An Open-Source Multi-Robot Simulator](https://ieeexplore.ieee.org/abstract/document/1389727) - ***IROS'04***, 2004. [[Project](http://gazebosim.org/)].
  * [ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning](https://arxiv.org/pdf/1605.02097v2.pdf) - ***CIG'16***, 2016. [[Project](http://vizdoom.cs.put.edu.pl/)].
  * [Is First Person Vision Challenging for Object Tracking? The TREK-100 Benchmark Dataset](https://arxiv.org/abs/2011.12263), 2020.  
  * **Visual Experience Database** [[Project](http://visualexperiencedatabase.org/research.html)]. [[Publications](http://visualexperiencedatabase.org/publications.html)].

#### Abstract Reasoning Challenge
  * [On the Measure of Intelligence](https://arxiv.org/pdf/1911.01547.pdf) - ***Google Research***, 2019.
  * [Abstract Reasoning Challenge](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/)

#### AI Birds Challenge
  * [AI-Birds](https://aibirds.org) - ***IJCAI***.
  * [Hi-Phy: A Benchmark for Hierarchical Physical Reasoning](https://openreview.net/forum?id=AcL1ORzw0Nf), 2021.

#### Minecraft
  * [Mining Learning and Crafting Scientific Experiments: A Literature Review on the Use of Minecraft in Education and Research](https://eric.ed.gov/?id=EJ1097278) - ***Journal on Eduction Technology & Society***, 2016.

##### Malmo Platform for Minecraft AI
  * [The Malmo Platform for Artificial Intelligence Experimentation](https://www.microsoft.com/en-us/research/publication/malmo-platform-artificial-intelligence-experimentation/) ***IJCAI'16***, 2016. 
  * [[Malmo](https://github.com/Microsoft/malmo#getting-started)]. 
  * [[Malmo-env](https://github.com/Microsoft/malmo/tree/master/MalmoEnv)]. 
  * [[Malmo-Tutorials](https://microsoft.github.io/malmo/0.17.0/Python_Examples/Tutorial.pdf)].
  * [[MineRL](https://minerl.io/)].
  * [[MarLo Challenge 2018](https://github.com/crowdAI/marLo)].

#####  **Artificial Intelligence**
  * [Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft](https://arxiv.org/abs/2106.14876), 2021.
  * [Learning to execute instructions in a Minecraft dialogue](https://www.aclweb.org/anthology/2020.acl-main.232/) - ***ACL'20***, 2020.
  * [Collaborative Dialogue in Minecraft](https://www.aclweb.org/anthology/P19-1537.pdf) - ***ACL'19***, 2019.
  * [Learning Skill Hierarchies from Predicate Descriptions and Self-Supervision](http://web.mit.edu/tslvr/www/papers/genplan20_camera_ready.pdf) - ***AAAI GenPlan Workshop***, 2020.
  * [AMRL: Aggregated Memory for Reinforcement Learning](https://openreview.net/pdf?id=Bkl7bREtDr) - ***ICLR'20***, 2020.
  * [MineRL: A Large-Scale Dataset of Minecraft Demonstrations](https://www.ijcai.org/Proceedings/2019/0339.pdf) ***IJCAI'19***, 2019. [[2020 Competition](https://arxiv.org/abs/2106.03748)].
  * [Design Mining for Minecraft Architecture](http://www.cs.cornell.edu/~eland/papers/aiide2018.pdf) - ***AAAI'18***, 2018.
  * [Adaptive Agents in Minecraft: A Hybrid Paradigm for Combining Domain Knowledge with Reinforcement Learning](https://link.springer.com/chapter/10.1007%2F978-3-319-71679-4_6) - ***AAMAS'17***, 2017.
  * [Asynchronous Data Aggregation for Training End to End Visual Control Networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/fp185-monfort-1.pdf) - ***AAMAS'17***, 2017.
  * [A Deep Hierarchical Approach to Lifelong Learning in Minecraft](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14630/13950) - ***AAAI'17***, 2017.
  * [Modular Multitask Reinforcement Learning with Policy Sketches](http://proceedings.mlr.press/v70/andreas17a.html) - ***ICML'17***, 2017.
  * [Control of memory, active perception, and action in minecraft](http://proceedings.mlr.press/v48/oh16.pdf) - ***ICML'16***, 2016.
  * [Learning Behavior from Demonstration in Minecraft via Symbolic Similarity Measures](https://github.com/YuzheSHI/awesome-agi-cocosci/blob/master/fdg2015.org/papers/fdg2015_paper_11.pdf) - ***FDG'15***, 2015.

#####  **Cognitive Science**
  * [How Players Speak to an Intelligent GameCharacter Using Natural Language Messages](http://todigra.org/index.php/todigra/article/view/88/139) - ***DiGRA***, 2018.
  * [Minecraft as a Generative Platform for Analyzing and Practicing Spatial Reasoning](https://link.springer.com/chapter/10.1007%2F978-3-030-57983-8_22) - ***Spatial Cognition'20***, 2020.
  * [Generative Design in Minecraft: Chronicle Challenge](http://computationalcreativity.net/iccc2019/papers/iccc19-lbp-7.pdf) - ***ICCC'20***, 2020.
  * [Minecraft as a Platform for Project-Based Learning in AI](https://aaai.org/ojs/index.php/AAAI/article/view/7070) - ***AAAI'20***, 2020.
  * [MC-Saar-Instruct: a Platform for Minecraft Instruction Giving Agents](https://www.aclweb.org/anthology/2020.sigdial-1.7.pdf) - ***SIGDial'20***, 2020.

*[Back to Top](#c)-->

<h2 id="academic-tools">Academic Tools<a class="headerlink" href="#academic-tools" title="Permanent link">&para;</a></h2>
<h3 id="courses">Courses<a class="headerlink" href="#courses" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://cbmm.mit.edu/education/courses/computational-cognitive-science">Computational Cognitive Science Courses</a> - <strong><em>MIT</em></strong>. Courses on computational cognitive science from MIT, Harvard, and Stanford.</p>
</li>
<li>
<p><a href="https://people.csail.mit.edu/asolar/SynthesisCourse/index.htm">Introduction to Program Synthesis</a> - <strong><em>MIT</em></strong>. Armando Solar-Lezama's elementary course on program synthesis.</p>
</li>
<li>
<p><a href="https://web.mit.edu/6.001/6.037/">Structure and Interpretation of Computer Programs</a> - <strong><em>MIT</em></strong>. [<a href="https://web.mit.edu/6.001/6.037/sicp.pdf">Book: SICP</a>]. [<a href="https://scholar.google.com/scholar?cluster=7488066943428166450&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Classic course on applying structural, procedural, and meta-linguistic abstraction to solve computational problems.</p>
</li>
<li>
<p><a href="https://faculty.ksu.edu.sa/sites/default/files/rosen_discrete_mathematics_and_its_applications_7th_edition.pdf">Discrete Mathematics and Its Applications</a>. Classic course on basic discrete mathematics, including matheatical logic, set theory, graph theory, formal language (and automata), basic number theory (e.g., counting), and other related topics.</p>
</li>
</ul>
<h3 id="programming">Programming<a class="headerlink" href="#programming" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://probmods.org/">Probabilistic Models of Cognition</a> - <strong><em>MIT</em></strong>. The probabilistic approach to cognitive science, which models learning and reasoning as inference in complex probabilistic models.</li>
</ul>
<h3 id="paper-writing">Paper Writing<a class="headerlink" href="#paper-writing" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://github.com/YuzheSHI/awesome-agi-cocosci/blob/master/LaTex/config.sty">LaTex Configuration</a> - <strong><em>LaTex</em></strong>. LaTex template for configuration file with elegant reference style (gray-colored reference, page backward reference).</p>
</li>
<li>
<p><a href="https://github.com/YuzheSHI/awesome-agi-cocosci/blob/master/BibTex/references_header.bib">BibTex Template</a> - <strong><em>BibTex</em></strong>. BibTex template for including abbreviations of journals and conferences in AI, Mathematics, and Cognitive Sciences.</p>
</li>
<li>
<p><a href="https://www.biorender.com/">bioRender</a> - <strong><em>bioRender</em></strong>. Create professional science figures in minutes by browsing thousands of pre-made icons and templates from more than 30 fields of life sciences.</p>
</li>
<li>
<p><a href="https://www.nature.com/documents/nature-summary-paragraph.pdf">How to construct a Nature summary paragraph</a> - <strong><em>Nature</em></strong>. Nature official guidelines for composing abstracts.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/d41586-020-03422-x">How to write a superb literature review</a> - <strong><em>Nature</em></strong>, 2020. Nature speaks to old hands and first timers about the work they did to make their reviews sing.</p>
</li>
<li>
<p><a href="https://www.nature.com/scitable/topicpage/scientific-papers-13815490/">Scientific Papers</a> - <strong><em>Nature</em></strong>. Nature guidance on writing scientific papers.</p>
</li>
<li>
<p><a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">The Machine Learning Reproducibility Checklist</a> - <strong><em>McGill University</em></strong>. Guidelines for introducing a machine learning algorithm with guarantee of reproducibility.</p>
</li>
</ul>
<h3 id="paper-reading">Paper Reading<a class="headerlink" href="#paper-reading" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://www.cs.uni-potsdam.de/bs/teaching/docs/courses/ss2020/scn/material/p83-keshavA.pdf">How to Read a Paper</a> - <strong><em>ACM SIGCOMM Computer Communication Review</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=7234542241721187587&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A comprehensive tutorial on reading scientific papers.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/nature.2017.21751">It's not just you: science papers are getting harder to read</a> - <strong><em>Nature</em></strong>, 2017. [<a href="https://scholar.google.com/scholar?cluster=4409814498614719804&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Nature perspective on reading scientific papers.</p>
</li>
<li>
<p><a href="https://be.mit.edu/sites/default/files/documents/HowToReadAScientificPaper.pdf">How to navigate a scientific paper with time constraints: a graphics approach</a> - <strong><em>MIT</em></strong>. MIT guidance on strategies for reading papers given different time constraints.</p>
</li>
<li>
<p><a href="https://textvis.lnu.se/">Text Visualization Browser</a> - <strong><em>ISOVIS group</em></strong>, 2015. [<a href="https://cs.lnu.se/isovis/pubs/docs/kucher-pacificvis15-postprint.pdf">Paper</a>]. [<a href="https://scholar.google.com/scholar?cluster=7000995325728444282&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A Hub of Text Visualization Techniques.</p>
</li>
</ul>
<h3 id="literature-management">Literature Management<a class="headerlink" href="#literature-management" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://www.nature.com/articles/nj7612-457a">Scientific literature: Information overload</a> - <strong><em>Nature</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=9898832432826237365&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Perspective on handling overloaded information from scientific literature.</p>
</li>
<li>
<p><a href="https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/">Microsoft Academic Graph</a> - <strong><em>Microsoft Research</em></strong>. Heterogeneous graph containing scientific publication records, citation relationships between those publications, as well as authors, institutions, journals, conferences, and fields of study. </p>
</li>
<li>
<p><a href="http://sonyis.me/paperpdf/Microsoft%20Academic%20Graph%20WWW%202015.pdf">An Overview of Microsoft Academic Service (MAS) and Applications</a> - <strong><em>WWW'15</em></strong>, 2015. [<a href="https://scholar.google.com/scholar?cluster=9075899176667058496&amp;hl=en&amp;as_sdt=0,5">All Versios</a>]. Original paper on Microsoft Academic Graph.</p>
</li>
<li>
<p><a href="https://blogs.lse.ac.uk/impactofsocialsciences/2021/05/27/goodbye-microsoft-academic-hello-open-research-infrastructure/">Goodbye, Microsoft Academic – Hello, open research infrastructure?</a> - <strong><em>LSE Impact Blog</em></strong>, 2021. An interpretation of Microsoft's strategy on research infrastructure.</p>
</li>
<li>
<p><a href="https://www.semanticscholar.org/">Semantic Scholar</a> - <strong><em>Allen Institute for AI Research</em></strong>. AI-powered scientific literature research tool.</p>
</li>
<li>
<p><a href="https://aclanthology.org/N18-3011/">Construction of the Literature Graph in Semantic Scholar</a> - <strong><em>NAACL'18</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=5500969515339734950&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. Semantic Scholar with extracting feature and metadata from raw paper data.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2020.acl-main.447/">S2ORC: The Semantic Scholar Open Research Corpus</a> - <strong><em>ACL'20</em></strong>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11978464475399626925&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. An open corpus of academic papers released by Semantic Scholar.</p>
</li>
<li>
<p><a href="https://www.litmaps.com/">Litmaps</a> - <strong><em>Litmap Ltd</em></strong>. For interactive literature map construction and linked document management.</p>
</li>
<li>
<p><a href="https://www.vosviewer.com/">VOSviewer</a> - <strong><em>Leiden University</em></strong>. For constructing and visualizing bibliometric networks.</p>
</li>
<li>
<p><a href="https://www.stateoftheart.ai/">StateOfTheArt.AI</a> - <strong><em>StateOfTheArtAI</em></strong>. For tracking, collecting and visualizing the development of AI research. </p>
</li>
</ul>
<h3 id="knowledge-management">Knowledge Management<a class="headerlink" href="#knowledge-management" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://www.loc.gov/aba/publications/FreeLCC/freelcc.html">Library of Congress Classification</a> - <strong><em>Library of Congress</em></strong>. Classification system of USA (PDF only).</p>
</li>
<li>
<p><a href="http://cct.nlc.cn/">Chinese Library Classification</a> - <strong><em>National Library of China</em></strong>. Classification system of P. R. China (online user interface in Chinese). [<a href="https://www.isko.org/cyclo/clc">English introduction at ISKO</a>]. [<a href="https://en.wikipedia.org/wiki/Chinese_Library_Classification">Wikipedia-EN</a>].</p>
</li>
<li>
<p><a href="https://rvk.uni-regensburg.de/regensburger-verbundklassifikation-online">DDC at German National Library</a> - <strong><em>Deutsche National Bibliothek</em></strong>. Deway Decimal Classification (DDC) based classification system of Germany (online user interface). [<a href="https://www.dnb.de/EN/Professionell/DDC-Deutsch/DDCinDNB/ddcindnb_node.html">DNB Website</a>].</p>
</li>
<li>
<p><a href="https://www.ndl.go.jp/jp/data/catstandards/classification_subject/ndlc.html">National Dite Library Classification</a> - <strong><em>National Diet Library of Japan</em></strong>. Classification system of Japan (PDF only).</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Knowledge_organization">Knowledge organization</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on knowledge organization methods.</p>
</li>
<li>
<p><a href="https://zettelkasten.de/">The Zettelkasten Method</a> - <strong><em>Bielefeld University</em></strong>. Relating ideas in graphs and multi-labels.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Zettelkasten">Zettelkasten</a> - <strong><em>Wikipedia</em></strong>. Wikipedia on the Zettelkasten method.</p>
</li>
<li>
<p><a href="https://roamresearch.com/">Roam Research</a> - <strong><em>Roam Research</em></strong>. For linked document management, visualization, and sharing. </p>
</li>
<li>
<p><a href="https://foambubble.github.io/foam/">Foam</a> - <strong><em>Foambubble</em></strong>. For linked document management, visualization, and sharing, opensourced softward built on VSCode.</p>
</li>
<li>
<p><a href="https://www.buildingasecondbrain.com/">Building a Second Brain</a> - <strong><em>Forte Labs, LLC</em></strong>. Connecting ideas in graphs.</p>
</li>
<li>
<p><a href="https://www.zotero.org/">Zotero</a> - <strong><em>Digital Scholar</em></strong>. For reference management to manage bibliographic data and research related materials.</p>
</li>
<li>
<p><a href="https://pdfs.semanticscholar.org/88f8/fa9dfbc0c2b296758dd932b871917c5c775a.pdf%C2%A0">Niklas Luhmann's Card Index: Thinking Tool, Communication Partner, Publication Machine</a> - <strong><em>Forgetting Machines: Knowledge Management Evolution in Early Modern Europe, Brill</em></strong>, 2016. [<a href="https://scholar.google.com/scholar?cluster=1786807670077004336&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://www.researchgate.net/profile/Alberto-Cevolini/publication/328624186_Where_Does_Niklas_Luhmann%27s_Card_Index_Come_From/links/609f818e299bf147699a401d/Where-Does-Niklas-Luhmanns-Card-Index-Come-From.pdf">Where Does Niklas Luhmann's Card Index Come From?</a> - <strong><em>Erudition and the Republic of Letters</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8279465066043884141&amp;hl=en&amp;as_sdt=0,5">All Versions</a>]. A simplified introduction on Luhmann's Zettelkasten.</p>
</li>
<li>
<p><a href="https://www.uni-bielefeld.de/fakultaeten/soziologie/forschung/luhmann-archiv/pdf/jschmidt_niklas-luhmanns-card-index_-sociologica_2018_12-1.pdf">Niklas Luhmann's Card Index: The Fabrication of Serendipity</a> - <strong><em>Sociologica</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=12440286698665929622&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://luhmann.surge.sh/communicating-with-slip-boxes">Communicating with Slip Boxes</a> - 2019. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Communicating+with+slip+boxes+luhmann&amp;btnG=">All Versions</a>].</p>
</li>
</ul>
<h2 id="institute-researcher">Institute &amp; Researcher<a class="headerlink" href="#institute-researcher" title="Permanent link">&para;</a></h2>
<h3 id="mit">MIT<a class="headerlink" href="#mit" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://cbmm.mit.edu/">Center for Brains, Minds and Machines (CBMM)</a> - <strong><em>MIT</em></strong>.</p>
</li>
<li>
<p><a href="https://cocosci.mit.edu/josh">Josh Tenenbaum</a> - <strong><em>Department of Brain and Cognitive Sciences, CSAIL, MIT</em></strong>, <a href="https://cocosci.mit.edu/">Computational Cognitive Science Group (CoCoSci Group)</a> - <strong><em>MIT</em></strong>.</p>
</li>
<li>
<p><a href="https://saxelab.mit.edu/people/rebecca-saxe">Rebecca Saxe</a> - <strong><em>Department of Brain and Cognitive Sciences, MIT</em></strong>, <a href="https://saxelab.mit.edu/">Social Cognitive Neuroscience Laboratory (SaxeLab)</a> - <strong><em>MIT</em></strong>.</p>
</li>
<li>
<p><a href="https://cbmm.mit.edu/about/people/schulz">Laura Schulz</a> - <strong><em>Department of Brain and Cognitive Sciences, MIT</em></strong>, <a href="https://eccl.mit.edu/">Early Childhood Cognition Lab</a> - <strong><em>MIT</em></strong>.</p>
</li>
<li>
<p><a href="https://people.csail.mit.edu/lpk/">Leslie Kaelbling</a> - <strong><em>Department of Electrical Engineering and Computer Science, CSAIL, MIT</em></strong>, <a href="https://lis.csail.mit.edu/">The Learning &amp; Intelligent Systems Group</a> - <strong><em>MIT</em></strong>.</p>
</li>
<li>
<p><a href="https://people.csail.mit.edu/asolar/">Armando Solar-Lezama</a> - <strong><em>Department of Electrical Engineering and Computer Science, CSAIL, MIT</em></strong>, <a href="http://groups.csail.mit.edu/cap/">Computer-Aided Programming Group</a> - <strong><em>MIT</em></strong>.</p>
</li>
</ul>
<h3 id="stanford">Stanford<a class="headerlink" href="#stanford" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a> - <strong><em>Computer Science Department, Human-Centered AI Institute, Stanford</em></strong>, <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> - <strong><em>Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://cocolab.stanford.edu/ndg.html">Noah Goodman</a> - <strong><em>Department of Psychology, Computer Science Department, Stanford</em></strong>, <a href="https://cocolab.stanford.edu/">Computation &amp; Cognition Lab (CoCoLab)</a> - <strong><em>Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://web.stanford.edu/~mcfrank/">Michael Frank</a> - <strong><em>Department of Psychology, Stanford</em></strong>, <a href="http://langcog.stanford.edu/">The Stanford Language and Cognition Lab</a> - <strong><em>Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://cicl.stanford.edu/member/tobias_gerstenberg/">Tobias Gerstenberg</a> - <strong><em>Department of Psychology, Stanford</em></strong>, <a href="https://cicl.stanford.edu/">Causality in Cognition Lab (CICL)</a> - <strong><em>Stanford</em></strong>.</p>
</li>
<li>
<p><a href="http://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> - <strong><em>Computer Science Department, Stanford</em></strong>, <a href="https://irislab.stanford.edu/">Intelligence through Robotic Interaction at Scale (IRIS Group)</a> - <strong><em>Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://comm.stanford.edu/faculty-bailenson/">Jeremy Bailenson</a> - <strong><em>Department of Communication, Stanford</em></strong>, <a href="https://stanfordvr.com/">Virtual Human Interaction Lab (VHIL)</a> - <strong><em>Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://jiajunwu.com/">Jiajun Wu</a> - <strong><em>Computer Science Department, Stanford</em></strong>.</p>
</li>
<li>
<p><a href="https://profiles.stanford.edu/judith-fan">Judith Fan</a> - <strong><em>Department of Psychology, Stanford</em></strong>, <a href="https://cogtoolslab.github.io/">Cognitive Tools Lab</a> - <strong><em>Stanford</em></strong>.</p>
</li>
</ul>
<h3 id="princeton">Princeton<a class="headerlink" href="#princeton" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://psych.princeton.edu/person/tania-lombrozo">Tania Lombrozo</a> - <strong><em>Department of Psychology, Princeton</em></strong>, <a href="https://cognition.princeton.edu/">Concepts &amp; Cognition Lab</a> - <strong><em>Princeton</em></strong>.</p>
</li>
<li>
<p><a href="https://cocosci.princeton.edu/tom/index.php">Thomas Griffiths</a> - <strong><em>Department of Psychology, Department of Computer Science, Princeton</em></strong>, <a href="https://cocosci.princeton.edu/index.php">Computational Cognitive Science Lab</a> - <strong><em>Princeton</em></strong>.</p>
</li>
</ul>
<h3 id="harvard">Harvard<a class="headerlink" href="#harvard" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://psychology.fas.harvard.edu/people/elizabeth-s-spelke">Elizabeth Spelke</a> - <strong><em>Department of Psychology, Harvard</em></strong>, <a href="https://www.harvardlds.org/">Harvard Laboratory for Developmental Studies</a> - <strong><em>Harvard</em></strong>.</p>
</li>
<li>
<p><a href="https://www.tomerullman.org/">Tomer Ullman</a> - <strong><em>Department of Psychology, Harvard</em></strong>, <a href="https://cocodev.fas.harvard.edu/">Computation, Cognition, and Development Lab (CoCoDev)</a> - <strong><em>Harvard</em></strong>.</p>
</li>
<li>
<p><a href="https://psychology.fas.harvard.edu/people/samuel-j-gershman">Samuel Gershman</a> - <strong><em>Department of Psychology, Harvard</em></strong>, <a href="https://gershmanlab.com/">Computational Cognitive Neuroscience Lab (CCN Lab)</a> - <strong><em>Harvard</em></strong>.</p>
</li>
<li>
<p><a href="https://psychology.fas.harvard.edu/people/fiery-cushman">Fiery Cushman</a> - <strong><em>Department of Psychology, Harvard</em></strong>, <a href="https://cushmanlab.fas.harvard.edu/">Moral Psychology Research Lab</a> - <strong><em>Harvard</em></strong>.</p>
</li>
</ul>
<h3 id="ucla">UCLA<a class="headerlink" href="#ucla" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="http://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning and Autonomy (VCLA)</a> - <strong><em>Department of Statistics, UCLA</em></strong>.</p>
</li>
<li>
<p><a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> - <strong><em>Department of Statistics, UCLA</em></strong>.</p>
</li>
<li>
<p><a href="http://www.stat.ucla.edu/~taogao/Taogao.html">Tao Gao</a> - <strong><em>Department of Statistics, Department of Psychology, UCLA</em></strong>, <a href="http://www.stat.ucla.edu/~taogao/index.html">Visual Intelligence Lab</a> - <strong><em>UCLA</em></strong>.</p>
</li>
<li>
<p><a href="https://www.psych.ucla.edu/faculty/page/hongjing">Hongjing Lu</a> - <strong><em>Department of Psychology, Department of Statistics, UCLA</em></strong>, <a href="http://cvl.psych.ucla.edu/">Computational Vision and Learning Lab (CVL)</a> - <strong><em>UCLA</em></strong>.</p>
</li>
<li>
<p><a href="http://web.cs.ucla.edu/~guyvdb/">Guy Van den Broeck</a> - <strong><em>Department of Computer Science, UCLA</em></strong>, <a href="http://starai.cs.ucla.edu/#">StarAI Lab</a> - <strong><em>UCLA</em></strong>.</p>
</li>
</ul>
<h3 id="uc-berkeley">UC Berkeley<a class="headerlink" href="#uc-berkeley" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://people.eecs.berkeley.edu/~anca/index.html">Anca Dragan</a> - <strong><em>Department of Electrical Engineering and Computer Science, UC Berkeley</em></strong>, <a href="http://interact.berkeley.edu/">Interactive Autonomy and Collaborative Technologies Laboratory (InterACT)</a> - <strong><em>UC Berkeley</em></strong>.</p>
</li>
<li>
<p><a href="https://psychology.berkeley.edu/people/fei-xu">Fei Xu</a> - <strong><em>Department of Psychology, UC Berkeley</em></strong>, <a href="https://babylab5.wixsite.com/bell">Berkeley Early Learning Lab (Xu Lab)</a> - <strong><em>UC Berkeley</em></strong>.</p>
</li>
<li>
<p><a href="http://alisongopnik.com/">Alison Gopnik</a> - <strong><em>Department of Psychology, UC Berkeley</em></strong>, <a href="http://www.gopniklab.berkeley.edu/">Cognitive Development &amp; Learning Lab (Gopnik Lab)</a> - <strong><em>UC Berkeley</em></strong>.</p>
</li>
<li>
<p><a href="http://colala.berkeley.edu/people/piantadosi/">Steve Piantadosi</a> - <strong><em>Department of Psychology, UC Berkeley</em></strong>, <a href="http://colala.berkeley.edu/">The computation and language lab (colala)</a> - <strong><em>UC Berkeley</em></strong>.</p>
</li>
<li>
<p><a href="http://www.celestekidd.com/">Celeste Kidd</a> - <strong><em>Department of Psychology, UC Berkeley</em></strong>, <a href="https://www.kiddlab.com/">Kidd Lab</a> - <strong><em>UC Berkeley</em></strong>.</p>
</li>
</ul>
<h3 id="bnu">BNU<a class="headerlink" href="#bnu" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://brain.bnu.edu.cn/English/Faculty/CurrentFaculty/Bzz/a552402e529a4f27b979378abd42c10e.htm">Yanchao Bi</a> - <strong><em>IDG/McGovern Institute for Brain Research and the State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University (BNU)</em></strong>, <a href="http://bilab.bnu.edu.cn/">Yanchao Bi's Concept Lab (Bi Lab)</a> - <strong><em>BNU</em></strong>.</li>
</ul>
<h3 id="pku">PKU<a class="headerlink" href="#pku" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://zhusongchun.net/">Song-Chun Zhu</a> - <strong><em>School of AI and Institute for AI, Peking University (PKU)</em></strong>.</p>
</li>
<li>
<p><a href="https://yzhu.io/">Yixin Zhu</a> - <strong><em>School of AI and Institute for AI, Peking University (PKU)</em></strong>, <a href="https://pku.ai/">Cognitive Reasoning Lab (CoRe Lab)</a> - <strong><em>PKU</em></strong>.</p>
</li>
</ul>
<h3 id="ucsd">UCSD<a class="headerlink" href="#ucsd" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://pages.ucsd.edu/~ztu/">Zhuowen Tu</a> - <strong><em>Department of Computer Science, UCSD</em></strong>, <a href="https://pages.ucsd.edu/~ztu/Group.htm">Machine Learning, Perception, and Cognition Lab (mlPC)</a> - <strong><em>UCSD</em></strong>.</p>
</li>
<li>
<p><a href="https://psychology.ucsd.edu/people/profiles/evul.html">Ed Vul</a> - <strong><em>Department of Psychology, UCSD</em></strong>, <a href="http://www.evullab.org/index.html">Computational Cognition Lab</a> - <strong><em>UCSD</em></strong>.</p>
</li>
</ul>
<h3 id="nyu">NYU<a class="headerlink" href="#nyu" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><a href="https://cs.nyu.edu/~davise/">Ernest Davis</a> - <strong><em>Department of Computer Science, Courant Institute of Mathematical Sciences, NYU</em></strong>.</p>
</li>
<li>
<p><a href="http://garymarcus.com/index.html">Gary Marcus</a> - <strong><em>Department of Psychology, NYU</em></strong>.</p>
</li>
<li>
<p><a href="https://cims.nyu.edu/~brenden/">Brenden Lake</a> - <strong><em>Department of Psychology, NYU</em></strong>, <a href="https://lake-lab.github.io/">Human &amp; Machine Learning Lab (Lake Lab)</a> - <strong><em>NYU</em></strong>.</p>
</li>
<li>
<p><a href="https://as.nyu.edu/faculty/todd-gureckis.html">Todd Gureckis</a> - <strong><em>Department of Psychology, NYU</em></strong>, <a href="http://gureckislab.org/">Computation &amp; Cognition Lab</a> - <strong><em>NYU</em></strong>.</p>
</li>
<li>
<p><a href="http://www.cns.nyu.edu/malab/people.html">Wei Ji Ma</a> - <strong><em>Department of Psychology, Center for Neural Science, NYU</em></strong>, <a href="http://www.cns.nyu.edu/malab/">Wei Ji Ma Lab</a> - <strong><em>NYU</em></strong>.</p>
</li>
</ul>
<h3 id="jhu">JHU<a class="headerlink" href="#jhu" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://perception.jhu.edu/chaz/">Chaz Firestone</a> - <strong><em>Department of Psychological and Brain Sciences, Johns Hopkins University (JHU)</em></strong>, <a href="https://perception.jhu.edu/">Hopkins Perception &amp; Mind Lab</a> - <strong><em>JHU</em></strong>.</li>
</ul>
<h3 id="sit">SIT<a class="headerlink" href="#sit" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://markkho.github.io/">Mark Ho</a> - <strong><em>Department of Computer Science, Stevens Institute of Technology (SIT)</em></strong>, <a href="https://codec-lab.github.io/">Computation and Decision-Making Lab</a> - <strong><em>SIT</em></strong>.</li>
</ul>
<h2 id="people-book">People &amp; Book<a class="headerlink" href="#people-book" title="Permanent link">&para;</a></h2>
<h3 id="john-hopcroft">John Hopcroft<a class="headerlink" href="#john-hopcroft" title="Permanent link">&para;</a></h3>
<p>Theoretical computer scientist.</p>
<ul>
<li>
<p><a href="http://elib.vku.udn.vn/bitstream/123456789/2543/1/2007.%20Introduction%20to%20Automata%20Theory%2C%20Languages%2C%20and%20Computations%20%283rd%20edition%29.pdf">Introduction to Automata Theory, Languages, and Computation</a> - <strong><em>Pearson</em></strong>, 2007. [<a href="https://scholar.google.com/scholar?cluster=326269839585842480">All Versions</a>].</p>
</li>
<li>
<p><a href="http://www.cs.cornell.edu/jeh/book%20no%20so;utions%20March%202019.pdf">Foundations of Data Science</a> - <strong><em>Cambridge University Press</em></strong>. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1802704438630899850">All Versions</a>].</p>
</li>
</ul>
<h3 id="ulf-grenander">Ulf Grenander<a class="headerlink" href="#ulf-grenander" title="Permanent link">&para;</a></h3>
<p>Applied mathematician, the founder of General Pattern Theory.</p>
<ul>
<li>
<p><a href="https://www.dam.brown.edu/ptg/REPORTS/calculustext.PDF">A Calculus of Ideas: A Mathematical Study of Thinking</a> - <strong><em>World Scientific Publishing Company</em></strong>, 2012. [<a href="https://scholar.google.com/scholar?cluster=12182416000849265255&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://global.oup.com/academic/product/general-pattern-theory-9780198536710?cc=lt&amp;lang=de#">General Pattern Theory: A Mathematical Study of Regular Structures</a> - <strong><em>Oxford University Press</em></strong>, 1993. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=General+Pattern+Theory&amp;btnG=">All Versions</a>].</p>
</li>
</ul>
<h3 id="david-marr">David Marr<a class="headerlink" href="#david-marr" title="Permanent link">&para;</a></h3>
<p>Computational Cognitive Neuroscientist, the establisher of the Levels of Analysis.</p>
<ul>
<li><a href="https://usa1lib.org/book/1223444/8e5ca8">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</a> - <strong><em>MIT Press</em></strong>, 1982. [<a href="https://scholar.google.com/scholar?cluster=14386368570811483142&amp;hl=en&amp;as_sdt=0,44">All Versions</a>].</li>
</ul>
<h3 id="michael-tomasello">Michael Tomasello<a class="headerlink" href="#michael-tomasello" title="Permanent link">&para;</a></h3>
<p>Cognitive scientist, set up the foundations of studying human communications. </p>
<ul>
<li>
<p><a href="https://1lib.net/book/541274/39859f">Origins of human communication</a> - <strong><em>MIT Press</em></strong>, 2010. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=2553369883266458474">All Versions</a>].</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/541275/1452f8?id=541275&amp;secret=1452f8">The cultural origins of human cognition</a> - <strong><em>Havard University Press</em></strong>, 2000. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=5000469061641945144">All Versions</a>].</p>
</li>
</ul>
<h3 id="judea-pearl">Judea Pearl<a class="headerlink" href="#judea-pearl" title="Permanent link">&para;</a></h3>
<p>Applied mathematician, proposed causal intervention on siamese bayesian networks. </p>
<ul>
<li>
<p><a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why: The New Science of Cause and Effect</a> - <strong><em>Basic Books</em></strong>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2505901292485349932&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/2780725/2ec8f1?id=2780725&amp;secret=2ec8f1">Causality: Models, Reasoning and Inference</a> - <strong><em>Cambridge University Press</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=10996260119229499611&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1">All Versions</a>].</p>
</li>
</ul>
<h3 id="susan-carey">Susan Carey<a class="headerlink" href="#susan-carey" title="Permanent link">&para;</a></h3>
<p>Developmental psychologist, proposed <em>object</em> as a core knowledge of human intelligence. </p>
<ul>
<li>
<p><a href="https://hk1lib.org/book/844457/42178f?id=844457&amp;secret=42178f">The Origin of Concepts</a> - <strong><em>Oxford University Press</em></strong>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11493102398422813821&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/3659332/11fa44">Conceptual Change in Childhood</a> - <strong><em>MIT Press</em></strong>, 1985. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=conceptual+change+in+childhood+susan+carey&amp;btnG=">All Versions</a>].</p>
</li>
</ul>
<h3 id="daniel-kahneman">Daniel Kahneman<a class="headerlink" href="#daniel-kahneman" title="Permanent link">&para;</a></h3>
<p>Computational cognitive scientist and Economist, set up the foundations for Decision Theory.</p>
<ul>
<li><a href="https://hk1lib.org/book/2181569/f5e85a?id=2181569&amp;secret=f5e85a">Thinking, fast and slow</a> - <strong><em>Farrar Straus Giroux</em></strong>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=3255681708785115121">All Versions</a>].</li>
</ul>
<h3 id="karl-popper">Karl Popper<a class="headerlink" href="#karl-popper" title="Permanent link">&para;</a></h3>
<p>Scientific philosophor, the founder of scientific verification theories.</p>
<ul>
<li>
<p><a href="https://hk1lib.org/book/511214/299596">The logic of scientific discovery</a> - <strong><em>Routledge</em></strong>, 2005. [<a href="https://scholar.google.com/scholar?cluster=5836864564733788424&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
<li>
<p><a href="https://hk1lib.org/book/2773070/c48f60">All Life is Problem Solving</a> - <strong><em>Routledge</em></strong>, 2001. [<a href="https://scholar.google.com/scholar?cluster=9799073870888093350&amp;hl=en&amp;as_sdt=0,5">All Versions</a>].</p>
</li>
</ul>
<h2 id="about">About<a class="headerlink" href="#about" title="Permanent link">&para;</a></h2>
<p>The initiator of this repo has been struggling to taxonomize related topics, since there are so many perspectives to follow, such as task-oriented, technique-oriented, and metaphysics-oriented. Finally he decided to focus on the perspective of <strong><em>The Sciences of Intelligence</em></strong>---each topic describes a phenomenon of intelligence, or an intelligent behavior---they show the objectives of reverse-engineering human intelligence for computational methods. These topics are never restricted to specific technical methods or tasks, but are trying to organize the nature of intelligence---from both <em>the software perspective</em> and <em>the hardware perspective</em>.</p>
<p>Obviously, this reading list is far from covering the every aspect of AGI and CoCoSci. Since the list is a by-product of the literature reviews when the initiator is working on Abduction and Bayesian modeling, other topics are also collected with biases, more or less. Abduction may be the way humans explain the world with the known, and discover the unknown, requiring much more investigations into its computational basis, cognitive underpinnings, and applications to AI. Please feel free to reach out!</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2016 - 2025
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/pangu/3.3.0/pangu.min.js"></script>
      
        <script src="../../_static/js/extra.js"></script>
      
        <script src="https://cdn.staticfile.org/jquery/3.3.1/jquery.min.js"></script>
      
        <script src="https://cdn.staticfile.org/github-repo-widget/e23d85ab8f/jquery.githubRepoWidget.min.js"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6890694312814945"></script>
      
    
  </body>
</html>
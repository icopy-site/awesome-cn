<div class="github-widget" data-repo="n2cholas/awesome-jax"></div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-6890694312814945" data-ad-slot="5473692530" data-ad-format="auto"  data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script>
<!--lint ignore double-link-->

<!--lint ignore double-link-->
[JAX](https://github.com/google/jax) å¸¦æ¥è‡ªåŠ¨å¾®åˆ†å’Œ [XLA compiler](https://www.tensorflow.org/xla) ä¸€èµ·é€šè¿‡ä¸€ä¸ª [NumPy](https://numpy.org/)ç±»ä¼¼ APIï¼Œç”¨äºå¯¹ GPU å’Œ TPU ç­‰åŠ é€Ÿå™¨è¿›è¡Œé«˜æ€§èƒ½æœºå™¨å­¦ä¹ ç ”ç©¶.
<!--lint enable double-link-->

è¿™æ˜¯å¾ˆæ£’çš„ JAX åº“ã€é¡¹ç›®å’Œå…¶ä»–èµ„æºçš„ç²¾é€‰åˆ—è¡¨. æ¬¢è¿æŠ•ç¨¿ï¼



<a name="libraries" />

## Libraries

- ç¥ç»ç½‘ç»œåº“
    - [Flax](https://raw.githubusercontent.com/google/flax) - ä»¥çµæ´»æ€§å’Œæ¸…æ™°åº¦ä¸ºä¸­å¿ƒ. <img src="https://img.shields.io/github/stars/google/flax?style=social" align="center">
    - [Haiku](https://raw.githubusercontent.com/deepmind/dm-haiku) - ä¸“æ³¨äºç®€å•æ€§ï¼Œç”± DeepMind çš„ Sonnet çš„ä½œè€…åˆ›å»º. <img src="https://img.shields.io/github/stars/deepmind/dm-haiku?style=social" align="center">
    - [Objax](https://raw.githubusercontent.com/google/objax) - å…·æœ‰ç±»ä¼¼äº PyTorch çš„é¢å‘å¯¹è±¡è®¾è®¡. <img src="https://img.shields.io/github/stars/google/objax?style=social" align="center">
    - [Elegy](https://poets-ai.github.io/elegy/)  - JAX ä¸­æ·±åº¦å­¦ä¹ çš„é«˜çº§ API. æ”¯æŒ Flaxã€Haiku å’Œ Optax. <img src="https://img.shields.io/github/stars/poets-ai/elegy?style=social" align="center">
    - [Trax](https://raw.githubusercontent.com/google/trax) - â€œå«ç”µæ± â€æ·±åº¦å­¦ä¹ åº“ï¼Œä¸“æ³¨äºä¸ºå¸¸è§å·¥ä½œè´Ÿè½½æä¾›è§£å†³æ–¹æ¡ˆ. <img src="https://img.shields.io/github/stars/google/trax?style=social" align="center">
    - [Jraph](https://raw.githubusercontent.com/deepmind/jraph) - è½»é‡çº§å›¾ç¥ç»ç½‘ç»œåº“. <img src="https://img.shields.io/github/stars/deepmind/jraph?style=social" align="center">
    - [Neural Tangents](https://raw.githubusercontent.com/google/neural-tangents) - ç”¨äºæŒ‡å®šæœ‰é™å’Œ _infinite_ å®½åº¦çš„ç¥ç»ç½‘ç»œçš„é«˜çº§ API. <img src="https://img.shields.io/github/stars/google/neural-tangents?style=social" align="center">
    - [HuggingFace](https://raw.githubusercontent.com/huggingface/transformers) - ç”¨äºå„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ (Flax) çš„é¢„è®­ç»ƒ Transformer ç”Ÿæ€ç³»ç»Ÿ. <img src="https://img.shields.io/github/stars/huggingface/transformers?style=social" align="center">
    - [Equinox](https://raw.githubusercontent.com/patrick-kidger/equinox) - å¯è°ƒç”¨çš„ PyTrees å’Œè¿‡æ»¤çš„ JIT/grad è½¬æ¢ =&gt; JAX ä¸­çš„ç¥ç»ç½‘ç»œ. <img src="https://img.shields.io/github/stars/patrick-kidger/equinox?style=social" align="center">
- [NumPyro](https://raw.githubusercontent.com/pyro-ppl/numpyro) - åŸºäº Pyro åº“çš„æ¦‚ç‡ç¼–ç¨‹. <img src="https://img.shields.io/github/stars/pyro-ppl/numpyro?style=social" align="center">
- [Chex](https://raw.githubusercontent.com/deepmind/chex) - ç”¨äºç¼–å†™å’Œæµ‹è¯•å¯é  JAX ä»£ç çš„å®ç”¨ç¨‹åº. <img src="https://img.shields.io/github/stars/deepmind/chex?style=social" align="center">
- [Optax](https://raw.githubusercontent.com/deepmind/optax) - æ¢¯åº¦å¤„ç†å’Œä¼˜åŒ–åº“. <img src="https://img.shields.io/github/stars/deepmind/optax?style=social" align="center">
- [RLax](https://raw.githubusercontent.com/deepmind/rlax) - ç”¨äºå®ç°å¼ºåŒ–å­¦ä¹ ä»£ç†çš„åº“. <img src="https://img.shields.io/github/stars/deepmind/rlax?style=social" align="center">
- [JAX, M.D.](https://raw.githubusercontent.com/google/jax-md) - åŠ é€Ÿçš„å¾®åˆ†åˆ†å­åŠ¨åŠ›å­¦. <img src="https://img.shields.io/github/stars/google/jax-md?style=social" align="center">
- [Coax](https://raw.githubusercontent.com/coax-dev/coax) - å°† RL è®ºæ–‡è½¬åŒ–ä¸ºä»£ç ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•. <img src="https://img.shields.io/github/stars/coax-dev/coax?style=social" align="center">
- [SymJAX](https://raw.githubusercontent.com/SymJAX/SymJAX) - ç¬¦å· CPU/GPU/TPU ç¼–ç¨‹. <img src="https://img.shields.io/github/stars/SymJAX/SymJAX?style=social" align="center">
- [mcx](https://raw.githubusercontent.com/rlouf/mcx) - è¡¨è¾¾å’Œç¼–è¯‘ç”¨äºé«˜æ€§èƒ½æ¨ç†çš„æ¦‚ç‡ç¨‹åº. <img src="https://img.shields.io/github/stars/rlouf/mcx?style=social" align="center">
- [Distrax](https://raw.githubusercontent.com/deepmind/distrax) - é‡æ–°å®ç° TensorFlow Probabilityï¼ŒåŒ…å«æ¦‚ç‡åˆ†å¸ƒå’ŒåŒå°„å™¨. <img src="https://img.shields.io/github/stars/deepmind/distrax?style=social" align="center">
- [cvxpylayers](https://raw.githubusercontent.com/cvxgrp/cvxpylayers) - æ„å»ºå¯å¾®å‡¸ä¼˜åŒ–å±‚. <img src="https://img.shields.io/github/stars/cvxgrp/cvxpylayers?style=social" align="center">
- [TensorLy](https://raw.githubusercontent.com/tensorly/tensorly) - å¼ é‡å­¦ä¹ å˜å¾—ç®€å•. <img src="https://img.shields.io/github/stars/tensorly/tensorly?style=social" align="center">
- [NetKet](https://raw.githubusercontent.com/netket/netket) - ç”¨äºé‡å­ç‰©ç†å­¦çš„æœºå™¨å­¦ä¹ å·¥å…·ç®±. <img src="https://img.shields.io/github/stars/netket/netket?style=social" align="center">

<a name="new-libraries" />

### New Libraries

æœ¬èŠ‚åŒ…å«åˆ¶ä½œç²¾è‰¯ä¸”æœ‰ç”¨çš„åº“ï¼Œä½†å°šæœªç»è¿‡å¤§é‡ç”¨æˆ·ç¾¤çš„å®æˆ˜æµ‹è¯•.

- ç¥ç»ç½‘ç»œåº“
    - [FedJAX](https://raw.githubusercontent.com/google/fedjax) - JAX ä¸­çš„è”åˆå­¦ä¹ ï¼Œå»ºç«‹åœ¨ Optax å’Œ Haiku ä¹‹ä¸Š. <img src="https://img.shields.io/github/stars/google/fedjax?style=social" align="center">
    - [Equivariant MLP](https://raw.githubusercontent.com/mfinzi/equivariant-MLP) - æ„å»ºç­‰å˜ç¥ç»ç½‘ç»œå±‚. <img src="https://img.shields.io/github/stars/mfinzi/equivariant-MLP?style=social" align="center">
    - [jax-resnet](https://raw.githubusercontent.com/n2cholas/jax-resnet/) - Flax ä¸­ ResNet å˜ä½“çš„å®ç°å’Œæ£€æŸ¥ç‚¹. <img src="https://img.shields.io/github/stars/n2cholas/jax-resnet?style=social" align="center">
- [jax-unirep](https://raw.githubusercontent.com/ElArkk/jax-unirep) - å›¾ä¹¦é¦†å®æ–½ [UniRep model](https://www.nature.com/articles/s41592-019-0598-1) ç”¨äºè›‹ç™½è´¨æœºå™¨å­¦ä¹ åº”ç”¨. <img src="https://img.shields.io/github/stars/ElArkk/jax-unirep?style=social" align="center">
- [jax-flows](https://raw.githubusercontent.com/ChrisWaites/jax-flows) - åœ¨ JAX ä¸­è§„èŒƒåŒ–æµç¨‹. <img src="https://img.shields.io/github/stars/ChrisWaites/jax-flows?style=social" align="center">
- [sklearn-jax-kernels](https://raw.githubusercontent.com/ExpectationMax/sklearn-jax-kernels) - ä½¿ç”¨ JAX çš„ `scikit-learn` å†…æ ¸çŸ©é˜µ. <img src="https://img.shields.io/github/stars/ExpectationMax/sklearn-jax-kernels?style=social" align="center">
- [jax-cosmo](https://raw.githubusercontent.com/DifferentiableUniverseInitiative/jax_cosmo) - å¯å¾®çš„å®‡å®™å­¦å›¾ä¹¦é¦†. <img src="https://img.shields.io/github/stars/DifferentiableUniverseInitiative/jax_cosmo?style=social" align="center">
- [efax](https://raw.githubusercontent.com/NeilGirdhar/efax) - JAX ä¸­çš„æŒ‡æ•°æ—. <img src="https://img.shields.io/github/stars/NeilGirdhar/efax?style=social" align="center">
- [mpi4jax](https://raw.githubusercontent.com/PhilipVinc/mpi4jax) - å°† MPI æ“ä½œä¸ CPU å’Œ GPU ä¸Šçš„ Jax ä»£ç ç›¸ç»“åˆ. <img src="https://img.shields.io/github/stars/PhilipVinc/mpi4jax?style=social" align="center">
- [imax](https://raw.githubusercontent.com/4rtemi5/imax) - å›¾åƒå¢åŠ å’Œè½¬æ¢. <img src="https://img.shields.io/github/stars/4rtemi5/imax?style=social" align="center">
- [FlaxVision](https://raw.githubusercontent.com/rolandgvc/flaxvision) - äºšéº»ç‰ˆ TorchVision. <img src="https://img.shields.io/github/stars/rolandgvc/flaxvision?style=social" align="center">
- [Oryx](https://github.com/tensorflow/probability/tree/master/spinoffs/oryx) - åŸºäºç¨‹åºè½¬æ¢çš„æ¦‚ç‡ç¼–ç¨‹è¯­è¨€.
- [Optimal Transport Tools](https://github.com/google-research/ott) - æ†ç»‘å®ç”¨ç¨‹åºä»¥è§£å†³æœ€ä½³è¿è¾“é—®é¢˜çš„å·¥å…·ç®±.
- [delta PV](https://raw.githubusercontent.com/romanodev/deltapv) - å…·æœ‰è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½çš„å…‰ä¼æ¨¡æ‹Ÿå™¨. <img src="https://img.shields.io/github/stars/romanodev/deltapv?style=social" align="center">
- [jaxlie](https://raw.githubusercontent.com/brentyi/jaxlie) - ç”¨äºåˆšä½“å˜æ¢å’Œä¼˜åŒ–çš„æç†è®ºåº“. <img src="https://img.shields.io/github/stars/brentyi/jaxlie?style=social" align="center">
- [BRAX](https://raw.githubusercontent.com/google/brax) - å¯å¾®åˆ†ç‰©ç†å¼•æ“æ¥æ¨¡æ‹Ÿç¯å¢ƒä»¥åŠå­¦ä¹ ç®—æ³•æ¥ä¸ºè¿™äº›ç¯å¢ƒè®­ç»ƒä»£ç†. <img src="https://img.shields.io/github/stars/google/brax?style=social" align="center">
- [flaxmodels](https://raw.githubusercontent.com/matthias-wright/flaxmodels) - Jax/Flax çš„é¢„è®­ç»ƒæ¨¡å‹. <img src="https://img.shields.io/github/stars/matthias-wright/flaxmodels?style=social" align="center">
- [CR.Sparse](https://raw.githubusercontent.com/carnotresearch/cr-sparse) - ç”¨äºç¨€ç–è¡¨ç¤ºå’Œå‹ç¼©æ„ŸçŸ¥çš„ XLA åŠ é€Ÿç®—æ³•. <img src="https://img.shields.io/github/stars/carnotresearch/cr-sparse?style=social" align="center">
- [exojax](https://raw.githubusercontent.com/HajimeKawahara/exojax) - ä¸ JAX å…¼å®¹çš„ç³»å¤–è¡Œæ˜Ÿ/æ£•çŸ®æ˜Ÿçš„è‡ªåŠ¨å¯å¾®è°±å»ºæ¨¡. <img src="https://img.shields.io/github/stars/HajimeKawahara/exojax?style=social" align="center">
- [JAXopt](https://raw.githubusercontent.com/google/jaxopt) - JAX ä¸­çš„ç¡¬ä»¶åŠ é€Ÿ (GPU/TPU)ã€å¯æ‰¹å¤„ç†å’Œå¯å¾®ä¼˜åŒ–å™¨. <img src="https://img.shields.io/github/stars/google/jaxopt?style=social" align="center">
- [PIX](https://raw.githubusercontent.com/deepmind/dm_pix) - PIX æ˜¯ JAX ä¸­çš„å›¾åƒå¤„ç†åº“ï¼Œç”¨äº JAX. <img src="https://img.shields.io/github/stars/deepmind/dm_pix?style=social" align="center">
- [bayex](https://raw.githubusercontent.com/alonfnt/bayex) - ç”± JAX æä¾›æ”¯æŒçš„è´å¶æ–¯ä¼˜åŒ–. <img src="https://img.shields.io/github/stars/alonfnt/bayex?style=social" align="center">
- [JaxDF](https://raw.githubusercontent.com/ucl-bug/jaxdf) - å…·æœ‰ä»»æ„ç¦»æ•£åŒ–çš„å¯å¾®æ¨¡æ‹Ÿå™¨æ¡†æ¶. <img src="https://img.shields.io/github/stars/ucl-bug/jaxdf?style=social" align="center">
- [tree-math](https://github.com/google/tree-math) - å°†åœ¨æ•°ç»„ä¸Šè¿è¡Œçš„å‡½æ•°è½¬æ¢ä¸ºåœ¨ PyTrees ä¸Šè¿è¡Œçš„å‡½æ•°.

<a name="models-and-projects" />

## Models and Projects

### JAX

- [Fourier Feature Networks](https://github.com/tancik/fourier-feature-networks) - æ­£å¼å®æ–½ [_Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains_](https://people.eecs.berkeley.edu/~bmild/fourfeat).
- [kalman-jax](https://github.com/AaltoML/kalman-jax) - ä½¿ç”¨è¿­ä»£å¡å°”æ›¼æ»¤æ³¢å’Œå¹³æ»‘å¯¹é©¬å°”å¯å¤«ï¼ˆå³æ—¶é—´ï¼‰é«˜æ–¯è¿‡ç¨‹è¿›è¡Œè¿‘ä¼¼æ¨æ–­.
- [GPJax](https://github.com/thomaspinder/GPJax) - JAX ä¸­çš„é«˜æ–¯è¿‡ç¨‹.
- [jaxns](https://github.com/Joshuaalbert/jaxns) - JAX ä¸­çš„åµŒå¥—é‡‡æ ·.
- [Amortized Bayesian Optimization](https://github.com/google-research/google-research/tree/master/amortized_bo) - ç›¸å…³ä»£ç  [_Amortized Bayesian Optimization over Discrete Spaces_](http://www.auai.org/uai2020/proceedings/329_main_paper.pdf).
- [Accurate Quantized Training](https://github.com/google-research/google-research/tree/master/aqt) - ç”¨äºåœ¨ JAX å’Œ Flax ä¸­è¿è¡Œå’Œåˆ†æç¥ç»ç½‘ç»œé‡åŒ–å®éªŒçš„å·¥å…·å’Œåº“.
- [BNN-HMC](https://github.com/google-research/google-research/tree/master/bnn_hmc) - è®ºæ–‡çš„å®ç° [_What Are Bayesian Neural Network Posteriors Really Like?_](https://arxiv.org/abs/2104.14421).
- [JAX-DFT](https://github.com/google-research/google-research/tree/master/jax_dft) - JAX ä¸­çš„ä¸€ç»´å¯†åº¦æ³›å‡½ç†è®º (DFT)ï¼Œå®ç°äº† [_Kohn-Sham equations as regularizer: building prior knowledge into machine-learned physics_](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.036401).
- [Robust Loss](https://github.com/google-research/google-research/tree/master/robust_loss_jax) - è®ºæ–‡å‚è€ƒä»£ç  [_A General and Adaptive Robust Loss Function_](https://arxiv.org/abs/1701.03077).

### Flax

- [Performer](https://github.com/google-research/google-research/tree/master/performer/fast_attention/jax) - æ‰§è¡Œè€…ï¼ˆé€šè¿‡ FAVOR+ çš„çº¿æ€§å˜æ¢å™¨ï¼‰æ¶æ„çš„äºšéº»å®ç°.
- [JaxNeRF](https://github.com/google-research/google-research/tree/master/jaxnerf) - å®æ–½ [_NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis_](http://www.matthewtancik.com/nerf) å…·æœ‰å¤šè®¾å¤‡ GPU/TPU æ”¯æŒ.
- [Big Transfer (BiT)](https://github.com/google-research/big_transfer) - å®æ–½ [_Big Transfer (BiT): General Visual Representation Learning_](https://arxiv.org/abs/1912.11370).
- [JAX RL](https://github.com/ikostrikov/jax-rl) - å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®ç°.
- [gMLP](https://github.com/SauravMaheshkar/gMLP) - å®æ–½ [_Pay Attention to MLPs_](https://arxiv.org/abs/2105.08050).
- [MLP Mixer](https://github.com/SauravMaheshkar/MLP-Mixer) - æœ€å°çš„å®ç° [_MLP-Mixer: An all-MLP Architecture for Vision_](https://arxiv.org/abs/2105.01601).
- [Distributed Shampoo](https://github.com/google-research/google-research/tree/master/scalable_shampoo) - å®æ–½ [_Second Order Optimization Made Practical_](https://arxiv.org/abs/2002.09018).
- [NesT](https://github.com/google-research/nested-transformer) - æ­£å¼å®æ–½ [_Aggregating Nested Transformers_](https://arxiv.org/abs/2105.12723).
- [XMC-GAN](https://github.com/google-research/xmcgan_image_generation) - æ­£å¼å®æ–½ [_Cross-Modal Contrastive Learning for Text-to-Image Generation_](https://arxiv.org/abs/2101.04702).
- [FNet](https://github.com/google-research/google-research/tree/master/f_net) - æ­£å¼å®æ–½ [_FNet: Mixing Tokens with Fourier Transforms_](https://arxiv.org/abs/2105.03824).
- [GFSA](https://github.com/google-research/google-research/tree/master/gfsa) - æ­£å¼å®æ–½ [_Learning Graph Structure With A Finite-State Automaton Layer_](https://arxiv.org/abs/2007.04929).
- [IPA-GNN](https://github.com/google-research/google-research/tree/master/ipagnn) - æ­£å¼å®æ–½ [_Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks_](https://arxiv.org/abs/2010.12621).
- [Flax Models](https://github.com/google-research/google-research/tree/master/flax_models) - åœ¨ Flax ä¸­å®ç°çš„æ¨¡å‹å’Œæ–¹æ³•çš„é›†åˆ.
- [Protein LM](https://github.com/google-research/google-research/tree/master/protein_lm) - ä¸ºè›‹ç™½è´¨å®æ–½ BERT å’Œè‡ªå›å½’æ¨¡å‹ï¼Œå¦‚ä¸­æ‰€è¿° [_Biological Structure å’Œ Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences_](https://www.biorxiv.org/content/10.1101/622803v1.full) å’Œ [_ProGen: Language Modeling for Protein Generation_](https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2).
- [Slot Attention](https://github.com/google-research/google-research/tree/master/ptopk_patch_selection) - å‚è€ƒå®ç° [_Differentiable Patch Selection for Image Recognition_](https://arxiv.org/abs/2104.03059).
- [Vision Transformer](https://github.com/google-research/vision_transformer) - æ­£å¼å®æ–½ [_An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_](https://arxiv.org/abs/2010.11929).
- [FID computation](https://github.com/matthias-wright/jax-fid) - æ¸¯å£ [mseitzer/pytorch-fid](https://github.com/mseitzer/pytorch-fid) åˆ°äºšéº».

### Haiku

- [AlphaFold](https://github.com/deepmind/alphafold) - AlphaFold v2.0 æ¨ç†ç®¡é“çš„å®ç°ï¼Œåœ¨ [_Highly accurate protein structure prediction with AlphaFold_](https://www.nature.com/articles/s41586-021-03819-2).
- [Adversarial Robustness](https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness) - å‚è€ƒä»£ç  [_Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples_](https://arxiv.org/abs/2010.03593) å’Œ [_Fixing Data Augmentation to Improve Adversarial Robustness_](https://arxiv.org/abs/2103.01946).
- [Bootstrap Your Own Latent](https://github.com/deepmind/deepmind-research/tree/master/byol) - è®ºæ–‡çš„å®ç° [_Bootstrap your own latent: A new approach to self-supervised Learning_](https://arxiv.org/abs/2006.07733).
- [Gated Linear Networks](https://github.com/deepmind/deepmind-research/tree/master/gated_linear_networks) - GLN æ˜¯ä¸€ç³»åˆ—æ— åå‘ä¼ æ’­çš„ç¥ç»ç½‘ç»œ.
- [Glassy Dynamics](https://github.com/deepmind/deepmind-research/tree/master/glassy_dynamics) - è®ºæ–‡çš„å¼€æºå®ç° [_Unveiling the predictive power of static structure in glassy systems_](https://www.nature.com/articles/s41567-020-0842-8).
- [MMV](https://github.com/deepmind/deepmind-research/tree/master/mmv) - ä¸­æ¨¡å‹çš„ä»£ç  [_Self-Supervised MultiModal Versatile Networks_](https://arxiv.org/abs/2006.16228).
- [Normalizer-Free Networks](https://github.com/deepmind/deepmind-research/tree/master/nfnets) - å®˜æ–¹ä¿³å¥å®ç° [_NFNets_](https://arxiv.org/abs/2102.06171).
- [NuX](https://github.com/Information-Fusion-Lab-Umass/NuX) - ä½¿ç”¨ JAX è§„èŒƒåŒ–æµç¨‹.
- [OGB-LSC](https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc) - æ­¤å­˜å‚¨åº“åŒ…å« DeepMind å¯¹ [PCQM4M-LSC](https://ogb.stanford.edu/kddcup2021/pcqm4m/) ï¼ˆé‡å­åŒ–å­¦ï¼‰å’Œ [MAG240M-LSC](https://ogb.stanford.edu/kddcup2021/mag240m/) ï¼ˆå­¦æœ¯å›¾ï¼‰
çš„è½¨è¿¹ [OGB Large-Scale Challenge](https://ogb.stanford.edu/kddcup2021/) (OGB-LSC).
- [Persistent Evolution Strategies](https://github.com/google-research/google-research/tree/master/persistent_es) - ç”¨äºè®ºæ–‡çš„ä»£ç  [_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies_](http://proceedings.mlr.press/v139/vicol21a.html).
- [WikiGraphs](https://github.com/deepmind/deepmind-research/tree/master/wikigraphs) - ç”¨äºé‡ç°ç»“æœçš„åŸºçº¿ä»£ç  [_WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Datase_](https://aclanthology.org/2021.textgraphs-1.7).

### Trax

- [Reformer](https://github.com/google/trax/tree/master/trax/models/reformer) - å®æ–½æ”¹é©è€…ï¼ˆé«˜æ•ˆå˜å‹å™¨ï¼‰æ¶æ„.


<a name="videos" />

## Videos

- [NeurIPS 2020: JAX Ecosystem Meetup](https://www.youtube.com/watch?v=iDxJxIyzSiM) - JAXï¼Œå®ƒåœ¨ DeepMind ä¸­çš„ä½¿ç”¨ï¼Œä»¥åŠå·¥ç¨‹å¸ˆã€ç§‘å­¦å®¶å’Œ JAX æ ¸å¿ƒå›¢é˜Ÿä¹‹é—´çš„è®¨è®º.
- [Introduction to JAX](https://youtu.be/0mVmRHMaOJ4) - åœ¨ JAX ä¸­ä»å¤´å¼€å§‹çš„ç®€å•ç¥ç»ç½‘ç»œ.
- [JAX: Accelerated Machine Learning Research | SciPy 2020 | VanderPlas](https://youtu.be/z-WSrQDXkuM) - JAX çš„æ ¸å¿ƒè®¾è®¡ã€å®ƒå¦‚ä½•ä¸ºæ–°ç ”ç©¶æä¾›åŠ¨åŠ›ä»¥åŠå¦‚ä½•å¼€å§‹ä½¿ç”¨å®ƒ.
- [Bayesian Programming with JAX + NumPyro â€” Andy Kitchen](https://youtu.be/CecuWGpoztw) - ä»‹ç»ä½¿ç”¨ NumPyro çš„è´å¶æ–¯å»ºæ¨¡.
- [JAX: Accelerated machine-learning research via composable function transformations in Python | NeurIPS 2019 | Skye Wanderman-Milne](https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python) - JAX ä»‹ç»ä»‹ç» [_Program Transformations for Machine Learning_](https://program-transformations.github.io) ä½œåŠ.
- [JAX on Cloud TPUs | NeurIPS 2020 | Skye Wanderman-Milne and James Bradbury](https://drive.google.com/file/d/1jKxefZT1xJDUxMman6qrQVed7vWI0MIn/edit) - æ¼”ç¤º TPU ä¸»æœºè®¿é—®.
- [Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and Beyond | NeurIPS 2020](https://slideslive.com/38935810/deep-implicit-layers-neural-odes-equilibrium-models-and-beyond) - æ•™ç¨‹ç”± Zico Kolterã€David Duvenaud å’Œ Matt Johnson åˆ›å»ºï¼ŒColab ç¬”è®°æœ¬å¯ç”¨ [_Deep Implicit Layers_](http://implicit-layers-tutorial.org).
- [Solving y=mx+b with Jax on a TPU Pod slice - Mat Kelcey](http://matpalm.com/blog/ymxb_pod_slice/) - åŒ…å« Colab ç¬”è®°æœ¬çš„å››éƒ¨åˆ† YouTube æ•™ç¨‹ç³»åˆ—ï¼Œä» Jax åŸºç¡€å¼€å§‹ï¼Œç„¶ååœ¨ v3-32 TPU Pod åˆ‡ç‰‡ä¸Šä½¿ç”¨æ•°æ®å¹¶è¡Œæ–¹æ³•è¿›è¡Œè®­ç»ƒ.
- [JAX, Flax & Transformers ğŸ¤—](https://github.com/huggingface/transformers/blob/9160d81c98854df44b1d543ce5d65a6aa28444a2/examples/research_projects/jax-projects/README.md#talks) - ä¸ºæœŸ 3 å¤©çš„æ¼”è®²ï¼Œå›´ç»• JAX / Flaxã€Transformersã€å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡å’Œå…¶ä»–é‡è¦è¯é¢˜è¿›è¡Œ.

<a name="papers" />

## Papers

æœ¬èŠ‚åŒ…å«ä¸“æ³¨äº JAX çš„è®ºæ–‡ï¼ˆä¾‹å¦‚åŸºäº JAX çš„åº“ç™½çš®ä¹¦ã€JAX ç ”ç©¶ç­‰ï¼‰. åœ¨ JAX ä¸­å®ç°çš„è®ºæ–‡åˆ—åœ¨ [Models/Projects](#projects) éƒ¨åˆ†.

<!--lint ignore awesome-list-item-->
- [__Compiling machine learning programs via high-level tracing__. Roy Frostig, Matthew James Johnson, Chris Leary. _MLSys 2018_.](https://mlsys.org/Conferences/doc/2018/146.pdf) - æè¿° JAX æ—©æœŸç‰ˆæœ¬çš„ç™½çš®ä¹¦ï¼Œè¯¦ç»†ä»‹ç»äº†å¦‚ä½•è·Ÿè¸ªå’Œç¼–è¯‘è®¡ç®—.
- [__JAX, M.D.: A Framework for Differentiable Physics__. Samuel S. Schoenholz, Ekin D. Cubuk. _NeurIPS 2020_.](https://arxiv.org/abs/1912.04232) - å¼•å…¥ JAX, MDï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å¾®ç‰©ç†åº“ï¼Œå…¶ä¸­åŒ…æ‹¬æ¨¡æ‹Ÿç¯å¢ƒã€äº¤äº’ç”µä½ã€ç¥ç»ç½‘ç»œç­‰.
- [__Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization__. Pranav Subramani, Nicholas Vadivelu, Gautam Kamath. _arXiv 2020_.](https://arxiv.org/abs/2010.09063) - ä½¿ç”¨ JAX çš„ JIT å’Œ VMAP å®ç°æ¯”ç°æœ‰åº“æ›´å¿«çš„å·®å¼‚ç§æœ‰åŒ–.
<!--lint enable awesome-list-item-->

<a name="tutorials-and-blog-posts" />

## Tutorials and Blog Posts

- [Using JAX to accelerate our research by David Budden and Matteo Hessel](https://deepmind.com/blog/article/using-jax-to-accelerate-our-research) - æè¿°äº† DeepMind çš„ JAX å’Œ JAX ç”Ÿæ€ç³»ç»Ÿçš„çŠ¶æ€.
- [Getting started with JAX (MLPs, CNNs & RNNs) by Robert Lange](https://roberttlange.github.io/posts/2020/03/blog-post-10/) - ä½¿ç”¨åŸºæœ¬ JAX è¿ç®—ç¬¦ä»å¤´å¼€å§‹æ„å»ºç¥ç»ç½‘ç»œæ¨¡å—.
- [Tutorial: image classification with JAX and Flax Linen by 8bitmp3](https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen) - äº†è§£å¦‚ä½•ä½¿ç”¨ Flax çš„ Linen API åˆ›å»ºä¸€ä¸ªç®€å•çš„å·ç§¯ç½‘ç»œå¹¶è®­ç»ƒå®ƒè¯†åˆ«æ‰‹å†™æ•°å­—.
- [Plugging Into JAX by Nick Doiron](https://medium.com/swlh/plugging-into-jax-16c120ec3302) - åœ¨ Kaggle èŠ±å‰åˆ†ç±»æŒ‘æˆ˜ä¸­æ¯”è¾ƒ Flaxã€Haiku å’Œ Objax.
- [Meta-Learning in 50 Lines of JAX by Eric Jang](https://blog.evjang.com/2019/02/maml-jax.html) - JAX å’Œå…ƒå­¦ä¹ çš„ä»‹ç».
- [Normalizing Flows in 100 Lines of JAX by Eric Jang](https://blog.evjang.com/2019/07/nf-jax.html) - ç®€æ˜å®æ–½ [RealNVP](https://arxiv.org/abs/1605.08803).
- [Differentiable Path Tracing on the GPU/TPU by Eric Jang](https://blog.evjang.com/2019/11/jaxpt.html) - å…³äºå®ç°è·¯å¾„è·Ÿè¸ªçš„æ•™ç¨‹.
- [Ensemble networks by Mat Kelcey](http://matpalm.com/blog/ensemble_nets) - é›†æˆç½‘ç»œæ˜¯ä¸€ç§å°†æ¨¡å‹é›†æˆè¡¨ç¤ºä¸ºå•ä¸ªé€»è¾‘æ¨¡å‹çš„æ–¹æ³•.
- [Out of distribution (OOD) detection by Mat Kelcey](http://matpalm.com/blog/ood_using_focal_loss) - å®ç°ä¸åŒçš„OODæ£€æµ‹æ–¹æ³•.
- [Understanding Autodiff with JAX by Srihari Radhakrishna](https://www.radx.in/jax.html) - äº†è§£ autodiff å¦‚ä½•ä½¿ç”¨ JAX å·¥ä½œ.
- [From PyTorch to JAX: towards neural net frameworks that purify stateful code by Sabrina J. Mielke](https://sjmielke.com/jax-purify.htm) - å±•ç¤ºäº†å¦‚ä½•ä»ç±»ä¼¼ PyTorch çš„ç¼–ç é£æ ¼è½¬å˜ä¸ºæ›´å…·åŠŸèƒ½æ€§çš„ç¼–ç é£æ ¼.
- [Extending JAX with custom C++ and CUDA code by Dan Foreman-Mackey](https://github.com/dfm/extending-jax) - æ¼”ç¤ºåœ¨ JAX ä¸­æä¾›è‡ªå®šä¹‰æ“ä½œæ‰€éœ€çš„åŸºç¡€ç»“æ„çš„æ•™ç¨‹.
- [Evolving Neural Networks in JAX by Robert Tjarko Lange](https://roberttlange.github.io/posts/2021/02/cma-es-jax/) - æ¢ç´¢ JAX å¦‚ä½•ä¸ºä¸‹ä¸€ä»£å¯æ‰©å±•ç¥ç»è¿›åŒ–ç®—æ³•æä¾›åŠ¨åŠ›.
- [Exploring hyperparameter meta-loss landscapes with JAX by Luke Metz](http://lukemetz.com/exploring-hyperparameter-meta-loss-landscapes-with-jax/) - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ JAX ä½¿ç”¨ SGD å’Œ Momentum æ‰§è¡Œå†…æŸå¤±ä¼˜åŒ–ã€ä½¿ç”¨æ¢¯åº¦çš„å¤–æŸå¤±ä¼˜åŒ–ä»¥åŠä½¿ç”¨è¿›åŒ–ç­–ç•¥çš„å¤–æŸå¤±ä¼˜åŒ–.
- [Deterministic ADVI in JAX by Martin Ingram](https://martiningram.github.io/deterministic-advi/) - ä½¿ç”¨ JAX è½»æ¾å¹²å‡€åœ°å®ç°è‡ªåŠ¨å¾®åˆ†å˜åˆ†æ¨ç† (ADVI).
- [Evolved channel selection by Mat Kelcey](http://matpalm.com/blog/evolved_channel_selection/) - è®­ç»ƒå¯¹ä¸åŒåˆ†è¾¨ç‡ä¸‹è¾“å…¥é€šé“çš„ä¸åŒç»„åˆå…·æœ‰é²æ£’æ€§çš„åˆ†ç±»æ¨¡å‹ï¼Œç„¶åä½¿ç”¨é—ä¼ ç®—æ³•æ¥ç¡®å®šç‰¹å®šæŸå¤±çš„æœ€ä½³ç»„åˆ.
- [Introduction to JAX by Kevin Murphy](https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/jax_intro.ipynb) - Colab ä»‹ç»äº†è¯¥è¯­è¨€çš„å„ä¸ªæ–¹é¢å¹¶å°†å…¶åº”ç”¨äºç®€å•çš„ ML é—®é¢˜.
- [Writing an MCMC sampler in JAX by Jeremie Coullon](https://www.jeremiecoullon.com/2020/11/10/mcmcjax3ways/) - å…³äºåœ¨ JAX ä¸­ç¼–å†™ MCMC é‡‡æ ·å™¨çš„ä¸åŒæ–¹æ³•ä»¥åŠé€Ÿåº¦åŸºå‡†æµ‹è¯•çš„æ•™ç¨‹.
- [How to add a progress bar to JAX scans and loops by Jeremie Coullon](https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/) - å…³äºå¦‚ä½•ä½¿ç”¨ `host_callback` æ¨¡å—åœ¨ JAX ä¸­å‘ç¼–è¯‘å¾ªç¯æ·»åŠ è¿›åº¦æ¡çš„æ•™ç¨‹.

<a name="community" />

## Community

- [JAX GitHub Discussions](https://github.com/google/jax/discussions)
- [Reddit](https://www.reddit.com/r/JAX/)

## Contributing

æ¬¢è¿æŠ•ç¨¿ï¼ é˜…è¯» [contribution guidelines](https://github.com/n2cholas/awesome-jax/blob/master/contributing.md) ç¬¬ä¸€çš„.

<div class="github-widget" data-repo="guillaume-chevalier/awesome-deep-learning-resources"></div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-6890694312814945" data-ad-slot="5473692530" data-ad-format="auto"  data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script>
## [Awesome Deep Learning Resources](https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources) [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

这是我最喜欢的深度学习资源的粗略列表. 它对我学习如何进行深度学习很有用，我用它来重温主题或参考.
一世 （[Guillaume Chevalier](https://github.com/guillaume-chevalier)) 已经建立了这个列表并仔细阅读了这里列出的所有内容.




<a name="trends" />

## Trends

这里是所有时间 [Google Trends](https://www.google.ca/trends/explore?date=all&q=machine%20learning,deep%20learning,data%20science,computer%20programming)，从 2004 年到现在，2017 年 9 月：
<p align="center">
  <img src="https://raw.githubusercontent.com/guillaume-chevalier/awesome-deep-learning-resources/master/google_trends.png" width="792" height="424" />
</p>

您可能还想看看 Andrej Karpathy  [new post](https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106) 关于机器学习研究的趋势.

我相信深度学习是让计算机更像人类一样思考的关键，并且具有很大的潜力. 一些困难的自动化任务可以很容易地解决，而这在早期的经典算法中是不可能实现的.

摩尔定律关于计算机科学硬件的指数进展率现在对 GPU 的影响比对 CPU 的影响更大，因为对原子晶体管的微小物理限制. 我们正在转向并行架构
[[read more](https://www.quora.com/Does-Moores-law-apply-to-GPUs-Or-only-CPUs) ]. 深度学习通过使用 GPU 在底层开发并行架构. 最重要的是，深度学习算法将来可能会使用量子计算并应用于机器大脑接口.

我发现智能和认知的关键是一个非常有趣的探索课题，目前还没有被很好地理解. 这些技术很有前景.


<a name="online-classes" />

## Online Classes

- **[DL&RNN Course](https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_awesome) - 我创建了这门关于深度学习和循环神经网络的内容丰富的课程.**
- [Machine Learning by Andrew Ng on Coursera](https://www.coursera.org/learn/machine-learning) - 著名的入门级在线课程 [certificate](https://www.coursera.org/account/accomplishments/verify/DXPXHYFNGKG3) . 授课人：Andrew Ng，斯坦福大学副教授； 百度首席科学家；  Coursera 董事长兼联合创始人.
- [Deep Learning Specialization by Andrew Ng on Coursera](https://www.coursera.org/specializations/deep-learning) - Andrew Ng 的新系列 5 门深度学习课程，现在使用 Python 而不是 Matlab/Octave，这导致 [specialization certificate](https://www.coursera.org/account/accomplishments/specialization/U7VNC3ZD9YD8).
- [Deep Learning by Google](https://www.udacity.com/course/deep-learning--ud730) - 涵盖高级深度学习概念的中级到高级课程，我发现一旦掌握了基础知识，这有助于发挥创造力.
- [Machine Learning for Trading by Georgia Tech](https://www.udacity.com/course/machine-learning-for-trading--ud501)  - 有趣的课程，用于获取应用于交易的机器学习基础知识以及一些人工智能和金融概念. 我特别喜欢 Q-Learning 的部分.
- [Neural networks class by Hugo Larochelle, Université de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) - Hugo Larochelle 在线免费提供有关神经网络的有趣课程，但我已经观看了其中的一些视频.
- [GLO-4030/7030 Apprentissage par réseaux de neurones profonds](https://ulaval-damas.github.io/glo4030/)  - 这是拉瓦尔大学教授菲利普·吉盖尔 (Philippe Giguère) 讲授的课程. 我特别发现它对多头注意力机制的罕见可视化非常棒，可以在 [slide 28 of week 13's class](http://www2.ift.ulaval.ca/~pgiguere/cours/DeepLearning/09-Attention.pdf).
- [Deep Learning & Recurrent Neural Networks (DL&RNN)](https://www.neuraxio.com/en/time-series-solution) - 关于深度学习和循环神经网络主题的最密集的加速课程（滚动到最后）.

<a name="books" />

## Books

- [Clean Code](https://www.amazon.ca/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882)  - 回到你愚弄的基础知识！ 了解如何为您的职业生涯打造干净的代码. 这是迄今为止我读过的最好的书，即使这个列表与深度学习有关.
- [Clean Coder](https://www.amazon.ca/Clean-Coder-Conduct-Professional-Programmers/dp/0137081073)  - 学习如何成为一名专业的编码员以及如何与您的经理互动. 这对任何编码职业都很重要.
- [How to Create a Mind](https://www.amazon.com/How-Create-Mind-Thought-Revealed/dp/B009VSFXZ4)  - 通勤时可以听音频版本. 这本书激励人们对思维进行逆向工程，并思考如何编写人工智能代码.
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) - 本书涵盖了神经网络和深度学习背后的许多核心概念.
- [Deep Learning - An MIT Press book](http://www.deeplearningbook.org/) - 然而，在本书的中途，它包含了关于如何思考实际深度学习的令人满意的数学内容.
- [Some other books I have read](https://books.google.ca/books?hl=en&as_coll=4&num=100&uid=103409002069648430166&source=gbs_slider_cls_metadata_4_mylibrary_title) - 这里列出的一些书籍与深度学习的相关性较低，但仍然与此列表有某种相关性.

<a name="posts-and-articles" />

## Posts and Articles

- [Predictions made by Ray Kurzweil](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil) - Ray Kurzweil 做出的中长期未来预测列表.
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - 必须阅读 Andrej Karpathy 的帖子 - 这就是我学习 RNN 的动力，它展示了它可以在最基本的 NLP 形式中实现的目标.
- [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) - 关于神经元如何映射信息的全新外观.
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) - 解释了 LSTM 单元的内部工作原理，此外，结论中还有有趣的链接.
- [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/) - 有趣的视觉动画，它是一个很好的介绍注意力机制的例子.
- [Recommending music on Spotify with deep learning](http://benanne.github.io/2014/08/05/spotify-cnns.html) - 对音频进行聚类非常棒 - 由 Spotify 的实习生发布.
- [Announcing SyntaxNet: The World’s Most Accurate Parser Goes Open Source](https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html) - Parsey McParseface 的诞生，一个神经语法树解析器.
- [Improving Inception and Image Classification in TensorFlow](https://research.googleblog.com/2016/08/improving-inception-and-image.html) - 非常有趣的 CNN 架构（例如：inception-style 卷积层在减少参数数量方面很有前途和效率）.
- [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) - 逼真的说话机器：完美的语音生成.
- [François Chollet's Twitter](https://twitter.com/fchollet) - Author of Keras - has interesting Twitter posts and innovative ideas.
- [Neuralink and the Brain’s Magical Future](http://waitbutwhy.com/2017/04/neuralink.html) - 关于大脑和脑机接口未来的发人深省的文章.
- [Migrating to Git LFS for Developing Deep Learning Applications with Large Files](http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/) - 轻松管理私人 Git 项目中的大文件.
- [The future of deep learning](https://blog.keras.io/the-future-of-deep-learning.html) - François Chollet 对深度学习未来的思考.
- [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) - 生长决策树并对其进行可视化，推断数据背后的隐藏逻辑.
- [Hyperopt tutorial for Optimizing Neural Networks’ Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/) - 学习自动而不是手动消除超参数空间.
- [Estimating an Optimal Learning Rate For a Deep Neural Network](https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0) - 在任何一次完整训练之前估计最佳学习率的聪明技巧.
 - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - 有助于理解“Attention Is All You Need”（AIAYN）论文. 
 - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - 也有助于理解“Attention Is All You Need”（AIAYN）论文.
 - [Improving Language Understanding with Unsupervised Learning](https://blog.openai.com/language-unsupervised/) - 在大量语料库上进行无监督预训练的许多 NLP 任务中的 SOTA.
 - [NLP's ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/) - 大家欢呼 NLP 的 ImageNet 时刻. 
 - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/) - 了解用于 NLP 的 ImageNet 时刻的不同方法. 
 - [Uncle Bob's Principles Of OOD](http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod) - 不仅需要 SOLID 原则来做干净的代码，而且众所周知的 REP、CCP、CRP、ADP、SDP 和 SAP 原则对于开发必须捆绑在不同分离包中的大型软件非常重要.
 - [Why do 87% of data science projects never make it into production?](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/) - 数据不容忽视，团队和数据科学家之间的沟通对于正确集成解决方案很重要.
 - [The real reason most ML projects fail](https://towardsdatascience.com/what-is-the-main-reason-most-ml-projects-fail-515d409a161f) - 专注于明确的业务目标，除非您有真正干净的代码，否则避免算法的支点，并且能够知道您编码的内容何时“足够好”.
 - [SOLID Machine Learning](https://www.umaneo.com/post/the-solid-principles-applied-to-machine-learning) - 应用于机器学习的 SOLID 原则.
 
<a name="practical-resources" />

## Practical Resources

<a name="librairies-and-implementations" />

### Librairies and Implementations
- [Neuraxle, a framwework for machine learning pipelines](https://github.com/Neuraxio/Neuraxle) - 构建和部署机器学习项目的最佳框架，并且还兼容大多数框架（例如：Scikit-Learn、TensorFlow、PyTorch、Keras 等）.
- [TensorFlow's GitHub repository](https://github.com/tensorflow/tensorflow) - 最著名的深度学习框架，包括高级和低级，同时保持灵活性.
- [skflow](https://github.com/tensorflow/skflow) - TensorFlow 包装器 à la scikit-learn.
- [Keras](https://keras.io/) - Keras 是另一个像 TensorFlow 一样有趣的深度学习框架，它主要是高级别的.
- [carpedm20's repositories](https://github.com/carpedm20) - 许多有趣的神经网络架构是由韩国人 Taehoon Kim，AKA carpedm20 实现的.
- [carpedm20/NTM-tensorflow](https://github.com/carpedm20/NTM-tensorflow) - 神经图灵机 TensorFlow 实现.
- [Deep learning for lazybones](http://oduerr.github.io/blog/2016/04/06/Deep-Learning_for_lazybones) - TensorFlow 中的迁移学习教程，用于从预训练 CNN 的高级嵌入中获得视觉，AlexNet 2012.
- [LSTM for Human Activity Recognition (HAR)](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) - 我的关于在时间序列上使用 LSTM 进行分类的教程.
- [Deep stacked residual bidirectional LSTMs for HAR](https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs) - 对之前项目的改进.
- [Sequence to Sequence (seq2seq) Recurrent Neural Network (RNN) for Time Series Prediction](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) - 我的关于如何预测数字时间序列的教程 - 可能是多通道的.
- [Hyperopt for a Keras CNN on CIFAR-100](https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100) - 在 CIFAR-100 数据集上自动（元）优化神经网络（及其架构）.
- [ML / DL repositories I starred](https://github.com/guillaume-chevalier?direction=desc&page=1&q=machine+OR+deep+OR+learning+OR+rnn+OR+lstm+OR+cnn&sort=stars&tab=stars&utf8=%E2%9C%93) - GitHub 充满了不错的代码示例和项目.
- [Smoothly Blend Image Patches](https://github.com/guillaume-chevalier/Smoothly-Blend-Image-Patches) - 平滑补丁合并 [semantic segmentation with a U-Net](https://vooban.com/en/tips-articles-geek-stuff/satellite-image-segmentation-workflow-with-u-net/).
- [Self Governing Neural Networks (SGNN): the Projection Layer](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) - 有了这个，您可以在深度学习模型中使用单词，而无需训练或加载嵌入.
- [Neuraxle](https://github.com/Neuraxio/Neuraxle) - Neuraxle 是一个机器学习 (ML) 库，用于构建整洁的管道，提供正确的抽象以简化 ML 应用程序的研究、开发和部署.
- [Clean Machine Learning, a Coding Kata](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code) - 通过练习学习良好的设计模式，用于以良好的方式进行机器学习.

<a name="some-datasets" />

### Some Datasets

这些是我发现在其上开发模型似乎很有趣的资源.

- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) - 用于机器学习的大量数据集.
- [Cornell Movie--Dialogs Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) - 这可用于聊天机器人.
- [SQuAD The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) - 可以在线探索的问答数据集，以及在该数据集上表现良好的模型列表.
- [LibriSpeech ASR corpus](http://www.openslr.org/12/) - 巨大的免费英语语音数据集，具有平衡的性别和说话者，这似乎是高质量的.
- [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets) - 一个很棒的公共数据集列表.
- [SentEval: An Evaluation Toolkit for Universal Sentence Representations](https://arxiv.org/abs/1803.05449) - 在许多数据集（NLP 任务）上对句子表示进行基准测试的 Python 框架. 
- [ParlAI: A Dialog Research Software Platform](https://arxiv.org/abs/1705.06476) - 另一个 Python 框架，可在许多数据集（NLP 任务）上对您的句子表示进行基准测试.


<a name="other-math-theory" />

## Other Math Theory

<a name="gradient-descent-algorithms-and-optimization" />

### Gradient Descent Algorithms & Optimization Theory

- [Neural Networks and Deep Learning, ch.2](http://neuralnetworksanddeeplearning.com/chap2.html) - 关于反向传播算法如何工作的概述.
- [Neural Networks and Deep Learning, ch.4](http://neuralnetworksanddeeplearning.com/chap4.html) - 神经网络可以计算任何函数的视觉证明.
- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.mr5wq61fb) - 公开反向传播的警告以及在训练模型时了解这一点的重要性.
- [Artificial Neural Networks: Mathematics of Backpropagation](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) - 在数学上描绘反向传播.
- [Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs](https://www.youtube.com/watch?v=56TYLaQN4N8) - 正确解释了 RNN 图的展开，并暴露了有关梯度下降算法的潜在问题.
- [Gradient descent algorithms in a saddle point](http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif) - 可视化不同的优化器如何与鞍点交互.
- [Gradient descent algorithms in an almost flat landscape](https://devblogs.nvidia.com/wp-content/uploads/2015/12/NKsFHJb.gif) - 可视化不同的优化器如何与几乎平坦的景观进行交互.
- [Gradient Descent](https://www.youtube.com/watch?v=F6GSRDoB-Cg) - 好的，我已经在上面列出了 Andrew NG 的 Coursera 课程，但是这个视频特别适合作为介绍和定义梯度下降算法.
- [Gradient Descent: Intuition](https://www.youtube.com/watch?v=YovTqTY-PYY) - 上一个视频的后续内容：现在添加直觉.
- [Gradient Descent in Practice 2: Learning Rate](https://www.youtube.com/watch?v=gX6fZHgfrow) - 如何调整神经网络的学习率.
- [The Problem of Overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I) - 对过度拟合以及如何解决该问题的一个很好的解释.
- [Diagnosing Bias vs Variance](https://www.youtube.com/watch?v=ewogYw5oCAI) - 了解神经网络预测中的偏差和方差以及如何解决这些问题.
- [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf) - 令人难以置信的 SELU 激活函数的出现.
- [Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf) - RNN 作为优化器：引入 L2L 优化器，一种元神经网络.

<a name="complex-numbers-and-digital-signal-processing" />

### Complex Numbers & Digital Signal Processing

好吧，信号处理可能与深度学习没有直接关系，但研究在开发基于信号的神经架构时有更多的直觉是很有趣的.

- [Window Functions](https://en.wikipedia.org/wiki/Window_function) - 维基百科页面列出了一些已知的窗口函数 - 请注意 [Hann-Poisson window](https://en.wikipedia.org/wiki/Window_function#Hann%E2%80%93Poisson_window) 对于贪婪的爬山算法（例如梯度下降）特别有趣. 
- [MathBox, Tools for Thought Graphical Algebra and Fourier Analysis](https://acko.net/files/gltalks/toolsforthought/) - 傅立叶分析的新面貌.
- [How to Fold a Julia Fractal](http://acko.net/blog/how-to-fold-a-julia-fractal/) - 处理复数和波动方程的动画.
- [Animate Your Way to Glory, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory/) - 物理引擎中的收敛方法，并应用于交互设计.
- [Animate Your Way to Glory - Part II, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory-pt2/) - 使用四元数进行旋转和旋转插值的漂亮动画，四元数是一种用于处理 3D 旋转的数学对象.
- [Filtering signal, plotting the STFT and the Laplace transform](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) - 关于信号处理的简单 Python 演示.


<a name="papers" />

## Papers

<a name="recurrent-neural-networks" />

### Recurrent Neural Networks

- [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828v4.pdf) - You_Again 对深度学习的总结/概述，主要是关于 RNN.
- [Bidirectional Recurrent Neural Networks](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf) - 使用 RNN 进行更好的分类，在时间轴上进行双向扫描.
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)  - 将两个网络合二为一组合成一个 seq2seq（序列到序列）编码器-解码器架构.  RNN 编码器 - 具有 1000 个隐藏单元的解码器.  Adadelta 优化器.
- [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - 4 个 1000 隐藏大小的堆叠 LSTM 单元，在 WMT&#39;14 英语到法语数据集上使用反向输入句子和波束搜索.
- [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf) - 在字符级 CNN 之上使用字级 LSTM 的漂亮递归模型，使用过多的 GPU 功率.
- [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) - NMT 主题的有趣概述，我主要阅读关于 RNN 的第 8 部分作为复习.
- [Exploring the Depths of Recurrent Neural Networks with Stochastic Residual Learning](https://cs224d.stanford.edu/reports/PradhanLongpre.pdf) - 基本上，在当前的情感分析案例中，残差连接可能比堆叠 RNN 更好.
- [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf) - 非常适合类似于 Photoshop 的“内容感知填充”来填充图像中缺失的补丁.
- [Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/pdf/1603.08983v4.pdf)  - 让 RNN 决定他们计算多长时间. 我很想看看它与神经图灵机的结合效果如何. 可以找到有关该主题的有趣交互式可视化 [here](http://distill.pub/2016/augmented-rnns/).

<a name="convolutional-neural-networks" />

### Convolutional Neural Networks

- [What is the Best Multi-Stage Architecture for Object Recognition?](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf) - 使用“局部对比度归一化”很棒.
- [ImageNet Classification with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) - AlexNet，2012 ILSVRC，ReLU 激活函数的突破.
- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901v3.pdf) - 对于“去卷积层”.
- [Fast and Accurate Deep Network Learning by Exponential Linear Units](https://arxiv.org/pdf/1511.07289v1.pdf) - CIFAR 视觉任务的 ELU 激活函数.
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556v6.pdf)  - 有趣的想法是在池化之前堆叠多个 3x3 conv+ReLU 以获得更大的过滤器尺寸，只需几个参数.  “ConvNet Configuration”也有一个很好的表格.
- [Going Deeper with Convolutions](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) - GoogLeNet：“初始”层/模块的外观，其想法是将卷积层并行化为许多具有“相同”填充的不同大小的 mini-conv，在深度上串联.
- [Highway Networks](https://arxiv.org/pdf/1505.00387v2.pdf) - 公路网络：残差连接.
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf) - 批量归一化 (BN)：通过对整个批次求和来归一化层的输出，然后执行特定可训练量的线性重新缩放和移位.
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) - U-Net 是一个编码器-解码器 CNN，它也具有跳跃连接，适用于每像素级别的图像分割.
- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) - 带有批量归一化层的非常深的残差层 - 也就是“如何过度拟合具有过多层的任何视觉数据集，并在给定足够数据的情况下使任何视觉模型在识别时正常工作”.
- [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261v2.pdf) - 用于改进具有残差连接的 GoogLeNet.
- [WaveNet: a Generative Model for Raw Audio](https://arxiv.org/pdf/1609.03499v2.pdf) - 使用基于扩张因果卷积的新架构生成史诗般的原始语音/音乐，以捕获更多的音频长度.
- [Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling](https://arxiv.org/pdf/1610.07584v2.pdf) - 用于 3D 模型生成的 3D-GAN 和来自嵌入的有趣 3D 家具算法（像 word2vec 词算法和 3D 家具表示）.
- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://research.fb.com/publications/ImageNet1kIn1h/) - 令人难以置信的快速分布式 CNN 训练.
- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf) - 在 CVPR 2017 上获得最佳论文奖，在 CIFAR-10、CIFAR-100 和 SVHN 数据集上的最先进性能的改进，这种新的神经网络架构被命名为 DenseNet.
- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf) - 融合了 U-Net 和 DenseNet 的思想，这个新的神经网络特别适用于图像分割中的大型数据集.
- [Prototypical Networks for Few-shot Learning](https://arxiv.org/pdf/1703.05175.pdf) - 在损失中使用距离度量从几个例子中确定一个对象属于哪个类.

<a name="attention-mechanisms" />

### Attention Mechanisms

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)  - LSTM 的注意力机制！ 大多数情况下，数字和公式及其解释对我有用. 我在那篇论文上发表了演讲 [here](https://www.youtube.com/watch?v=QuvRWevJMZ4).
- [Neural Turing Machines](https://arxiv.org/pdf/1410.5401v2.pdf)  - 在让神经网络学习具有长时间依赖关系的看似良好泛化的算法方面表现出色. 序列回忆问题.
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf) - LSTMs 在 CNNs 特征图上的注意力机制创造了奇迹.
- [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340v3.pdf) - 一个非常有趣和创造性的文字问答作品，这是一个突破，与此有关.
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf) - 探索注意力机制的不同方法.
- [Matching Networks for One Shot Learning](https://arxiv.org/pdf/1606.04080.pdf) - 通过使用注意机制和查询将图像与其他图像进行比较以进行分类，从而对低数据进行一次性学习的有趣方式.
- [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf) - 2016 年：在编码器/解码器上具有注意力机制的堆叠残差 LSTM 是 NMT（神经机器翻译）的最佳选择.
- [Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) - 基于 NTM 的可微存储器的改进：现在是可微神经计算机 (DNC).
- [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) - 这产生了关于在框架 seq2seq 问题公式中进行 NMT 的工作边界的直觉.
- [通过在梅尔谱图上调节 WaveNet 进行自然 TTS 合成
预测](https://arxiv.org/pdf/1712.05884.pdf) - A [WaveNet](https://arxiv.org/pdf/1609.03499v2.pdf) 用作声码器的声音可以以从 Tacotron 2 LSTM 神经网络生成的 Mel 频谱图为条件，并注意从文本生成整洁的音频.
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (AIAYN) - 引入带有位置编码的多头自注意力神经网络，在没有任何 RNN 和 CNN 的情况下进行句子级 NLP - 这篇论文是必读的（另见 [this explanation](http://nlp.seas.harvard.edu/2018/04/03/attention.html) 和 [this visualization](http://jalammar.github.io/illustrated-transformer/) 论文）. 

<a name="other" />

### Other

- [ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections](https://arxiv.org/abs/1708.00630) - 在您的深度神经网络中用词投影替换词嵌入，这不需要预先提取的字典，也不需要存储嵌入矩阵. 
- [Self-Governing Neural Networks for On-Device Short Text Classification](http://aclweb.org/anthology/D18-1105)  - 这篇论文是上面 ProjectionNet 的续集.  SGNN 在 ProjectionNet 上有详细阐述，优化的细节更深入（另见我 [attempt to reproduce the paper in code](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) 并观看 [the talks' recording](https://vimeo.com/305197775)).
- [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080) - Classify a new example from a list of other examples (without definitive categories) and with low-data per classification task, but lots of data for lots of similar classification tasks - it seems better than siamese networks. To sum up: with Matching Networks, you can optimize directly for a cosine similarity between examples (like a self-attention product would match) which is passed to the softmax directly. I guess that Matching Networks could probably be used as with negative-sampling softmax training in word2vec's CBOW or Skip-gram without having to do any context embedding lookups. 


<a name="youtube" />

## YouTube and Videos

- [Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG](https://www.youtube.com/watch?v=QuvRWevJMZ4) - 一个关于注意力机制的阅读小组的演讲（论文：Neural Machine Translation by Jointly Learning to Align and Translate）.
- [Tensor Calculus and the Calculus of Moving Surfaces](https://www.youtube.com/playlist?list=PLlXfTHzgMRULkodlIEqfgTS-H1AY_bNtq) - 正确概括 Tensor 的工作原理，但仅观看一些视频就已经对掌握概念大有帮助.
- [Deep Learning & Machine Learning (Advanced topics)](https://www.youtube.com/playlist?list=PLlp-GWNOd6m4C_-9HxuHg2_ZeI2Yzwwqt) - 我发现有趣或有用的有关深度学习的视频列表，这是所有内容的混合.
- [Signal Processing Playlist](https://www.youtube.com/playlist?list=PLlp-GWNOd6m6gSz0wIcpvl4ixSlS-HEmr) - 我编写的关于 DFT/FFT、STFT 和拉普拉斯变换的 YouTube 播放列表 - 我对我的软件工程学士学位很生气，不包括信号处理课程（除了量子物理课程中的一点）.
- [Computer Science](https://www.youtube.com/playlist?list=PLlp-GWNOd6m7vLOsW20xAJ81-65C-Ys6k) - 我编写的另一个 YouTube 播放列表，这次是关于各种 CS 主题.
- [Siraj's Channel](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/videos?view=0&sort=p&flow=grid) - Siraj 有关于深度学习的有趣、快节奏的视频教程.
- [Two Minute Papers' Channel](https://www.youtube.com/user/keeroyz/videos?sort=p&view=0&flow=grid) - 一些研究论文的有趣而浅薄的概述，例如关于 WaveNet 或神经风格迁移.
- [Geoffrey Hinton interview](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview) - Andrew Ng 采访 Geoffrey Hinton，他谈到了他的研究和突破，并为学生提供了建议.
- [Growing Neat Software Architecture from Jupyter Notebooks](https://www.youtube.com/watch?v=K4QN27IKr0g) - 关于如何在使用 Jupyter Notebooks 时构建机器学习项目的入门.

<a name="misc-hubs-and-links" />

## Misc. Hubs & Links

- [Hacker News](https://news.ycombinator.com/news) - 也许我是如何发现机器学习的 - 有趣的趋势在成为大问题之前就出现在该网站上.
- [DataTau](http://www.datatau.com/) - 这是一个类似于 Hacker News 的中心，但专门针对数据科学.
- [Naver](http://www.naver.com/)  - 这是一个韩语搜索引擎 - 讽刺的是，最好与谷歌翻译一起使用. 令人惊讶的是，有时深度学习搜索结果和可理解的高级数学内容比谷歌搜索更容易出现.
- [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/) - 具有 TF/IDF 功能的 arXiv 浏览器.
- [Awesome Neuraxle](https://github.com/Neuraxio/Awesome-Neuraxle) - Neuraxle 的一个很棒的列表，一个用于编码干净的生产级 ML 管道的 ML 框架.


<a name="license" />

## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

在法律允许的范围内， [Guillaume Chevalier](https://github.com/guillaume-chevalier) 已放弃本作品的所有版权和相关或邻接权.

<div class="github-widget" data-repo="guillaume-chevalier/awesome-deep-learning-resources"></div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-6890694312814945" data-ad-slot="5473692530" data-ad-format="auto"  data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script>
## [Awesome Deep Learning Resources](https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources) [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

这是我最喜欢的深度学习资源的粗略清单. 对于学习如何进行深度学习，用于重新访问主题或供参考，它对我很有用.
一世 （[Guillaume Chevalier](https://github.com/guillaume-chevalier)）已建立此列表，并仔细浏览了此处列出的所有内容.




<a name="trends" />

## Trends

这是空前 [Google Trends](https://www.google.ca/trends/explore?date=all&q=machine%20learning,deep%20learning,data%20science,computer%20programming)，从2004年至今，即2017年9月：
<p align="center">
  <img src="https://raw.githubusercontent.com/guillaume-chevalier/awesome-deep-learning-resources/master/google_trends.png" width="792" height="424" />
</p>

您可能还想看看Andrej Karpathy [new post](https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106) 关于机器学习研究的趋势.

我相信深度学习是使计算机更像人类思考的关键，并且具有很大的潜力. 用这种方法可以轻松解决一些艰巨的自动化任务，而用经典算法则无法实现.

关于计算机科学硬件的指数级进步速度的摩尔定律现在对GPU的影响比对CPU的影响更大，这是因为原子晶体管的微小物理限制. 我们正在转向并行架构
[[read more](https://www.quora.com/Does-Moores-law-apply-to-GPUs-Or-only-CPUs) ]. 深度学习通过使用GPU在后台开发并行架构. 最重要的是，深度学习算法可能会使用量子计算，并在将来应用于机器脑接口.

我发现，智力和认知的关键是一个非常有趣的主题，并且尚未得到很好的理解. 这些技术是有前途的.


<a name="online-classes" />

## Online Classes

- [Machine Learning by Andrew Ng on Coursera](https://www.coursera.org/learn/machine-learning) -具有知名度的入门级在线课程 [certificate](https://www.coursera.org/account/accomplishments/verify/DXPXHYFNGKG3) . 授课人：斯坦福大学副教授吴安德； 百度首席科学家 Coursera董事长兼联合创始人.
- [Deep Learning Specialization by Andrew Ng on Coursera](https://www.coursera.org/specializations/deep-learning) -Andrew Ng的新系列5深度学习课程，现在使用Python而不是Matlab / Octave，这导致了 [specialization certificate](https://www.coursera.org/account/accomplishments/specialization/U7VNC3ZD9YD8).
- [Deep Learning by Google](https://www.udacity.com/course/deep-learning--ud730) -涵盖了高级深度学习概念的中高级课程，我发现一旦掌握了基础知识，它就会有助于创新.
- [Machine Learning for Trading by Georgia Tech](https://www.udacity.com/course/machine-learning-for-trading--ud501)  -有趣的课程，用于学习适用于交易的机器学习基础知识以及一些AI和金融概念. 我特别喜欢有关Q学习的部分.
- [Neural networks class by Hugo Larochelle, Université de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) -关于有趣的关于神经网络的课程，Hugo Larochelle免费提供了在线课程，但我已经观看了其中一些视频.
- [GLO-4030/7030 Apprentissage par réseaux de neurones profonds](https://ulaval-damas.github.io/glo4030/)  -这是拉瓦尔大学教授PhilippeGiguère开设的课程. 我特别发现了它罕见的多头注意力机制可视化效果，可以在 [slide 28 of week 13's class](http://www2.ift.ulaval.ca/~pgiguere/cours/DeepLearning/09-Attention.pdf).
- [Deep Learning & Recurrent Neural Networks (DL&RNN)](https://www.neuraxio.com/en/time-series-solution) -关于“深度学习和递归神经网络”的主题中，最丰富，最加速的课程（最后滚动）.

<a name="books" />

## Books

- [Clean Code](https://www.amazon.ca/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882)  -回到您愚弄的基本知识！ 了解如何为您的职业做清洁代码. 即使此列表与深度学习有关，这也是迄今为止我读过的最好的书.
- [Clean Coder](https://www.amazon.ca/Clean-Coder-Conduct-Professional-Programmers/dp/0137081073)  -了解如何成为一名专业的编码员，以及如何与您的经理互动. 这对任何编码职业都很重要.
- [How to Create a Mind](https://www.amazon.com/How-Create-Mind-Thought-Revealed/dp/B009VSFXZ4)  -通勤时听音频版本很不错. 本书旨在激励人们进行逆向工程，并思考如何编写AI.
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) -本书涵盖了神经网络和深度学习背后的许多核心概念.
- [Deep Learning - An MIT Press book](http://www.deeplearningbook.org/) -在本书中途，它包含了令人满意的数学内容，涉及如何思考实际的深度学习.
- [Some other books I have read](https://books.google.ca/books?hl=en&as_coll=4&num=100&uid=103409002069648430166&source=gbs_slider_cls_metadata_4_mylibrary_title) -此处列出的一些书籍与深度学习的相关性较小，但仍与该列表相关.

<a name="posts-and-articles" />

## Posts and Articles

- [Predictions made by Ray Kurzweil](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil) -雷·库兹韦尔（Ray Kurzweil）提出的中长期的未来预测.
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) -必须阅读Andrej Karpathy的帖子-这是促使我学习RNN的动力，它演示了以NLP的最基本形式可以实现的目标.
- [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) -重新了解神经元如何映射信息.
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) -说明LSTM单元的内部工作原理，此外，它还具有有趣的结论.
- [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/) -有趣的视觉动画，它是注意力机制的一个很好的介绍.
- [Recommending music on Spotify with deep learning](http://benanne.github.io/2014/08/05/spotify-cnns.html) -非常适合对音频进行聚类-由Spotify的实习生发布.
- [Announcing SyntaxNet: The World’s Most Accurate Parser Goes Open Source](https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html) -Parsey McParseface的诞生，一种神经语法树解析器.
- [Improving Inception and Image Classification in TensorFlow](https://research.googleblog.com/2016/08/improving-inception-and-image.html) -非常有趣的CNN架构（例如，在减少参数数量方面，初始样式的卷积层很有希望且有效）.
- [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) -逼真的说话器：完美的声音产生.
- [François Chollet's Twitter](https://twitter.com/fchollet) - Author of Keras - has interesting Twitter posts and innovative ideas.
- [Neuralink and the Brain’s Magical Future](http://waitbutwhy.com/2017/04/neuralink.html) -关于大脑和脑机接口的未来的启发性文章.
- [Migrating to Git LFS for Developing Deep Learning Applications with Large Files](http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/) -轻松管理您私有Git项目中的大文件.
- [The future of deep learning](https://blog.keras.io/the-future-of-deep-learning.html) -弗朗索瓦·乔列（FrançoisChollet）关于深度学习的未来的思想.
- [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) -增长决策树并将其可视化，推断数据背后的隐藏逻辑.
- [Hyperopt tutorial for Optimizing Neural Networks’ Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/) -学习自动削减超参数空间，而不是手工削减.
- [Estimating an Optimal Learning Rate For a Deep Neural Network](https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0) -巧妙的技巧，可以在任何一次完整的训练之前估算出最佳的学习速度.
 - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) -有助于理解“注意就是您所需要的”（AIAYN）论文. 
 - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) -也有助于理解“注意就是您所需要的”（AIAYN）文件.
 - [Improving Language Understanding with Unsupervised Learning](https://blog.openai.com/language-unsupervised/) -从大型语料库的无监督预训练中跨许多NLP任务进行SOTA.
 - [NLP's ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/) -全部欢呼NLP的ImageNet时刻. 
 - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/) -了解用于NLP ImageNet时刻的不同方法. 
 - [Uncle Bob's Principles Of OOD](http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod) -不仅需要SOLID原则来编写干净的代码，而且众所周知的REP，CCP，CRP，ADP，SDP和SAP原则对于开发必须捆绑在不同单独软件包中的大型软件也非常重要.
 - [Why do 87% of data science projects never make it into production?](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/) -数据不可忽视，团队与数据科学家之间的沟通对于正确集成解决方案非常重要.
 - [The real reason most ML projects fail](https://towardsdatascience.com/what-is-the-main-reason-most-ml-projects-fail-515d409a161f) -专注于明确的业务目标，避免使用算法的枢纽，除非您拥有真正干净的代码，并且能够知道何时编写的代码“足够好”.
 
<a name="practical-resources" />

## Practical Resources

<a name="librairies-and-implementations" />

### Librairies and Implementations
- [Neuraxle, a framwework for machine learning pipelines](https://github.com/Neuraxio/Neuraxle) -用于构建和部署机器学习项目的最佳框架，并且也与大多数框架兼容（例如：Scikit-Learn，TensorFlow，PyTorch，Keras等）.
- [TensorFlow's GitHub repository](https://github.com/tensorflow/tensorflow) -最知名的深度学习框架，包括高层和底层，同时保持灵活性.
- [skflow](https://github.com/tensorflow/skflow) -TensorFlow包装器scikit-learn.
- [Keras](https://keras.io/) -Keras是另一个有趣的深度学习框架，如TensorFlow，它主要是高层的.
- [carpedm20's repositories](https://github.com/carpedm20) -韩国人Taehoon Kim（又名carpedm20）实现了许多有趣的神经网络体系结构.
- [carpedm20/NTM-tensorflow](https://github.com/carpedm20/NTM-tensorflow) -神经图灵机TensorFlow实施.
- [Deep learning for lazybones](http://oduerr.github.io/blog/2016/04/06/Deep-Learning_for_lazybones) -TensorFlow中的转移学习教程，用于从预先训练的CNN的高级嵌入中获得视觉，AlexNet 2012.
- [LSTM for Human Activity Recognition (HAR)](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) -有关在时间序列上使用LSTM进行分类的教程.
- [Deep stacked residual bidirectional LSTMs for HAR](https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs) -对先前项目的改进.
- [Sequence to Sequence (seq2seq) Recurrent Neural Network (RNN) for Time Series Prediction](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) -关于如何预测数字的时间序列的教程-可能是多通道的.
- [Hyperopt for a Keras CNN on CIFAR-100](https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100) -在CIFAR-100数据集上自动（元）优化神经网络（及其体系结构）.
- [ML / DL repositories I starred](https://github.com/guillaume-chevalier?direction=desc&page=1&q=machine+OR+deep+OR+learning+OR+rnn+OR+lstm+OR+cnn&sort=stars&tab=stars&utf8=%E2%9C%93) -GitHub上充满了不错的代码示例和项目.
- [Smoothly Blend Image Patches](https://github.com/guillaume-chevalier/Smoothly-Blend-Image-Patches) -平滑的补丁合并 [semantic segmentation with a U-Net](https://vooban.com/en/tips-articles-geek-stuff/satellite-image-segmentation-workflow-with-u-net/).
- [Self Governing Neural Networks (SGNN): the Projection Layer](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) -这样一来，您就可以在深度学习模型中使用单词，而无需训练或加载嵌入内容.
- [Neuraxle](https://github.com/Neuraxio/Neuraxle) -Neuraxle是一个机器学习（ML）库，用于构建整洁的管道，提供正确的抽象以简化ML应用程序的研究，开发和部署.
- [Clean Machine Learning, a Coding Kata](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code) -通过练习学习用于设计机器学习的良好设计模式.

<a name="some-datasets" />

### Some Datasets

Those are resources I have found that seems interesting to develop models onto.

- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) -大量的ML数据集.
- [Cornell Movie--Dialogs Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) -这可以用于聊天机器人.
- [SQuAD The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) -可以在线浏览的问题解答数据集，以及在该数据集上表现良好的模型列表.
- [LibriSpeech ASR corpus](http://www.openslr.org/12/) -庞大的免费英语语音数据集，具有平衡的性别和说话者，这似乎是高质量的.
- [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets) -很棒的公共数据集列表.
- [SentEval: An Evaluation Toolkit for Universal Sentence Representations](https://arxiv.org/abs/1803.05449) -一个Python框架，用于在许多数据集（NLP任务）上对句子表示形式进行基准测试. 
- [ParlAI: A Dialog Research Software Platform](https://arxiv.org/abs/1705.06476) -另一个Python框架，用于在许多数据集（NLP任务）上对句子表示形式进行基准测试.


<a name="other-math-theory" />

## Other Math Theory

<a name="gradient-descent-algorithms-and-optimization" />

### Gradient Descent Algorithms & Optimization Theory

- [Neural Networks and Deep Learning, ch.2](http://neuralnetworksanddeeplearning.com/chap2.html) -关于反向传播算法如何工作的概述.
- [Neural Networks and Deep Learning, ch.4](http://neuralnetworksanddeeplearning.com/chap4.html) -视觉证明神经网络可以计算任何功能.
- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.mr5wq61fb) -揭露反向道具的警告和在训练模型时了解这一点的重要性.
- [Artificial Neural Networks: Mathematics of Backpropagation](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) -用数学方法描绘反向支撑.
- [Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs](https://www.youtube.com/watch?v=56TYLaQN4N8) -正确解释了RNN图的展开，并揭示了有关梯度下降算法的潜在问题.
- [Gradient descent algorithms in a saddle point](http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif) -可视化不同的优化器如何与鞍点交互.
- [Gradient descent algorithms in an almost flat landscape](https://devblogs.nvidia.com/wp-content/uploads/2015/12/NKsFHJb.gif) -可视化不同的优化器如何与几乎平坦的景观交互.
- [Gradient Descent](https://www.youtube.com/watch?v=F6GSRDoB-Cg) -好的，我已经在上面列出了Andrew NG的Coursera类，但是该视频特别适合作为介绍并定义了梯度下降算法.
- [Gradient Descent: Intuition](https://www.youtube.com/watch?v=YovTqTY-PYY) -上一个视频的内容如下：现在添加直觉.
- [Gradient Descent in Practice 2: Learning Rate](https://www.youtube.com/watch?v=gX6fZHgfrow) -如何调整神经网络的学习率.
- [The Problem of Overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I) -关于过度拟合以及如何解决该问题的很好的解释.
- [Diagnosing Bias vs Variance](https://www.youtube.com/watch?v=ewogYw5oCAI) -了解神经网络预测中的偏差和方差以及如何解决这些问题.
- [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf) -令人难以置信的SELU激活功能的外观.
- [Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf) -RNN作为优化器：引入了L2L优化器（一种元神经网络）.

<a name="complex-numbers-and-digital-signal-processing" />

### Complex Numbers & Digital Signal Processing

好的，信号处理可能与深度学习没有直接关系，但是研究在基于信号的神经体系结构开发中拥有更多的直觉是有趣的.

- [Window Functions](https://en.wikipedia.org/wiki/Window_function) -列出了一些已知窗口功能的Wikipedia页面-请注意， [Hann-Poisson window](https://en.wikipedia.org/wiki/Window_function#Hann%E2%80%93Poisson_window) 对于贪婪的爬山算法（例如，梯度下降）而言，它特别有趣. 
- [MathBox, Tools for Thought Graphical Algebra and Fourier Analysis](https://acko.net/files/gltalks/toolsforthought/) -傅立叶分析的新面貌.
- [How to Fold a Julia Fractal](http://acko.net/blog/how-to-fold-a-julia-fractal/) -处理复数和波动方程的动画.
- [Animate Your Way to Glory, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory/) -物理引擎中的收敛方法，并应用于交互设计.
- [Animate Your Way to Glory - Part II, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory-pt2/) -用于旋转和带有四元数的旋转插值的精美动画，四元数是用于处理3D旋转的数学对象.
- [Filtering signal, plotting the STFT and the Laplace transform](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) -有关信号处理的简单Python演示.


<a name="papers" />

## Papers

<a name="recurrent-neural-networks" />

### Recurrent Neural Networks

- [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828v4.pdf) -You_Again的深度学习摘要/概述，主要是有关RNN的.
- [Bidirectional Recurrent Neural Networks](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf) -在时间轴上进行双向扫描的RNN进行更好的分类.
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)  -将两个网络合二为一的seq2seq（序列到序列）编码器-解码器体系结构.  RNN编码器-具有1000个隐藏单元的解码器.  Adadelta优化器.
- [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) -在WMT&#39;14英语到法语数据集上，具有4个堆叠的LSTM单元，它们的隐藏大小为1000，带有反向输入语句和波束搜索.
- [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf) -在字符级CNN上使用单词级LSTM的不错的递归模型，使用了过多的GPU功能.
- [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) -关于NMT的主题，有趣的概述是，我主要阅读有关RNN的第8部分，并作为重温.
- [Exploring the Depths of Recurrent Neural Networks with Stochastic Residual Learning](https://cs224d.stanford.edu/reports/PradhanLongpre.pdf) -基本上，在呈现的情绪分析案例中，残余连接可以比堆叠的RNN更好.
- [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf) -非常适合像photoshop一样的“内容感知填充”来填充图像中缺少的色块.
- [Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/pdf/1603.08983v4.pdf)  -让RNN决定计算时间. 我很想知道它与神经图灵机的结合效果如何. 可以找到关于该主题的有趣的交互式可视化 [here](http://distill.pub/2016/augmented-rnns/).

<a name="convolutional-neural-networks" />

### Convolutional Neural Networks

- [What is the Best Multi-Stage Architecture for Object Recognition?](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf) -非常适合使用“局部对比归一化”.
- [ImageNet Classification with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) -AlexNet，2012年ILSVRC，突破了ReLU激活功能.
- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901v3.pdf) -用于“ deconvnet层”.
- [Fast and Accurate Deep Network Learning by Exponential Linear Units](https://arxiv.org/pdf/1511.07289v1.pdf) -CIFAR视觉任务的ELU激活功能.
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556v6.pdf)  -有趣的想法是在仅使用几个参数的情况下合并多个3x3 conv + ReLU，然后合并到更大的过滤器中. 对于“ ConvNet配置”，还有一个不错的表.
- [Going Deeper with Convolutions](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) -GoogLeNet：“起始”层/模块的出现，其想法是将conv层并行化为许多不同大小的微型conv，并使用“相同”的填充，并在深度上进行连接.
- [Highway Networks](https://arxiv.org/pdf/1505.00387v2.pdf) -公路网：残余连接.
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf) -批量归一化（BN）：通过还对整个批次进行求和来归一化图层的输出，然后执行线性重新缩放和某个可训练量的平移.
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) -U-Net是编码器/解码器CNN，也具有跳过连接，适合在每个像素级别进行图像分割.
- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) -具有批处理归一化层的非常深的残留层-又名“如何在具有足够多的层的情况下过度拟合任何视觉数据集，并使任何视觉模型在识别时都能正常工作”.
- [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261v2.pdf) -用于改进带有残余连接的GoogLeNet.
- [WaveNet: a Generative Model for Raw Audio](https://arxiv.org/pdf/1609.03499v2.pdf) 史诗原始语音/音乐生成，具有基于因果卷积的新架构，可捕获更多音频长度.
- [Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling](https://arxiv.org/pdf/1610.07584v2.pdf) -3D-GAN用于生成3D模型并从嵌入中获得有趣的3D家具算术（想像具有3D家具表示形式的word2vec单词算术）.
- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://research.fb.com/publications/ImageNet1kIn1h/) -CNN的快速分布式培训.
- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf) -2017年CVPR最佳论文奖，该新神经网络架构被称为DenseNet，对CIFAR-10，CIFAR-100和SVHN数据集的最新性能产生了改进.
- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf) -融合了U-Net和DenseNet的思想，这种新的神经网络特别适用于图像分割中的大型数据集.
- [Prototypical Networks for Few-shot Learning](https://arxiv.org/pdf/1703.05175.pdf) -在损失中使用距离度量，从一些示例中确定对象属于哪个类.

<a name="attention-mechanisms" />

### Attention Mechanisms

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)  -LSTM的注意机制！ 通常，数字和公式及其解释对我很有用. 我在那张纸上发表了演讲 [here](https://www.youtube.com/watch?v=QuvRWevJMZ4).
- [Neural Turing Machines](https://arxiv.org/pdf/1410.5401v2.pdf)  -在让神经网络学习算法方面具有出色的长期综合性方面表现出色. 序列调用问题.
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf) -LSTM在CNN特征图上的关注机制确实令人惊讶.
- [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340v3.pdf) -关于文本问题解答的一项非常有趣且富有创造力的工作，这是一项突破，与之相关.
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf) -探索注意力机制的不同方法.
- [Matching Networks for One Shot Learning](https://arxiv.org/pdf/1606.04080.pdf) -通过使用注意力机制和查询将图像与其他图像进行分类的有趣的低数据单次学习方法.
- [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf) -2016年：对于编码器/解码器具有注意机制的堆叠式残留LSTM，最适合NMT（神经机器翻译）.
- [Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) -改进了基于NTM的可区分内存：现在是可区分神经计算机（DNC）.
- [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) -在框架化的seq2seq问题表述中得出关于进行NMT的工作范围的直觉.
-[通过在梅尔谱图上调节WaveNet合成天然TTS
预测]（https://arxiv.org/pdf/1712.05884.pdf）-A [WaveNet](https://arxiv.org/pdf/1609.03499v2.pdf) 用作声码器的条件可以是Tacotron 2 LSTM神经网络生成的梅尔频谱图，但要注意从文本生成纯净的音频.
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) （AIAYN）-引入具有位置编码的多头自注意神经网络以在没有任何RNN或CNN的情况下执行句子级NLP-这篇文章是必读 [this explanation](http://nlp.seas.harvard.edu/2018/04/03/attention.html) 和 [this visualization](http://jalammar.github.io/illustrated-transformer/) 本文）. 

<a name="other" />

### Other

- [ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections](https://arxiv.org/abs/1708.00630) -在深度神经网络中，用单词投影替换单词嵌入，不需要预先提取字典，也不需要存储嵌入矩阵. 
- [Self-Governing Neural Networks for On-Device Short Text Classification](http://aclweb.org/anthology/D18-1105)  -本文是以上ProjectionNet的续集.  SGNN在ProjectionNet上进行了详细说明，并且对优化进行了更深入的详细介绍（另请参见 [attempt to reproduce the paper in code](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) 看 [the talks' recording](https://vimeo.com/305197775)).
- [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080) - Classify a new example from a list of other examples (without definitive categories) and with low-data per classification task, but lots of data for lots of similar classification tasks - it seems better than siamese networks. To sum up: with Matching Networks, you can optimize directly for a cosine similarity between examples (like a self-attention product would match) which is passed to the softmax directly. I guess that Matching Networks could probably be used as with negative-sampling softmax training in word2vec's CBOW or Skip-gram without having to do any context embedding lookups. 


<a name="youtube" />

## YouTube and Videos

- [Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG](https://www.youtube.com/watch?v=QuvRWevJMZ4) -一个关于注意力机制的阅读小组的演讲（论文：通过共同学习对齐和翻译的神经机器翻译）.
- [Tensor Calculus and the Calculus of Moving Surfaces](https://www.youtube.com/playlist?list=PLlXfTHzgMRULkodlIEqfgTS-H1AY_bNtq) -适当地概括Tensors的工作原理，但是仅仅观看一些视频已经对理解这些概念很有帮助.
- [Deep Learning & Machine Learning (Advanced topics)](https://www.youtube.com/playlist?list=PLlp-GWNOd6m4C_-9HxuHg2_ZeI2Yzwwqt) -我发现有趣或有用的有关深度学习的视频列表，这是所有内容的组合.
- [Signal Processing Playlist](https://www.youtube.com/playlist?list=PLlp-GWNOd6m6gSz0wIcpvl4ixSlS-HEmr) -我撰写的有关DFT / FFT，STFT和Laplace变换的YouTube播放列表-我对自己的软件工程学士学位不包括信号处理课程（量子物理学课程中的一点点除外）感到很生气.
- [Computer Science](https://www.youtube.com/playlist?list=PLlp-GWNOd6m7vLOsW20xAJ81-65C-Ys6k) -我撰写的另一个YouTube播放列表，这次涉及各种CS主题.
- [Siraj's Channel](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/videos?view=0&sort=p&flow=grid) -Siraj提供了有趣的，快节奏的有关深度学习的视频教程.
- [Two Minute Papers' Channel](https://www.youtube.com/user/keeroyz/videos?sort=p&view=0&flow=grid) -一些研究论文的有趣而浅薄的概述，例如有关WaveNet或神经样式转换的论文.
- [Geoffrey Hinton interview](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview) -吴安达（George Ng）采访了杰弗里·欣顿（Geoffrey Hinton），后者谈论了他的研究和波谷，并为学生提供建议.
- [Growing Neat Software Architecture from Jupyter Notebooks](https://www.youtube.com/watch?v=K4QN27IKr0g) -有关使用Jupyter笔记本时如何构建机器学习项目的入门.

<a name="misc-hubs-and-links" />

## Misc. Hubs & Links

- [Hacker News](https://news.ycombinator.com/news) -也许是我发现ML的方式-有趣的趋势在出现大问题之前就已经出现在该网站上.
- [DataTau](http://www.datatau.com/) -这是类似于Hacker News的中心，但特定于数据科学.
- [Naver](http://www.naver.com/)  -这是韩语搜索引擎-具有讽刺意味的是，最好与Google翻译配合使用. 令人惊讶的是，有时深度学习搜索结果和可理解的高级数学内容在那里显示比在Google搜索上更容易.
- [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/) -具有TF / IDF功能的arXiv浏览器.
- [Awesome Neuraxle](https://github.com/Neuraxio/Awesome-Neuraxle) -Neuraxle的绝妙清单，Neuraxle是一个用于编码清洁生产级ML管道的ML框架.


<a name="license" />

## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

在法律允许的范围内， [Guillaume Chevalier](https://github.com/guillaume-chevalier) 放弃了此作品的所有版权以及相关或邻近的权利.
